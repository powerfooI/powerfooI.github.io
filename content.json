{"posts":[{"title":"对Python,Golang和C++三种语言GC机制的简单调查","text":"调查背景在公司一项任务是需要调用Python的SDK爬取相关的数据信息，数据的量在10亿这个量级，故不能够一下子得到结果。这个程序运行十来天的可能性比较大，但是问题来了，程序跑过一阵子（1小时）之后爬取效率明显降低。重启之后效率恢复，这就让人有点不爽了，本来数据量就多，用初速度爬取也需要十来天，这样一减速得爬到什么时候呢？通过使用top工具，观察到爬虫脚本在运行过程中占用的内存从800MB上升到了1.4GB，速度也随内存占用量的上升而减慢。（图片来自MacOS的top，与程序的运行环境中的top不太一样） 我猜可能是内存泄漏了，各种查资料之后用python的gc和objgraph进行程序内存使用情况分析。虽然针对于爬虫程序的分析无果，但是对GC机制有了一点兴趣，于是稍微了解了一下。 下文的 GC 既指 Garbage Collection， 也指 Garbage Collector。 接触Python一年多，Golang九个月，C++是大一时OOP课上教授的语言。其实都只是了解了皮毛，仅仅停留在“会用”这个层面。 三种语言的GC机制Python1. 引用计数引用计数是一个很简单的实现方式，顾名思义： 当一个对象被引用时，该对象的引用次数+1； 当引用这个对象的另一个对象被GC回收时，该对象的引用次数-1；当GC监测到某对象的引用次数为0时则将该对象回收。 但是这个方案无法解决循环引用。 2. 分代回收根据对象的存活周期不同将内存划分为新生代和老年代，存活周期短的为新生代，存活时间长的为老年代。这样就可以根据每块内存的特点采用适当的收集算法。在新生代对象中进行高频回收，在这次回收中没有被清理的对象移动到老年代对象中，老年代对象执行低频回收。 上面是分代回收的定义，但是python没有简单的把对象分为新和老两个代际，而是分为了三代。每一代的对象达到了一定的数量（Threshold）之后GC会执行相应代际的对象回收，这个阈值是可以通过gc包进行设置的（gc.set_threshold），我调用gc.get_threshold()得到的结果是(700, 10, 10)。不像引用计数，分代回收是可以进行控制的，甚至是关闭。如果觉得GC太频繁造成了性能瓶颈，那么可以提高阈值，降低GC频率。 Golang1. STWGolang的早期版本被无数人所诟病的问题之一就是它的GC，它用的是标记清除方法，也叫Stop The World(STW)。该方法从根变量开始迭代，遍历所有被引用的对象，能够访问到的都标记上“被引用”；之后对没有标记过的对象进行清理，即回收不可达的对象。但是每次执行该算法都会让正常执行的内存负荷型程序出现明显的卡顿，这也是为什么该方法又被叫做STW的原因。 2. 三色标记法该方法是对标记清除算法的改进，原理如下： 起初所有对象都是白色的； 从根对象出发扫描所有可达对象，标记为灰色，放入待处理队列； 从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色； 重复步骤3，直到灰色对象的队列为空。这时仍为白色的对象被当作垃圾回收。 下面是演示图： C++ In early programming languages, developers were responsible for all memory management in their programs. This meant before creating a list or an object, you first needed to allocate the memory for your variable. After you were done with your variable, you then needed to deallocate it to “free” that memory for other users.[2] C++就是early programming language，它较Python和Golang而言更偏底层和接近系统，将内存管理的工作交给程序员来完成。简而言之，就是没有GC（当然后来的C++11、14的新特性在此不提及）我之前写的某程序就因为想要释放整个数组的空间但是只写了delete xxx而不是delete[] xxx，仅释放了数组首位元素的空间，导致了剩余所有元素内存资源的浪费。为了减轻程序员手动管理内存的痛苦，C++11推出的智能指针算是一个宝贝。 参考资料 GO GC 垃圾回收机制 Python Garbage Collection: What It Is and How It Works 如何理解智能指针？ - 知乎","link":"/2019/06/12/GarbageCollectionSurvey/"},{"title":"暑期的生活啊","text":"已经实习四周了，学院安排的实习期已经只剩下一周，但是直到这两天公司 Buddy 才给我安排了一个比较重要且连续的工作，让我有点左右为难：下周之后我到底还去不去呢？之前四周的工作都是延续暑假前所做，多是一些较零散、不系统的碎活儿，其实这样的安排确实容易让人觉得枯燥无味，缺乏意义和价值。“摸鱼”让我觉得内心有愧，如果要做自己的事情，为什么不回学校呢？ 一周五天上班的日子让我分外珍惜周末的两天，因为周中下班回到宿舍已经快晚上 8 点了。这一晚上一般就会在我是要刷题，还是要看一些资讯，还是看 B 站这样的纠结之中过去，第二天有点后悔，但是回来之后又重复前一天的过程。究其根本，我觉得是因为我暑假想做的事情太多了。 实习 刷题 社工 上线小程序 写诸如这篇博客的东西 学习一些奇妙有趣的东西 blabla（我觉得这个引用框好好看 暑假我也更加清晰地认识了我的本质，我对不了解的事物很难提起兴趣，但是一旦喜欢某件事物之后就会开始不断的去了解去接触，就是这样一个循环。所以，我的生活好小啊。 看到手机上给自己的警言：“想的多了，做的就少”，这是高中时期自己给自己打气用的，希望自己能够心无旁骛地做自己该做的事情。但是来到大学之后发现，没有明确的事情是那么的该做了，只有想做的事情比较明确，然而想做的事情太多，在纠结之中又会不经意掉入“想太多”的境地。给自己定一个小目标吧，8 月里把心沉下来，否则这个暑假将和去年暑假一样浮躁。","link":"/2019/07/27/SummerVocation/"},{"title":"Github Actions的基本使用","text":"背景在毕业设计时捣鼓了一会应用的持续集成(Continuous Integration, CI)和持续部署(~ Deployment, CD)，发现确实可以为自己省下很多力气： 不用每次把代码通过 scp 或者 sftp 传到服务器上再 build 运行 也不用在本地交叉编译之后再传到服务器上 之前在公司实习时所有的分支合并都会涉及到 CI 和 CD，当时为了让代码编译通过费了很多心思。虽然自己push代码的时候比较费劲，但是确确实实可以给应用的部署和可用性提供保障。 最近在看阮一峰大神的技术博客时偶然看到了Github Actions的入门基本教程，发现 github 把 CI 脚本商品化、组件化放到 Marketplace 里供用户挑选和使用是一个非常不错的思路，让 github 的开源文化更加吸引人了。我把上面那篇博客看完之后发现其实和 Travis CI 差不多，或者说其实所有的 CI 系统都差不多。抱着接触新事物的热情，我还去看了 github actions 的官方文档，下面对我所了解到的一些信息进行一个汇总。 核心概念Github actions 中有下面几个重要的概念，直接上原文档： Workflow Jobs Steps Actions Runner 1. Workflow 工作流，工作流是由在 git 项目目录中的 .github/workflows/*.yml 文件定义的自动执行程序，可用于项目的构建、测试、打包、发布和部署。工作流又由一个或多个任务组成。 2. Jobs 任务，由一组步骤组成的单位。在工作流文件中可以定义任务如何运行：是并行执行还是顺序执行，以及以什么条件、什么顺序执行。在 Github-hosted 宿主机中，每一个任务都是在一个全新的虚拟环境中运行的。 3. Steps 每一个步骤都是一个单独的任务，可以执行 shell 命令或者是执行 action。一个任务里的所有步骤都是在相同的虚拟环境中执行的，使得不同的步骤可以通过文件系统共享信息。 4. Actions 翻译成动作有点太难听了，还是就叫 Action 吧。Action 是工作流中最小的可移植构建模块，你可以创建自己的 action，也可以使用社区里的 action，还可以对公开的 action 进行DIY。如果在工作流中使用，必须将 action 包含在步骤当中。 5. Runner 我把它叫做宿主机，宿主机分为两种：Github组装的和用户自组装的，他们存在着一些不同。宿主机等待用户的各种任务，一旦宿主机接受了任务，它会执行任务里的 actions，并把运行进度、日志和结果传给 github，用户可以在 actions 页面中查看这些信息。日志最多保存 30 天。 简单使用目标我的博客用的是 hexo 在 github pages 上构建的，博客所在位置是一个 repo，hexo 源码所在位置又是另一个 repo，我希望可以合并这两个 repo。或者退而求其次，我希望可以一次 commit 实现两个 repo 的 work done。这就要求我： 在源码 repo 处启用 actions 进行自动构建 将构建好的 repo 源码推送到 gh-pages repo 当中 在 Marketplace 中寻觅我找了四个公开的社区 action 想进行使用，发现他们存在着各式各样的问题，难怪最多的一个项目也只有 21 个 star。 自己写工作流文件deploy.yml123456789101112131415161718192021222324252627282930313233343536name: deployon: push: branches: [ master ]jobs: build: name: Build runs-on: macos-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v1 - run: npm install - run: npm install hexo -g - run: npm install hexo --save - run: hexo g - name: config git run: git config --global user.email &quot;$EMAIL&quot; &amp;&amp; git config --global user.name &quot;@NAME&quot; env: EMAIL: *** NAME: *** - name: bind github remote run: | cd ./public git init git remote add origin https://$PERSONAL_TOKEN@github.com/$USERNAME/$USERNAME.github.io.git env: USERNAME: fool-wang PERSONAL_TOKEN: $ - name: push public run: | cd ./public git add . git commit -m &quot;auto deploy&quot; git push -f origin master 该工作流每次在我向 master 分支 push 代码时会触发，其中任务只有一个 build，我给他设定的名字也是 Build。runs-on 指定宿主机，在这里因为我的电脑是 macos 系统，所以我希望宿主机可以和我的电脑操作系统相同，所以选择了 macos-latest。下面到了最最关键的步骤，一条一条地解释我做了什么（其实我也知道我的 push -f 很不优雅）： 用了官方的 actions/checkout 来获取 repo 文件内容 用了官方的 actions/setup-node 来获取 node.js 运行时环境，其实主要就是需要 npm 运行 npm install 命令，为项目安装依赖 运行 npm isntall hexo -g 命令，为虚拟环境全局安装 hexo 运行 npm install hexo --save 命令，这一步我非常不解，但是如果没有这一步之后的步骤会报错 运行 hexo g 命令，生成 public 静态页面文件 配置 git 信息，通过环境变量传入我的邮箱地址和姓名信息 通过我的 personal access token 将博客 repo 绑定到生成的 public 文件夹远端 将 public 内的文件强制 push 到博客 repo 中，希望实现文件替换，也就是实现了部署过程 问题，大问题我发现宿主机内执行 hexo g 命令生成的文件与在本地电脑上生成的文件不一致，下面放几张图片： 上图是本地执行命令的生成的文件，下图是宿主机执行命令生成的文件，差别那么大就很离谱。 更离谱的是，博客 repo 上新部署的 index.html 文件内容是空。 结语Github actions 的基本使用倒是已经掌握了，但是没办法实现博客源码自动构建和自动部署是有点困扰我，其中最神奇的问题就是 hexo g 生成的文件不同。不知道这个是我的打开方式错误，还是 github actions 环境有问题，还是 hexo 有问题。（最后一般都是我有问题） Github actions 使用体验不错： 在私有 repo 中也能够免费使用，真心觉得微软爸爸好 任务执行流程、日志和结果呈现十分友好 marketplace 的思路新奇，有许许多多开发者贡献代码，社区活跃度高，今后一定会变得更易用 构建失败后会有邮件提醒 之后有时间还会继续深入了解 actions，争取把今天遇到的问题给解决掉了。 后续hexo g 命令生成的文件与在本地不同是因为…我漏掉了 git clone 模板库的步骤。第二天早上在 hexo g 之前补上了下载模板库的命令之后就正常工作了。熬夜害人熬夜害人。 参考文献 GitHub Actions 入门教程 GitHub Action 官方文档","link":"/2020/05/09/HowToUseGithubActions/"},{"title":"论文阅读: Role-Based Access Control Models, 1996","text":"摘要论文描述的方法简称 RBAC，基于角色的访问控制，是目前非常主流的访问控制方法。用户（User）、角色（Role）和权限（Permission）是论文中最主要的三个概念。角色和用户组这个概念很像，但是用户组仅仅只是一个用户集合，而角色则联系着用户和权限。说到底都是为了让用户获取权限来执行相应的操作，不管是加入角色还是用户组，都只是一个中间态。 RBAC0 是基础模型，是任何系统实现 RBAC 的最低要求。RBAC1 和 RBAC2 都包含了 RBAC0，但是分别加入了不同的特性。RBAC1 加入了角色层级特性，即角色 A 可以通过继承角色 B 获取角色 B 拥有的权限。RBAC2 加入了约束特性，控制用户对 RBAC 中不同资源、组件的权限。而 RBAC3 包括了角色层级和约束。示意图如下所示，其中基础模型 RBAC0 是实线部分。 文章并没有给出模型具体的实现方案，这些模型主要用作产品开发的指导准则或设计原型。 RBAC 模型族RBAC0基础模型主要由用户 U，角色 R 和权限 P 三个实体集组成，图表中也表示出了会话 S。用户代表使用系统的每一个“人”；角色代表负责一系列职责的人群，用职业或者岗位来理解；会话是将从用户到多个角色的映射。权限需要详细说明。 权限是访问一个或多个数据对象或其他资源的许可证，权限通常是正向的，即拥有权限后可以访问对应的对象。但是有的模型把权限设计成为负向的，即获得这个权限后将无法访问对象。RBAC 模型中将负向权限建模成为“约束”，后文会提及。权限的含义完全取决于系统的类别和系统的实现细节。操作系统中区分了文件、目录、设备和端口等对象的读、写和执行操作；数据库对关系、元组、属性和视图等对象的查询、更新、删除和插入操作进行了区分。 图 1 还展示了用户分配 UA 和权限分配 PA 关系，它们都是多对多关系。RBAC 的关键就蕴藏在这两个关系当中。RBAC0 中各组成部分为： U, R, P, S，分别代表用户、角色、权限和会话 PA，权限分配，权限到角色的多对多关系 UA，用户分配，用户到角色的多对多关系 _user_， 从一个会话映射到一个用户的函数 _roles_，从一个会话映射到一个角色集的函数 RBAC1RBAC1 在 0 的基础上加入了角色层级，这个特性在现代组织管理当中显得很自然。层级高的角色继承层级低的角色的权限，如 Team Leader 可以访问所有项目的细节而 Project Programmer 只能访问所属项目。 从离散数学的角度来说，角色间的层级是偏序关系，即满足了自反性、传递性和反对称性的关系。 私有权限或私有角色通常用于控制角色继承时的权限作用域，如对进展中的为完成工作访问权限如果从低级角色直接继承到高级角色似乎不是特别合适。如 Team Leader 不应总是收到每一位 Programmer 的 CI Build 失败通知。 RBAC2RBAC2 在 0 的基础上加入了约束特性，这个特性有时被称作 RBAC 的灵魂。约束是是一个强力的机制，可以用来打造更高水准的组织管理架构。下面是文章介绍到的一些常见约束： 互斥约束，互斥顾名思义就是用户只能获得其中一个角色身份，或者同样的权限只能赋予其中一个角色； 基数约束，一个角色最多只能拥有一定数量的成员； 先需角色约束，给用户分配角色 A 之前需要该用户拥有角色 B 的身份； 角色层级约束，没错就是 RBAC1 中的角色层级，它也可以被看作是一类约束。 RBAC3RBAC3 就是把 1 和 2 组合起来，既有角色层级又有约束条件。 基于 RBAC 的管理模型上面提到 RBAC 模型当中所有组成部分都是由一个安全管理员直接负责，这样的情况只有在极小的系统中才有可能存在。任何稍大一些的系统都有多个安全管理员。由于 RBAC 的主要好处就是实现了权限的管理，那么如何让 RBAC 来管理 RBAC 本身呢？论文提出的管理模型在图 4 中展示，其上半部分就是之前的图 1(b)部分，其下半部分是上半部的镜像，其中包含了管理角色 AR 和管理权限 AP。权限只能授予给角色，管理权限只能授予给管理角色。 RBAC 模型中的管理权限可以视作修改用户分配 UA，权限分配 PA 和用户层级 RH 关系的能力。这样就可以实现用 RBAC 模型管理自己了。 参考文献[1] Sandhu R S, Coyne E J, Feinstein H L, et al. Role-based access control models[J]. Computer, 1996, 29(2): 38-47.","link":"/2020/05/18/ReadRBAC/"},{"title":"当我写H5时，我到底在写什么？","text":"背景“活动宣传写个H5就可以了”，”学生节不如做一个H5小游戏吧，比如之前那种学堂路躲避乌鸦“……在这几年的学习和生活中我听到过很多次H5，也真正看到过、使用过、制作过H5，但是我对H5这一个词的真正定义还是不太了解。它给我的直观印象就是在手机上运行的、包含许多动画甚至音乐的纯前端/弱后端页面，最常见于微信群、公众号和朋友圈，或许这也是大多数人对其的印象。对自己接触的东西一点也不了解是不能接受的，所以要简单调查调查。 H5的由来几种定义 h5是HTML的第5级标题标签； H5是在手机上运行的、包含许多动画甚至音乐的纯前端/弱后端页面； h5是2014年10月由W3C制定的HTML新一代标准，其中包含了新标签、新属性、多媒体和本地存储等特性。 H5为什么火？H5是HTML的第5代标准，它不是一个新的应用，不是一个编程语言，甚至不是一个实体(Web应用、微信小程序等)，他就是常规意义上的HTML新版本，和C++17、C++20差不多。HTML等Web应用相关语言和Golang、C++等语言还不一样，它们的标准还需要各大浏览器的运行时支持才算是“真正”拥有了新特性。但是如今大多数用户都能够很方便地通过电脑、手机接触到Web应用，其中不乏动画酷炫、插画精美、音乐好听、让人眼前一亮的应用，开始有人将这样的应用叫做H5，后来逐渐传开再也无法很好地定义了。 普通用户是不会去了解H5到底是什么的，隔行如隔山，弄明白这个对用户来说作用不大。据我观察，各大H5应用模板网站也不会强调这个H5到底是怎么做的，而是统一作为模板打包卖给用户就行。H5可能是纯前端的，也有可能有弱后端，但是“注重呈现”应该是H5的核心，有“动态海报”内味儿。接下俩的H5统一取第二个定义。 H5的组成 H5也是Web应用，那么就离不开HTML、CSS和JavaScript，HTML负责文档结构、CSS负责视觉样式、JS负责业务逻辑，在写H5的时候其实是在写这三种语言。HTML5中的如&lt;canvas&gt;、&lt;audio&gt;、&lt;video&gt;和&lt;svg&gt;等新标签确实为前端的图形渲染提供了良好的载体，但是应用的代码组织与以前差别并不是很大。如今许多浏览器都支持HTML5标准，许多前端程序员也在使用HTML5标准提供的新标签，那么可以说现在只要是写前端都是在写H5。 SPA - 高级H5单页面应用(Single Page Application)的说法最近几年非常流行，前后端分离的开发风格、Web技术和云计算的发展使得浏览器中运行的Web应用无论是开发流程还是使用体验上都越来越接近原生应用(Native Application)。许多SPA的功能全体量大，不输给原生应用。HTML5中提供的本地存储(localStorage)特性相信许多开发者都用过，使用它可以构建相当强大的功能。那么把它叫做高级H5也不过分吧？（doge 另外，如Vuejs、Reactjs等在内的前端开发框架也简化了SPA的开发。 我以前接触过Vuejs和微信小程序，当时觉得微信小程序是大部分借鉴了Vuejs的精髓。前一阵子又看了看Reactjs的文档，发现微信小程序的架构里也能发现Reactjs的影子……虽然React有facebook官方的支持，但是有一说一它的中文文档是真的不行，过时的文档google出来结果竟然还在前列。相比之下Vue虽然没有大公司支持，但是越来越赢得开发者的青睐，其文档的完善也是一大因素吧。","link":"/2020/06/07/WhatIsH5/"},{"title":"为什么我要写博客？","text":"内外因素我喜欢记录我很喜欢记录，就像首页那句话说的“有着记录的想法，没有记录的时间”一样：我喜欢把生活里有趣的、激发自己思考的事物都记下来，倒不是为了写给谁看或者写给未来的自己看，光是写下来的过程会让我觉得很好，不知道怎么用语言形容的一种“满足感”；但是在学校的时候事情实在是太多了，学习、社工、项目还有各种琐碎的事情让我根本没有时间把所想所见记录下来，所以也就有了这个博客里年更的情况出现了。 2019 年的 12 月 31 日，我在自己的非主流 QQ 空间发了一篇名为《别了，我的一零年代》的日志，回顾了一下自己的中学和大学本科时期的得与失，2020 年年底实在是太忙了，没能够再写一篇“年终总结”。不过现在想起来也不是很遗憾，因为 2020 年不是很能激起我的记录欲。好！在！已经研一，兵荒马乱的研一上学期也已经度过了，至少未来一年半内不会有太多课程上的压力，这样可以让我有更多连续的时间能够用来记录。这是让我非常开心的，我猜我今后会多写一些东西在博客上吧，应该会。 我应该记录“学计算机的人应该有一个博客”是我从大学入学时就有的一个想法，这样朴素的情感大概是在我拜读了阮一峰和廖雪峰等大神博客之后就在心里扎根了的。确实，不管是大一入学、大二做课程大作业、大三在校外实习，通过查询和浏览博客是我解决特定问题的主要方式。所以我觉得自己应该也要成为这样的一个“博主”，虽然没有特别亮眼的技术实力，但是通过写博客督促自己不断的学习也是一件足够酷且有益的事情。 还有一件事情值得一提，在写《Github Actions 的基本使用》时其实我是边学边写，一开始觉得有个地方是 GitHub Actions 的 Bug ，迫于无奈和夜深，发布博客之后就赶紧溜去睡觉了。第二天回看博客的时候一眼就发现了问题所在，这样的特点也从另一方面激励我继续写博客。 “记录”对我是有好处的。 近期的一点牢骚清华的竞争氛围太强了：每个人都是高中的佼佼者，到了大学有的人依旧游刃有余，有的人就力不从心了，不管排名保持高中的状态还是变好或变坏都是非常正常的事情。找准自己的定位和方向，保持努力就可以了，没有什么比热情更重要。在知乎上经常能看到“清华 XX 系大四了还一事无成怎么办”这样的问题，如果要我回答的话我确实无法组织出一段话来回答这样的问题，在提问者的上下文里我其实也是一事无成的。 跟人比较确实上头，确实会增加焦虑。可是我不会把自己推到这样的焦虑当中，我不喜欢跟别人比，我只希望自己可以充实和从容，不用妄自菲薄，也不要自以为是。自己只要保持这样的心态和步调就不太在意身边的压力了。 之后写点什么之前有的两个标签是“学习”和“瞎写”，现在觉得“瞎写”有点莫名其妙，之后打算换成“随笔”。有考虑增加一两个标签（例如“游记”和“相册”等），也希望自己能够维持“学习”和其他标签博客的比例。 最后，希望 2021 年一切都好。 迟到的新年小作文","link":"/2021/01/16/WhyBlog/"},{"title":"聊聊微信小程序及其框架","text":"为什么是微信小程序微信很早就有一套专用的JS-SDK在微信客户端上面使用，其开放了录音、二维码、地图、支付等几十个 API，能够支持微信服务号的运转，当时大部分支付、扫码等功能的对接方都是这样的服务号。但是在 2016 年 1 月 11 日微信之父张小龙表示服务号还不够优秀，微信正在研究一个新的服务形态，起名叫做“微信小程序”。在 2017 年 1 月 9 日，第一批微信小程序低调上线，而选择这一天也是为了向 iPhone 1 代的发布致敬。 很快，微信小程序依靠微信的庞大用户量迅速的占领了市场，其他各个大厂见势相继效仿，但是从目前来看它们都很难和微信小程序再抗衡了——不管是用户数量还是开发者数量。这倒也不太奇怪，这种行业往往都是快鱼吃慢鱼不是大鱼吃小鱼，emmm…不过背靠腾讯的微信也不能说是小鱼吧。 为什么要写微信小程序首先想说的是，微信小程序的生态是比较混乱的，开发起来有一点难受，有不少情绪比较激动的开发者直接说微信小程序就是“s**t”，我也能够理解其中的一大部分感情。 我从大二参加微信举办的第一届大学生微信小程序大赛到现在已经有 3 年了，这期间虽然不是一直在开发小程序，但是每年都会或多或少因为课程或者是项目的原因接触小程序开发，所以对微信小程序还是有一定的了解的。 小程序非常不好写。我觉得有以下几个方面的原因: 一个页面(Page)拆成四个文件这样分散的组织形式让我觉得有点难受，有的人可能会说这是关注点分离，但是我觉得 Vue 在单文件内拆成多个部分的形式可能更好一点，一个重要的事情值得注意，关注点分离不等于文件类型分离。而且你说一个叫做page的页面，它下面的四个文件名是叫page.wxml好还是叫index.html好呢？ 微信开发者工具非常不好用，首先它很占内存，内存小的机器上运行微信开发者工具一段时间后会出现奇奇怪怪的问题，因为遇到的次数有点多也没有特别记得，在这里也说不出来了。 因为在微信的生态里面，处处都要依照微信的规矩来进行开发或者运维（这倒也没办法），用某某接口需要是某某类目的小程序，需要有某某资质的主体等等……申请接口、申请上线等种种流程都十分繁琐。 官方文档有些混乱，而且最近两三年内没有进行过较大的更新改进。 但是有的时候又不得不写它，因为它对用户更加友好。如果换作是我的话，我也不会为了去餐厅点餐或者是在奶茶店买奶茶专门下载一个 App，为了扫健康码就更不会。微信小程序就是为了替代微信服务号这样的“用完就关”的应用场景的，开发的时候把它当作微信服务号 2.0 来开发就好了，不要提高自己的心理预期以为自己是在开发一款 App。这样想的话，开发起来就会好接受多了。 小程序框架浅析逻辑层和渲染层 微信小程序通过微信客户端(Native)这一桥梁向外界服务器发送请求和接受响应、调用手机本地接口(拍照、上传文件等)。小程序分为了渲染层和逻辑层，小程序页面的 WXSS、WXS 和 WXML 运行在渲染层，而 JS 运行在逻辑层。WXS 是专门给小程序推出的一个脚本语言，是 JS 的子集，运行在渲染层执行一些简单的数据处理任务，据官方文档说使用 WXS 在 Android 上没有太多性能提升，但是在 iOS 上用其执行相关任务能加速 2~20 倍。 微信小程序的渲染层与逻辑层也通过 Native 进行通信，比如渲染层将触发的事件传输到逻辑层，逻辑层将更新的数据传输到渲染层等等。可以看到上面这张图里渲染层分了很多个 Webview，其中每个 Webview 都代表了一个小程序页面。 WXML 文件其实是标识了页面的元素及其相互关系，在微信小程序的编译过程中，WXML 文件会被编译成为 JS 对象用来在渲染层维护一个虚拟的 DOM 树。通过与逻辑层的数据进行组合，形成一个数据和结构都完整的虚拟 DOM 树用以渲染。每次在逻辑层调用 setData 方法时，逻辑层都会将这个消息传输到渲染层，渲染层通过对比发来的数据与之前的数据，将有差异的数据应用到 DOM 树上，从而进行更新渲染。 目前市面上有许多的小程序开发框架，如 Taro、Uni-App、mpvue、WePY 等，有实验显示使用小程序框架进行开发会使得小程序性能提升，其中很大一部分原因就是开发框架会对 setData 的调用进行优化，减少逻辑层和渲染层的实际通讯次数，从而提升性能。 而采用渲染层和逻辑层的架构主要是为了阻断页面渲染和逻辑处理，从而加强监管、提升性能。 小程序页面生命周期 从微信小程序官方文档上面的生命周期图示(方便起见，我把纵向的长图拆成了左右两个部分)可以看得很清楚，逻辑层与渲染层分别进行初始化。 逻辑层执行完 onLoad 和 onShow 两个生命周期函数之后，等待渲染层初始化完成的通知； 逻辑层收到通知之后，将初始数据传输给渲染层，渲染层拿到数据进行首次渲染之后再次通知逻辑层，让其执行 onReady 生命周期函数； 执行完 onReady 函数之后，小程序就处在 Active 状态了； 如果小程序在激活状态下被最小化到微信的后台或从后台被唤起，则会调用 onHide 和 onShow，如果在激活状态被关闭，则会调用 onUnload (一般不会用到)。 小程序从后台被唤起时的启动叫做热启动，第一次打开或者距离上一次打开已经过去了足够长的时间时叫做冷启动。他们进入页面生命周期的位置不同，开发的时候需要格外注意一下。 我自己的小程序开发方法开发微信小程序有许许多多的方式，最原始质朴的方式就是直接使用微信开发者工具进行编码和调试，这是我在大二的时候使用的方法。我已经很久没有用过这种方法了，但是它至今仍然给我很不好的回忆，究其根本主要是因为 IDE 实在是太难用了。 后来使用 WebStorm 来进行微信小程序编码，但是发现我自己的笔记本电脑(18 年老电脑了)同时带 WebStorm 和微信开发者工具两个“重型” IDE 实在是有点吃不消，遂作罢。 后来接触到了 gulp 这个前端工程化工具，可以让编码过程更加自由，尤其是能够使用 sass 等 css 预处理器，让我觉得开发起来清爽了很多。目前我是用VS Code进行编码，同时使用了minapp, Live Sass Compiler这两个插件，能够原地使用 sass，也没有增加过多的复杂性，很符合我自己的“编码哲学”hhhh。 我在写代码的时候一直觉得奥卡姆剃刀原理是真理，可以不要的就一定不要。这样的想法使得我在之前的开发中从没有想过用小程序开发框架，“一次编码自动构建多个平台的小程序”，我需要开发的只有“微信小程序”这一个而已，不需要增加那么多的复杂性。 但是最近越来越受不了小程序复杂的设计了，尤其是写惯了 Vue 再来写小程序，简直就和降智了似的。 之后我应该会看一些小程序框架的文档，先从 Uni-App 和 WePY 这两个开始吧。Uni-App 是用 Vue 语法开发小程序的框架，支持一键生成多端小程序。而 WePY 则是微信官方推出的小程序开发框架，仅支持微信小程序。这也算是立了 flag 吧。","link":"/2021/01/21/WechatMiniprogram/"},{"title":"小程序框架：WePY、uni-app 和 Taro","text":"在上一篇博客《聊聊微信小程序及其框架》里面立了个调研微信小程序开发框架的 flag ，这篇博客就来填这个坑——我迫不及待地想要掌握一个能够“舒适编写”小程序代码的框架。 我之前提到我最想先要了解的是 WePY 和 uni-app 这两个小程序框架，WePY 是微信官方出品的小程序框架，uni-app 是使用 Vue 开发小程序的最火的小程序框架，但是这两个框架都让我特别失望。 令人失望的 WePY 和 uni-appWePY其实最期待的应该是微信的亲儿子 WePY 吧，毕竟是官方出品，质量应该会有所保障，但是看了文档并自己安装运行之后发现还是不对胃口。原因主要有以下几个方面： 语法以 Vue 为基础但是又在 Vue 的基础上进行了一点魔改，让人产生一定程度的混乱。如果要使用类 Vue 的语法来写小程序，为什么不直接用 Vue 来写呢？ 插件支持不到位，例如 vscode 中的插件 Wepy 就是完全用 Vetur 魔改出来的，在 .wpy 文件中的提示和各项支持并不到位，会提示有各种 warning 和 error； 使用 @wepy/cli 工具创建标准模板项目之后发现并不能正常编译。 第一印象很重要，如果这个框架第一时间没有让我感觉到便利，甚至是让我觉得很麻烦，那我是绝对不会再继续在继续研究它的。到这里，WePY 的尝试就结束了。 uni-appuni-app 是由 DCloud 开发的一个小程序框架，致力于让开发者用 Vue 写一套代码编译到 10 个平台都能运行。虽然一开始对于这种为系统引入过多复杂性的框架有些抵触，但是最简单的 WePY 凉了，所以也愿意相信 uni-app 说的: 即使不跨端，uni-app 也是更好的小程序开发框架、更好的 App 跨平台框架、更方便的 H5 开发框架。不管领导安排什么样的项目，你都可以快速交付，不需要转换开发思维、不需要更改开发习惯。 知乎上也有相关的帖子，说 uni-app 比较香，写起来很省心。抱着尝试的心态看了一下 uni-app 的官方文档、用 vue-cli 安装了 DCloud 官方的项目模板尝试编译运行、下载了 HBuilderX 尝试编译运行。运行是能运行，但是效果马马虎虎，体验有点糟糕，印象大打折扣。主要的问题有以下几点： 官网文档有些混乱，非常非常非常着重的说明如何进行跨端的适配、条件编译有何注意事项等等，对于只想简单在单一平台编译运行小程序的开发者来说不够简洁； 命令行工具的表现和 HBuilderX 的不一致，大部分情况下命令行下都编译运行不了，或者是当我想引入一些如 Vant 的组件库的时候小程序就编译错误； 上一条提到的部分问题如果使用 DCloud 的官方 IDE HBuilderX 的话就可以用“导入插件”的方式解决一部分。但是体验非常非常差，简直就是道德绑架——要编译就要用我的 HBuilderX。什么？你没安装啊，那打扰了； 尝试使用官方推出的插件将使用原生微信小程序语法编写的代码转换成 vue 风格的 uni-app 代码，但是转换出来之后一团糟，根本没办法运行。 uni-app 最大的问题在于它太复杂、太希望构建自己的生态、太希望牢牢抓住开发者了。复杂的东西往往都很脆弱，经不起折腾，稍微碰一下就错误百出。它几乎是强制开发者使用他们的 HBuilderX 来进行小程序开发，这一点我无法接受，你不让我自由选择开发工具，我就可以不用你。但是这些问题也情有可原，毕竟它是 DCloud 的主要业务之一，需要给公司带来一定比例的收益。 Taro 宇宙最强的泰罗奥特曼Taro 文档阅读我一直觉得很多开发者都非常可爱，从起名上就可见一斑，比如 Taro 这个小程序跨端框架、分布式应用程序协调服务 ZooKeeper 和分布式系统基础框架 Hadoop 等。Taro 是京东的凹凸实验室推出的小程序跨端框架，和 uni-app 一样支持写一套代码编译到不同的地方。目前已经进入了 3.x 阶段，除了可以用 React/Nerv 进行开发，还支持了 Vue/Vue3。我马上看了看 Taro 的官方文档，比 uni-app 好太多了：快速开始、基础教程、进阶指南、迁移指南层层递进；官方组件库和 API 列表罗列清楚，对它们在 React 和 Vue 中的使用方法说明得很清楚；还提供了专门的“教程”模块，对于刚上手的开发者十分友好；更重要的是，它提供了一套原生的 UI 组件库 taro-ui ，甚至还照顾了 Vue 的版本推出了 taro-ui-vue。依靠这些了解，我就觉得以后就是它了，要是再有新的小程序项目，我应该会首选使用 Taro 进行开发。 尝试初始项目于是，我赶紧创建了 taro-taste 文件夹。 本来打算随便写一个 demo 小程序，又不想再单独写一套后端代码（再简单也懒得弄了），就用之前申请的一个小程序用来作为云开发的基础。用 taro 自带的 cli 工具初始化了一个微信小程序的云开发模板。用微信开发者工具打开之后，一切表现都很正常，没有什么复杂的事情，没有类似 uni-app 一样的满屏 error，云函数也能够正常调用。但是用 Vue 开发小程序有一些其他的点是需要重新适应的，例如： template 部分的基础元素只能用 view 而不能用 div； 事件名称或许不一样，如点击事件需要用 tap 而不用 click 等； 安装 taro 官方的 package 最好改用淘宝的源或者直接使用 cnpm ，不然就会出现安装失败的问题。 不过整体而言已经很满意了，对 Vue 的支持也比较全面，之后应该会真正用它来进行开发吧。 不如了解一下新东西但是有一点比较奇怪的是，在 Vue 项目模板的首页文件中，根节点 view 元素有个 className=&quot;index&quot; 的属性，这是 React 类名的写法。想到 Taro 在 3.x 版本以前只支持 React/Nerv 的情况，出现 className 这个属性应该是 3.x 版本目前还没有把对 Vue 的支持做得很完善吧。 恰好我最近在做其他项目开发的时候感觉 Vue 在代码复用、数据更新和访问方面的一些体验不是特别友好，而且组件文件规模很难控制（这应该是我的编码水平问题…），不如去学习尝试一下 React 吧。听说 Hooks 配合函数组件用起来很舒服，而且 React 完全是在写 javascript/typescript，开发起来应该会比一个文件里写 template/script/style 来回切会容易把控一点吧。 当然，它们归根到底只是开发框架而已，前端项目开发万变不离其宗，但是了解一个很酷的新东西本身就可以给我很大的动力，对吧？","link":"/2021/03/14/MiniprogramFrameworks/"},{"title":"从 Vue 到 React —— 第一印象","text":"学习 React 的初衷之前说过想要学习 React，秉持着边学边记录的想法，我随即开启了这篇帖子。 从 Vue 说起我是一个 Vue 的忠实粉丝，虽然没有全部读过 Vue 的源码，但是对它的基本实现原理和大体的使用方法还是比较熟悉的。我自己思考过，我觉得我喜欢 Vue 大概率是因为我最早接触的前端开发框架就是 Vue。记得是 2018 年参加一项 SRT 的时候，为了开发一款简单的后台管理系统，我开始学习 javascript 以及 Vue。 当时我还是第一次接触脚本语言，对没错，我接触 javascript 比接触 python 更早一些。熟悉了 C/C++ 的继承机制之后，突然要接受 javascript 的原型链继承一时间有点缓不过来。但是后来还是被磨平了棱角，被迫接受了原型这一设计。为了开发 Web 应用通过一位学长的介绍接触到了 Vue 这个框架。我直接惊为天人，还能这样写？因为根据我更早之前的一些浅显的印象，Web 开发是分别要编写 HTML，javascript 和 CSS 三种文件的。现在用 Vue 一个文件就可以生成一个完好的页面，着实非常酷炫。也是在 Vue 这里，我了解到了组件、生命周期、全局状态管理、前端路由等等一些重要的概念，所以先入为主地对 Vue 有强烈好感也正常吧。 Vue 的好处就不说了，这里主要想说以下我遇到的问题： 在写 Nuxt 项目写到功能比较重的组件时，一个组件 Vue 文件代码可能会到 800 ~ 900 行，写完 template 后滑滚轮滑到 script 部分找到 methods 里对应的地方写函数，如果模板部分出问题了还得滑回去。文件行数一旦长了，这个上下滑动找代码块的过程真的有点难受，一旦思考过程中出现一小段的空挡，那么就有可能导致思绪完全断掉。 另外，Vue 的代码复用方案我觉得不是特别好：模板方面的槽和脚本方面的混入这两个我觉得不是特别优雅，尤其是混入。混入的文件一旦多了可能会造成组件难以维护的问题，例如不知道模板中使用的某个属性到底是 mixin 中来的还是从 data 或者 props 中来的，如果出现覆盖那么覆盖规则、覆盖后的值又是什么。 插件支持我觉得也没有做得特别好，虽然 Vetur 已经做到相当好了，但有一些情况仍旧是解决不了的，比如上面说的 mixin，其中的属性或者方法就没有办法在调用的页面当中提供补全提醒。 最后，因为总是遇到一些不可名状的 Bug，我打算以后项目的开发要用 typescript，但是 Vue 对 typescript 的支持不是特别好（听说 Vue 3.x 版本有所改善，但是看了一下 Vue3 的文档觉得 composition API 和 React 实在是太像了，不是特别感冒），所以有些打退堂鼓。 总的来说，Vue 的问题不是特别大，我也没有在做很大型很复杂的项目，上面说的小瑕疵不是本质问题（有问题应该也是我自己能力还没到位）。但是总让我觉得有可以更好的地方。 React 哪里吸引我React 是 Facebook 推出的用于构建用户界面的 Javascript 库，应该是目前最热门的“前端框架”，加上引号是因为我觉得它其实不算是一个框架，只是一个“库”，扩展了 js/ts 的语法特性，使它们可以更方便地写 Web 应用。 它很火，看着也很酷，更原汁原味，更轻量化，更自由，有更多的可能性，这应该就是我想了解它的原因吧。 对 React 的第一印象读了一下官方的入门教程，React 给我印象最深的有下面几个方面： 完全使用 js/ts 编写，没有增加新的文件类型，在 vscode 中编码体验良好（jsx/tsx 不太算新类型吧）； state 与普通属性区别开来，如果需要更新需要显式的调用 setState 方法，虽然牺牲了一定的灵活性，但是也一定程度上促进了数据的安全访问；同时，这样近于“严苛”的 state 更改方法会鼓励开发者将组件拆分成更小的部分； 提供函数组件和类组件两种写法，函数组件的写法可以省很多空间； React Hooks 看起来像是 Vue Composition API 的原型，函数组件用起来会比较“优雅”； JSX 渲染函数的写法可以将实现某功能项的代码尽可能的收缩到同一个空间区域，拥有更好的空间“局部性”，免于在 template 和 script 之间来回切换打断思路的困扰； 阿里的 Ant Design 为 React 前端开发提供了一个很好的组件库和设计规范，不得不说，阿里对优化用户体验真的有很深入的研究。 之后探索的方向但是其实前端框架归根结底做的事情都是一样的，是让开发者能够较为轻松地开发出易于测试、易于维护、方便拓展、体验良好的跨平台的 Web 应用。不管是之前写 Vue 代码还是之后要写的 React 的代码，都是为了这样一个同样的目的。然而如果仅仅光想着实现功能而不去思考更多，很容易陷入为写代码而写代码的陷阱当中。 因为我在实验室课题组里负责了几个项目，几乎每个项目都是不超过3人的小团队。如果开发周期稍微变长一些，项目的功能就会变得异常复杂，而作为开发者，在测试的时候很容易就会局限在正常（或者说正确的）业务逻辑当中，不会发现一些匪夷所思的问题。（感觉真的很难让一个要写正确逻辑业务代码的开发者去做一些极不符合预期的事情。）测试很重要。只要不是自己小打小闹的学习性质地写着玩儿，测试就是极其重要的。测试保障的是软件的质量，在任何有甲方的项目当中，没有进行详尽测试都是不负责的。在写后端代码的时候，测试起来比较容易，因为提供的 API 是可预期的，传入什么样的参数，返回什么样的响应，都是清清楚楚的。至少对于开发者自己来说，后端代码都是白盒，将单元测试覆盖率提高就可以显著地提高代码的可靠程度。那么前端测试呢？ 前端测试中主要有单元测试、组件测试和端到端测试。单元测试其实并不区分写的是前端还是后端的代码，都是追求更高的代码覆盖率。而组件测试和端到端测试可能对于前端应用来说更重要吧，因为前端应用给用户操作的组合是无限的，用户可以选择以任何路径，触发页面任何元素可能的动作，想要详尽地测试属实比较困难，但是组件测试和端到端测试应该都是比较成熟的测试方案，能够在一定程度上提高应用的可靠程度。 之后除了继续接触 React 之外，我还会去了解一些组件测试和端到端测试的最佳实践，扩充一下自己的前端知识储备。","link":"/2021/03/27/Vue2React-1/"},{"title":"ESLint 入门","text":"背景最近一两年写前端项目时一直都有接触 ESLint，很多文档和博客也一直都推荐使用开发者 ESLint，但是一直以来都没有好好地学习过它。最近因为使用 Nuxt 开发时 ESLint 缓存出问题导致浪费了半个小时，我越发觉得有必要深入地了解一下这个前端开发中最常使用的代码风格规范工具了。（不得不说，Nuxt 这个框架真的有点难用。） ESLint 的用途和初衷ESLint 是在 ECMAScript/JavaScript 代码中识别和报告模式匹配的工具，它的目标是保证代码的一致性和避免错误。维基百科上这样解释的：lint, or a linter, is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. 就是说 ESLint 是写 javascript 时用来分析静态代码是否存在语法错误、代码风格错误和可疑结构的工具。 ESLint 的配置文件配置文件基础配置文件可以使用 .js, .yaml/.yml, .json 格式的文件和 package.json 中的 eslintConfig 属性来定义。读取的优先级如下： .eslintrc.js .eslintrc.cjs .eslintrc.yaml .eslintrc.yml .eslintrc.json package.json 注：eslint 只会读取优先级最高的一个配置文件。 最重要的 rulesESLint 的配置文件通过配置检查规则来给代码做静态检查，rules 对象中的键值对都代表一个规则，规则的值是一个数组，数组的第一个值是规则的检查力度，从 0 到 2 代表检查的力度越来越严格，0 代表不提示，1 代表给出 warning，2 代表给出 error。也可以直接用力度单词表示，如”off”, “warning” 和 “error”。数组第二个及以后的值表示的是传给该规则的参数。 下面给的配置文件片段规定在项目中 1. 如果使用了分号则报错；2. 引号只能用双引号，使用了单引号就会报错。 .eslintrc.js123456module.exports = { rules: { semi: ['error', 'always'], quotes: ['error', 'double'], },} 共享规则库另外，ESLint 还可以使用其他共享规则进行扩展。在配置文件中使用 extends 即可，如下面的一行配置项表示引入了 eslint 官方推荐的规则。（官方非常推荐使用这个规则库） .eslintrc.js123module.exports = { extends: 'eslint:recommended',} 当然有 recommended 就有 all, 但官方不推荐在生产环境下使用 all 这个共享库，因为其中的核心规则会随着版本的变化而改变，会导致预料之外的情况。 引入第三方规则库会有三种效果：1. 打开相应的规则；2. 改变检查力度但是不修改其它选项；3. 直接覆盖原本的规则。 使用插件eslint 的插件其实就是一个 npm 包，可以给 eslint 提供包括但不限于加入新规则和导出共享规则的功能。如下面的配置文件就引入了 react 的 eslint 插件。 1234567module.exports = { plugins: ['react'], extends: ['eslint:recommended', 'plugin:react/recommended'], rules: { 'react/no-set-state': 'off', },} Glob 模式匹配eslint 运行后会默认在指定的目录下对所有的 .js 文件进行扫描，如果在配置文件中使用了 overrides 参数，则可以对需要进行检测的文件进行指定。如下面的配置文件则指定扫描 bin/*.js 和 lib/*.js 并排除了所有的测试文件。 1234567891011121314module.exports = { rules: { quotes: ['error', 'double'], }, overrides: [ { files: ['bin/*.js', 'lib/*.js'], excludedFiles: '*.test.js', rules: { quotes: ['error', 'single'], }, }, ],} 使用配置注释在文件的开头使用 eslint 开头的注释语句可以控制 eslint 对该文件的检测行为。在配置文件中如果配置了 noInlineConfig 属性为 true 的话，就不能使用文件内注释的方式进行配置了。下面摘抄几个配置注释的例子： 123456789101112131415161718console.log('1. 整个文件不检查 ===， 使用分号会报错')/* eslint eqeqeq: &quot;off&quot;, semi: [&quot;error&quot;, &quot;always&quot;] */console.log('2. 在 -- 之后给出一些说明')/* eslint eqeqeq: &quot;off&quot;, curly: &quot;error&quot; -- Here's a description about why this configuration is necessary. */console.log('3. 在此文件中直接禁用/启用 eslint')/* eslint-disable *//* eslint-enable */console.log('4. 整个文件中禁用某些规则')/* eslint-disable no-alert, no-console */console.log('5. 下一行禁用某项规则')// eslint-disable-next-lineconsole.log('6. 这一行禁用规则')console.log('foo') // eslint-disable-line 常用配置项摘自另外一篇博客。 123456789101112131415161718192021222324252627282930313233module.exports = { 'no-var': 'error', // 要求或禁止 var 声明中的初始化 'init-declarations': 2, // 强制使用单引号 quotes: ['error', 'single'], // 要求或禁止使用分号而不是 ASI semi: ['error', 'never'], // 禁止不必要的分号 'no-extra-semi': 'error', // 强制使用一致的换行风格 'linebreak-style': ['error', 'unix'], // 空格2个 indent: ['error', 2, { SwitchCase: 1 }], // 指定数组的元素之间要以空格隔开(,后面)， never参数：[ 之前和 ] 之后不能带空格，always参数：[ 之前和 ] 之后必须带空格 'array-bracket-spacing': [2, 'never'], // 在块级作用域外访问块内定义的变量是否报错提示 'block-scoped-var': 0, // if while function 后面的{必须与if在同一行，java风格。 'brace-style': [2, '1tbs', { allowSingleLine: true }], // 双峰驼命名格式 camelcase: 2, // 数组和对象键值对最后一个逗号， never参数：不能带末尾的逗号, always参数：必须带末尾的逗号， 'comma-dangle': [2, 'never'], // 控制逗号前后的空格 'comma-spacing': [2, { before: false, after: true }], // 控制逗号在行尾出现还是在行首出现 'comma-style': [2, 'last'], // 圈复杂度 complexity: [2, 9], // 以方括号取对象属性时，[ 后面和 ] 前面是否需要空格, 可选参数 never, always 'computed-property-spacing': [2, 'never'],} 命令行工具使用 eslint 的同名命令行工具可以对代码进行分析并将分析报告输出，其主要的使用方式是 eslint [options] [file|dir|glob]*，即最终的分析对象可以是文件、目录和 glob。可以通过选项对工具的行为进行自定义，下面总结一些比较重要的命令行工具选项。 --ext: 指定检测的文件拓展名，eslint 将在给定目录中对指定类型的文件进行扫描检测。如果不给出该选项，默认是 *.js； --fix: 修复检测出来的问题，但是有时候给了这个选项也修复不了相应的问题，这时候就得手动修改相应位置的代码了； --fix-dry-run: 修改检测出来的问题但是不进行保存； --cache: 传递该参数运行命令行工具的话会将检测结果进行缓存，下一次再执行缓存模式时将只针对有变化的文件进行扫描； --init: 初始化 eslint 的配置文件，传入这个参数会进入 eslint 的配置向导，从而生成符合用户期望的配置文件。配置向导如下所示：","link":"/2021/03/28/EslintTaste/"},{"title":"青岛（一）","text":"说在前面最近两个月遇到了不少的烦心事，让我颓废了好一阵子：懒得写博客，懒得跟人说话。很久没有到过海边，都要忘了在海边吹风是什么感觉了，趁着这次五一假期，和两个朋友一起去青岛玩了玩。虽然总共只待了 3 天 4 晚，但是还是有两点收获： 青岛是一个不错的城市； 不要在黄金周出游； 首先，临海的青岛让我这个南方人感受到了久违的湿润，终于不用担心因为干燥导致的皮肤瘙痒和手指倒刺，让我能够把身体乳和护手霜安心地放在行李箱里。青岛风景很美，海鲜很丰富，从视觉和味觉上都给了我满足感。 可能也是因为它的好，让大家在黄金周里都想去看看。我们 4 月 29 号晚上到时还很冷清，街道上车马稀疏，但到了 5 月 1 号立马到处都人挤人起来——我们计划中的不少的去处都是被人海给劝退的，比如信号山公园和小麦岛等等。我很讨厌人扎堆的地方，不管再好的去处，一旦人群密集起来我就会变得很焦躁。今年五一期间大家因为疫情缓和而报复性出游，让我感受到了人们的热情。所以趁着还在学校读书，假期安排比较自由，以后还是尽可能在非公共节假日出游吧。 栈桥、啤酒和皮皮虾栈桥4 月 30 号我们开始了第一天的游玩，风特别大，尤其是在海边。虽然 14 度的气温不比北京低多少，但是直观感受起来体感温度可能只有七八度，大概率是 60%的湿度导致的吧。 我们住在青岛火车站旁边的一栋高楼里，距离海边特别近，随便吃了点早餐就往海边走了。路上遇到了很多位推销旅游项目（游船、快艇、潜水等）的大叔大妈，“4 月 30 号人少半价，明天再来就不划算了”。我对这种主动推销的旅游项目不是特别感兴趣，我只想自己沿着海岸走走看看、踩踩沙滩，不想因为“今天便宜”就去坐船或者潜水。 但是旅游嘛，总是要花些冤枉钱的。他们俩很想坐游船和潜水捕捞，一起来的还是不要扫他们兴比较好，我就也买了游船的票。潜水是真的不敢去，大风低温的天气下水和冬泳没什么区别了。游览栈桥的计划就暂停了，我们坐上一辆面包车被一起送到了不远的潜水俱乐部。在他俩潜水的时间里，我就在八大峡广场周围走了走，吹吹海风，拍拍照片。下面是几张我比较喜欢的照片。 因为是刚接触摄影，对用光、构图和相机参数都没有特别深入的了解，基本都是用半自动模式靠感觉瞎拍的。但是把看到的东西好好的拍下来的仪式感也非常契合我对“记录”本身的喜爱。 他们俩从水里出来之后很长一段时间都在瑟瑟发抖，可以看出确实很冷，同时我也庆幸自己没跟着去。他们还从海底捞出来一些贝壳和海星，观感不太好就不在这里放照片了，而且到晚上它们都发臭了…收拾好后我们才开始往栈桥方向走。这时候是中午 12 点多，但是温度反而比上午 10 点更低了，沿着海边从俱乐部走到栈桥的过程中经历了急风骤雨和雨过天晴，现在想起来这也算是一种奇妙的体验了。 栈桥景区的结构挺漂亮的，拱桥长廊和末端的回澜阁一起组成了“海上如意”的形状。有很多人喂海鸥，有人把吃食放手掌上，海鸥都不过来；有人直接把吃食往空中抛，靠近的海鸥们会竞相争食。还挺有意思的。 啤酒博物馆每去到一个地方我就想去当地的博物馆。提起青岛肯定就会想起青岛啤酒，在网上看了攻略的我们在这样寒冷的天气里还是决定离开栈桥前往啤酒博物馆。我们叫了辆滴滴，但是司机偏偏不走系统推荐的稍微远一点但是不堵的路线，而是选择了走大学路。虽然行程时间比预计的多了一倍，我们却能够在车上仔细品味网红街的美，嗯我就是在吐槽这个出租车司机。 我觉得青岛啤酒博物馆建设和运营得非常好。它分成了 A/B 馆，A 馆是不收费的，视角宏大地讲述了青岛啤酒的历史和成就，B 馆里则细致地介绍了啤酒的酿造工艺，赠饮啤酒原浆，出售文创纪念品，甚至还展示了青岛啤酒的生产车间。中途和终途酒馆都可以领取啤酒，对于喜欢喝啤酒的人来说这里是个不错的去处。 晚餐从啤酒博物馆出来我们就到了台东步行街，这里整条街都是海鲜大排档。我们几个人在“晚饭吃什么”这个问题上出现了分歧，有人想回去吃，有人想就近吃，有人想在回去路上边走边看，一度有些不愉快。这时也有店家从路对面走过来招徕，拉踩其他家店，看到这个状况我们都不想在这边吃任何一家店了。 回去路上看到一家店叫做“沂水人家”，大众点评上查了一下发现评价还不错，我们就选定了这家店。不过，不知道是不小心还是有意为之，他家菜单里写的烤多宝鱼是 78 元/只，但是我们点了一只之后店家跟我们说是 78 元/斤，虽然味道还可以，但是感觉上还是比较奇怪的。除此之外，我们还点了酱猪蹄、清炒茼蒿、辣炒蛤蜊和椒盐皮皮虾，味道都还不错。尤其是椒盐皮皮虾，椒盐永远的神！火候控制得很好，香酥的、椒盐味的壳变得很有风味的同时，虾肉保持了鲜嫩的状态。在这顿饭里皮皮虾毫无疑问是 MVP。 崂山、天主教堂、五四广场和海我发现开始写之后就变得很啰嗦，总是想把看到的听到的都写下来，记录一天的行程就花了很长的时间。索性拆成两篇来写吧！","link":"/2021/05/05/JourneyToTsingTao-1/"},{"title":"青岛（二）","text":"从青岛回来之后马上就是学生节和一系列的课程、任务和作业，根本没时间和心情来写这篇博客，所以青岛（二）这一篇拖了很久…但是过了一个多月的时间很难再拾起当时的心情了，从拍下来的照片里才能记起来去过哪里有哪些好玩的地方，可以说是看图说话。反正也没人看，我直接这样随便写写就当给自己一个交代好了，不喜欢做半途而废的事情。正义虽然会迟到，但永远不会缺席。 这一篇我们主要是去了崂山、信号山公园、天主教堂、五四广场转了转。 崂山仰口海边不知道我们怎么想的，不远千里的到青岛去爬山受累，可能这就是正经的游客吧？在前往青岛的高铁上我们才开始做攻略，在马蜂窝上找了几个青岛游的攻略，都说崂山是必须要去的地方，所以我们把崂山列在我们第二天的行程当中。好巧不巧我们选了仰口区，从市区里过去要做两个小时的地铁大巴，实在是太久了。在第一天的 2 万步之后我坐完这两小时的行程时已经有一点蔫了，到了景区下车的时候热浪扑面，虽然前一天很冷，但是这天中午阳光非常充足，把前一天的凉意一扫而空。仰口景区的结构是这样的，景点的起点是在山脚，离海非常近的位置，我们先在海边玩了玩。买了风筝想在海边放，但是奈何海风太猛且方向不确定，很难让风筝飞得高远且持久。朋友都弄的鞋里都是沙子了，但就是不能让风筝飞得让自己满意——只能怪风况不佳咯。 此时已经是 12 点多，我们还没有吃午饭，于是灰溜溜的到周边去找地方吃饭。景点里的餐厅又贵又难吃这是我们的共识，于是我们找了一家超市，各吃了一桶方便面（实惨）并买了一些饮料和物资背在包里开始上山了。 山和海天公不作美，我们刚检票入园就下起了雨，我们撑伞走了一段时间雨才完全停。不过这也算好，这也使得景区没有特别热。仰口景区有些道观和寺庙，这些我不是特别感兴趣就不提了。随着爬的高度越来越高，视野逐渐开阔时，我的心情还是很激动的，毕竟很少看到这样的山海景，波澜壮阔、云海翻涌的场景不由的在脑海里开始放映。我们爬到一定高度后在一个亭子里拍了下面第一张照片，山、海、天交相辉映…很难形容我当时的想法。 上面的照片是随着高度的不断提升排列的，每上一定的高度都会有不同的景象，每个地方我都拍了一些，其中最后一张是在山顶拍的。 山顶风特别大。有一个景点叫做“天苑”，一块巨石被另外的石块三个点支撑着却稳稳地立了起来，很有意思。山顶的驻足点由山体本身和一些悬空的木质围栏、栈道组成，些微恐高的我站在木质栈道上有点胆战心惊的，生怕被风给吹垮了或者是被这么多人的重量给压垮。站在上面看着山麓茂密的树林和蔚蓝的海水及其上随风而动的云的阴影，体会到了在北京爬山体会不到的感觉。（毕竟在北京爬山都看不到海） 觅天洞明明可以走平坦的路，我们偏偏选了特别难走的一条。去往山顶可以走常规的楼梯和一个叫做觅天洞的地方，我们也没多想就选择走觅天洞这条路了。进“洞”之前我们看到了一些警示标语——“患心脏病或者心脑血管疾病人士请折返”，我们还纳闷这不就是一个景点至于这样警告游客吗？我们偏不信就进去了。 然而结果就是它真的和它的名字一样是一个洞，内部非常的潮湿、狭窄、黑暗。我们在其中全程蹲下来扶着墙走，遇到需要爬楼梯的地方我们也几乎是贴墙爬行，用“贴地爬行”一点也不夸张，非常狼狈。爬了二十多米的高度之后终于到了开阔一点的地方，能够看到阳光了。我至今能够想起当时大家一起“苟”的样子，除了我们几个小伙子外还有一些爷爷奶奶辈的人，我非常担心这些人群的安全。非常不推荐以后有人走这个地方：我觉得这个地方作为一个景点是不合格的，标语警示强度不足，谁能知道里面是这样一个样子呢？ 没有图，就是一个约莫一人宽的洞。非要说它给我留下什么好的印象的话，那就是或许可以当作体验一下桃花源记渔人见到“山有小口，仿佛若有光，便舍船，从口入”的经历吧。 下山下山感觉比上山还累，因为上山的时候腿脚已经很疲惫了，下山走楼梯又要不断地给膝盖压力，到山脚的时候感觉膝盖有点废了。又是两个小时的大巴地铁回到住处之后天已经黑了，随便吃了一点东西我们就休息了。 其实还按照网上的攻略指示去了一趟“劈柴院”，就是一条哪里都有的古风商业街。真的是 哪 里 都 有。一点意思都没有。 天主教堂、五四广场其实没有什么新颖的和印象深刻的东西值得一提。就放几张照片吧，从上到下依次是天主教堂里的窗户、鱼山路某咖啡馆里的大猫、奥帆中心的帆船和海。 在六月的夏天里写五月的海一篇游记拖了一个月才凭借着回忆来写属实有点不可思议，但是这一个月里有很多的课程任务、课题组和辅导员的工作需要处理。直到自己最近把课程的事情都处理完，才能闲下来写这篇博客。 我发现当我有其他非常要紧且糟心的事情需要解决的时候，我总是很难静下心来做写博客、拍照片、写闲代码这样的事情。可能我的“多线程”能力还有待提高吧，或许是我的抗压能力不太行？听过很多关于现代人时间管理方面的说法，时间是海绵里的水，挤一挤总是会有的，觉得现在看大家在朋友圈里玩得很欢乐，其实玩乐之后还是有很多事情是亟待完成的。 绝大多数的人无非都是带着脚镣跳舞，但是我带着脚镣的时候我想的不是如何跳舞，而是如何把脚镣解开。这或许是不太现实、不太成熟的想法。生活中谁能够完全把手铐脚镣解开呢？解开一个脚镣之后又会发现自己被新的、更大的脚镣锁住，如何戴着脚镣跳舞，如何把脚镣舞跳好可能是未来的我给自己的重要课题。 北京的六月特别热，让我又想起了青岛五月的海风了。","link":"/2021/06/13/JourneyToTsingTao-2/"},{"title":"我毕业了","text":"从 2016 年 8 月来到北京上学，到现在已经快要五年了。近期借着学校给 2020 届补办毕业典礼的契机，得以穿上学士服跟朋友们同学们一起拍毕业照。课程学习生活暂告一段落，心里有很多感慨不吐不快。 大概从以下几个方面来聊一聊吧：专业选择给我的影响，母校之于我的意义以及自己的成长和收获。 专业选择用为期两天的高考结束了漫长枯燥的高中生活之后，我进入了软件学院继续学习，专业名称是软件工程。其实我填报志愿时对母校的专业不是很了解，在招生组老师和学长学姐的介绍之下选择了这个和计算机相关性最大的专业。（对没错，当时我们省不招计算机） 差距很大作为小镇做题家的零基础计算机小白，在入学之后很快地就感受到了和其他同学的差距，尤其是编程作业一些同学一小时完成而我要花上一下午甚至一整天的时候。学校的学期中学习压力很大，26 学分左右的课程基本上意味着一周五天都是课，每天都有作业要做。当时编程能力十分落后带来的影响导致一些基础数理课也有些吃紧，好在最后都没有触到挂科这个的情况。我经常问我室友或者年级里的其他同学作业问题，怕总是问一个同学把他问烦我就在一些同学之间来回换着问。在这里要由衷感谢几位大神和我的室友们。 兴趣渐浓我从小就对电脑这东西比较感兴趣，不管是电脑游戏还是一些电脑应用，总是喜欢自己捣鼓。来到软件学院也算是得偿所愿，尤其是很多课程的大作业（或许有的学校叫做“课程设计”）都是写游戏，很对我的胃口。本科应该一共写了 3 个游戏，虽然质量都很差，但是写的过程还是快乐的。我发现我喜欢自己造一些东西出来，写一个游戏、发布一个包或者库、开发一个系统这些都是比较吸引我的事情。比起考试这样的紧张刺激的课程考核形式，开发游戏或者系统这样成果看得见摸得着的事情，让我觉得更加踏实。可能总的来说我还是比较希望规避不确定性，拥抱更多的确定性吧。或许这也从一个方面体现了其实我不是天赋型选手，勤能补拙应该是我需要考虑的第一要义。 大三上了《软件工程》这门课，和另外三个队友真刀真枪的开发一个系统：需求分析、原型设计、技术选型、数据库设计、功能实现、系统测试、文档撰写这些软工流程我们完整地体验了下来，最后课程成绩比较好。我也发现了我确实挺喜欢写代码的。后来的数据库、计算机网络等课程，包括后来到旷视实习的经历都进一步加深了我的兴趣。让我逐渐明晰喜欢的事情。 现在开发的琴房预约系统目前已经上线，虽然之前的队友不太有热情来维护这个项目了，但是就算是我一个人我也会把这个项目好好维护到我自己毕业；作为辅导员，我自己开发了一个系统给自己和同学们用来查询成绩和排名，也给其他辅导员开发了一个计算排名和查询成绩的网站；面对新颖的、高效的、优雅的技术我总是会心生向往，很难控制自己想去了解一下的冲动，可能就是喜欢折腾吧。每次想尝试一个东西我都会在 github 上创建一个叫做 XXX-taste 的 repository，现在类似的 repo 好像已经有十来个了。我觉得目前我对我所做的事情还是保有相当的热情，也愿意为之投入更多的时间。 母校之于我在学校待了五年，要说它给我带来了什么或者让我失去了什么可能很难用言语梳理出来，列一下现在能想到的几点吧： 学校给我上的第一课就是接受平凡，在高中三年大大小小的考试中我很少考第二名，但是到了这里要接受自己是个计算机小白，排名只在中等水平的事实。 如果拿不了95分，那么80分也挺好，好好完成课业任务，心态要稳。与其跟别人在成绩上卷，不如做自己感兴趣的、有意义的事情。 大学里学的不只是课内知识，更要学习待人处事的方法，增强面对挫折的勇气，拓宽所处世界的视野。 朋友很重要，尤其在校园这个小社会当中，他们就是自己的靠山了。 入党是一件非常庄严的事情，如果不是明确清晰地认同党的路线和纲领，请不要申请入党或者发展这样的同学。 不要总是窝在寝室打游戏，户外真实的风和阳光比游戏中的虚拟更加美好。 总结和展望大学里拿过一些4.0，也险些挂了某些课程；当过班长、学生会主席，现在还在继续做着辅导员这份学生工作；感情方面谈过异地恋、异国恋，当过舔狗，被劈过腿。生活体验可以说是非常丰富了（甚至有些离谱），从这些经历中得到了什么成长呢？得到了较强的心理承受能力吗哈哈，挺过挂科边缘的压力还是需要一定的承受力的；一边忙着各种课程作业，一边手忙脚乱地筹办学生节也是很难得的体验，更何况我们还产出了很多优秀的文艺节目；谈的几段感情都是异地，美好的回忆当然是有的，但是遗憾更多吧，这几段经历都无一例外地让我变得更加成熟了。 距离完全跃入人海还剩下不到两年的时间，通过这五年的时间我应该能够以更好的姿态来迎接未来两年的挑战，也希望剩下的两年里可以多做些自己想做的事情，不留遗憾。冲鸭~ 再回过头来看，我发现自己是极其幸运的，所学专业和兴趣爱好高度重合，这使得我在持续的学习过程中能够保持专注、维持平和。关于平稳心态的一切对我过去、当今和未来的生活都有着、都将有至关重要的影响。","link":"/2021/06/29/Graduation2021/"},{"title":"记2021年小程序大赛","text":"从本科到现在总共参加过三次微信小程序大赛，前几天刚完成赛区决赛的答辩，心想着以后应该都不想再参加了，于是想记录一下这次的参赛经历。 2018 和 20202018 阴差阳错微信小程序从 2017 年正式上线博得广大关注，在第二年 2018 年春季，微信就推出了全国高校微信小程序开发大赛，吸引了很多大学生参加，我就是其中一个。正在读大二的我报名了一位学长组织的 SRT(Student Research Training) 项目，原本的题目是要开发一款在线客服平台。然而当时已经把项目申请里面提到的客服平台提前完成了，于是我们真正做的就变成了小程序。甚至还用这个小程序参加了第一届的微信小程序大赛。 虽然做的东西和项目选题没太大关系，不记得当时具体开发的小程序有些什么功能，最后也没有拿到什么奖项（但隐约记得有证书）但通过这项目我第一次接触到了 Javascript、Vue 和微信小程序这些新东西，也认识了几位非常靠谱的队友（在之后的课程作业组队当中也延续了组队），我的收获不少，所以还是非常感谢这次项目经历的。 2020 毫无波澜第二次是在 2020 年用课题组的一个项目参加的比赛，进了华北赛区的决赛，但着实因为我们准备的不够充分而且其他队伍的作品也比较优秀，我们最终也止步于此。得到这个结果我不是特别遗憾，因为我在准备赛区答辩时就有了心理预期，所以说这次参加比赛的内心感受就是毫无波澜吧。 2021 年小程序背景如果说对第二次参加比赛的小程序我只是一知半解，那么今年参加比赛的作品我就是了如指掌。因为这是我们大三学期上《软件工程》课程时做的课程作业，它的背景是给学校艺教中心开发一套能够方便琴房管理和琴房预约的系统。 本来以为这个作业像往常那样只停留在“作业”这个层面，不会有实际使用价值。但是授课老师认为我们组做得不错，于是真正帮我们联系到了艺教中心的老师开了个会。会上艺教中心老师对我们的系统非常满意，这让我们喜出望外，但同时也给我们提了一些问题，例如还没有接入清华校内统一身份认证接口、没有接入校内支付接口等等，但我们相信后面都可以解决的。 那个学期过得很快，我们组的作品获得了 A 的评分，但是我们小程序的上线过程却不那么顺利：因为要接入校内的接口，我们需要把程序部署到校内的服务器上，之后经历了漫长的申请服务器、申请域名、申请安全扫描、申请证书、申请身份认证接口权限、申请财务接口权限等流程，期间还经历了新冠疫情爆发回不了学校的一个学期以及需求变动重新开发新模块… 功夫不负有心人，终于万事俱备申请上线了，那报名参加比赛吧。小程序名字叫做“掌上艺教”。 定位和需求分析小程序的定位掌上艺教是一个涵盖清华大学艺术教育中心琴房租赁和课程管理的一站式系统。面向三类不同的用户，他提供不同的功能。 面向在校师生、教职工和居民，掌上艺教提供了校内外两种安全高效的身份认证方式，认证用户可查看琴房空闲时间、预约琴房、在线付款、查看订单和核验电子票等；面向艺教中心开课教师，除了琴房预约外，还提供课程申报、志愿填报等功能；艺教办公室老师可以使用系统的管理后台应用对琴房租赁和课程业务进行管理； 需求痛点清华大学艺教中心历史悠久，业务众多。它每年开设艺术类选修课程 160 门次，指导学生艺术团 12 支队伍共 1300 名队员的排练，同时需要管理校内众多文艺场馆。 但是艺教中心目前的业务存在信息化不足、办公效率低的问题。表现在具体的业务当中有以下两个方面： 在琴房业务中，艺教中心采用“办卡储值，打孔消费”这样比较原始的机制，容易造成浪费和卡片丢失的问题；同学们要想预约琴房只能亲自去艺教中心询问是否有空余的琴房，这样费时又费力；另外，老师如果想查询、统计和备份数据或者是进行用户管理就很难办到了。这样的局面对于老师管理琴房和同学预约使用琴房来说都非常麻烦。 在课程管理上，开课教师和办公室老师们通过邮件和微信沟通开课意向和期望志愿缺乏系统性，而且在邮件或微信往来过程中信息很容易丢失；最最要命的是，收集到课程信息后，办公室老师需要把 160 门课程的开课时间、上课地点手动地排在一张 Excel 大表里。手动操作出错的概率很高，但是一旦出错会给开课教师、艺教中心和学校教务带来很大的麻烦。下面这张图片就是手动排课时用的表格截图，当然这只是冰山一角。 我们的系统就是要解决上述的痛点。 功能介绍我们开发了掌上艺教小程序和与之配套的两个管理后台（分别对应琴房和课程业务），其中小程序的主要功能有： 身份认证：对校内外用户分别进行相应的身份认证，对接清华大学统一身份认证； 查看课程表：开课教师能够查看当前学期自己开课的课程表； 课程信息查看：开课教师能够查看自己或者其他教师所开课程的信息； 开课意向填报：开课征集期间，任课教师可以在小程序中填写开课意向和排课志愿； 排课信息确认：艺教办公室老师在后台生成排课表之后，开课教师能够查看并确认课表是否符合预期； 琴房浏览：使用掌上艺教能查看艺教中心对外开放租赁的琴房列表并查看它们的信息如空闲时间、租赁单价、位置简介等； 支付定金：预订琴房后可在线支付定金，完成支付后将获取订单核验二维码； 代金券折扣：支付定金时可使用代金券抵扣指定额度的琴房定金； 电子票核验：琴房入检人员可以扫描支付完成的订单二维码核验信息并准入。 课程管理后台的主要功能有： 课程信息管理：可以对系统内课程信息进行增删改查，包括但不限于某一学期内开课教师所创建的课程； 教学场地资源管理：可在系统中维护教学场地包括教室地点、教室容量、教室属性等在内的信息； 排课表生成与调整：开课教师提交完成排课志愿后，系统将根据开课教师的志愿信息和在系统中设置的权重策略，对所有课程进行自动编排。自动生成出的课表可以进行手动调整； 后台账号管理：可以对人员进行账号管理，包括启用/禁用账号、账号密码重置及权限管理； 教师名单维护：通过维护教师名单可控制哪些用户能够使用排课系统； 通知公告：在管理后台可使用富文本编辑器编辑发布通知公告，并查看教师们的阅览情况（已读或者未读）。 琴房管理后台的主要功能有： 琴房信息维护：可以对琴房列表和琴房信息进行维护，包括琴房名称、位置、头图和占用规则（规律性占用）等； 订单管理：可以通过姓名、手机号、证件号、预约时间段、预约琴房等条件进行订单查询与统计，并查看或修改订单详情； 用户管理：可通过姓名、手机号、证件号等字段查询用户详情，进入用户详情可将用户纳入黑名单、查看用户的订单和代金券等； 用户组管理：通过设置用户组对琴房的预约权限、租赁单价和用户列表，可以实现用户的权限管理和租赁单价管理； 代金券管理：可向指定用户手动发放代金券或向指定用户组周期性发放代金券用于琴房定金抵扣。 系统架构 系统架构方面，我们选用了 Vue.js 作为 Web 应用的开发框架，小程序则使用微信原生框架进行开发。前后端通讯方面我们使用的是 GraphQL（可能需要科学上网），这和传统的 RESTful API 相比有很多优点，在这里就不展开说了。后端则借鉴了流行的微服务架构的思想，把两部分后端服务进行了实现和部署上的拆分（可以看到两个后端服务的开发语言都不一样），同时两个服务都采取了容器化的部署方式，极大地降低了开发和运维成本，同时减少了系统整体不可用的风险。 后端服务拆分实现部署的开发方式我们还是第一次采用，这确实给我们的系统带来了很高的灵活性，技术栈的选择也更加自由。而且，通过实现服务间通信，各个实体之间的交流更加密切，可以做到更多有趣的事情。例如微信小程序用户的登录状态存在一个服务 A 上，当另一个服务 B 也需要用到该状态时，可以通过直接向 A 发请求拿到相应的信息。在我们的系统当中，服务间通信是直接发 HTTP 请求，以前实习时做的系统用的是 gRPC 和 protobuf 来完成的。不过本质都差不多。 比赛结果赛区决赛当天晚上其实就已经出结果了，我们的小程序是华北赛区第 5 名，是三次比赛当中名次最高的了。虽然可能进不了全国决赛，但是我还是能够接受这个名次的。前几名的作品确实有着非常好的产品定位（例如第 2 名“帮你学拼音打字”目标是帮助中老年人学习使用手机拼音打字）和非常高的完成度（例如第 1 名“方仔照相馆”），我们的小程序作为一款在校内使用的半开放式小程序，格局比起他们的来说还是小了不少。 最后的感想2021 年的微信小程序大赛其实已经稍见颓势，从取消微信小游戏赛道这方面就能看出微信不太重视这场比赛了。参赛作品的多样性和创新性也随着时间的推移逐渐降低，越来越难以见到让人眼前一亮的好的作品。所以比赛不再受重视也不奇怪，微信一开始推出这个比赛就是想借助有热情有活力的大学生群体带动小程序的市场，而到了 2019~2020 年微信小程序其实已经占据市场主导地位，不再需要依靠比赛来扩大影响力了。 另外比赛的风气也不太好，2020 年的华北赛区决赛好像就出现了相互举报的状况，有点无语也有点无聊。不知道明年这个比赛还会不会办下去，不过就算继续办我也不太想再参与：我的热情被小程序开发消磨掉不少，也没有太多创新的点子想付诸小程序开发。小程序对于用户来说有着很不错的体验，但是对于我们开发者来说小程序是处处受限的前端应用，这也不能用那也不行。没错现在我已经不想“用户至上”了，我想做更纯粹一些的开发。","link":"/2021/07/20/TheThirdMiniprogramCompetition/"},{"title":"如何相对优雅地使用 GraphQL","text":"关于 GraphQL，它的官网(需要科学上网)是这样介绍的： GraphQL 既是一种用于 API 的查询语言也是一个满足你数据查询的运行时。 GraphQL 对你的 API 中的数据提供了一套易于理解的完整描述，使得客户端能够准确地获得它需要的数据，而且没有任何冗余，也让 API 更容易地随着时间推移而演进，还能用于构建强大的开发者工具。 作为 RESTful API 的竞品，GraphQL 从开源之初就备受关注，一是因为它是由 Facebook 开源的项目，二是它挑战了 RESTful API 的地位，这是很关键的一点。RESTful API 利用 URI 的具体内容和请求方法来区分请求的资源或者方法，其中的资源 URI 容易与路由路径产生混淆和重复；资源数量达到一定的数目之后，如何给资源 URI 起名或许也是一件困难的事情。而 GraphQL 则鼓励开发者将所有需要请求的信息显式的写在请求体当中，精确到具体的字段，不多也不少。 我参与的几个项目都是用 GraphQL 作为 API 的基础，我总结出了一个在前端相对优雅地使用 GraphQL 的方法。这篇博客不讨论 GraphQL 的基本概念，主要介绍这个方法。（为省篇幅，这篇博客里面的代码均不做异常处理） GraphQL 的原始用法GraphQL 在前端的表现其实并不新奇：根据定义好的 schema，前端用 post 请求将 query string 和可选的 variables 包装在 body 当中传给后端的某个节点，后端正确响应之后以前端查询的结构将数据返回。 利用基础方法那么根据这个基础我们就能够想到在代码中的用法了，首先在 api.js 中定义一个请求的基础方法： api/index.js1234567891011121314export async function graphql(q, vars) { const resp = await $axios({ method: &quot;post&quot;, url: BASE_URL + &quot;/api/graphql&quot;, data: { query: q, variables: vars, }, headers: { token: getYourToken(), }, }); return resp.data.data;} 然后在所有需要请求的地方使用这个基础方法就可以完成 graphql 的请求，可以直接把请求参数嵌在具体的请求方法里面，也可以使用 variables 的方法（但这样会需要在字符串中多写些变量定义）。 index.js1234567891011121314151617181920212223242526272829import { graphql } from &quot;./api&quot;;async function getPerson1() { const resp = await graphql( `query { person(id: ${id}) { id name address age gender mobile } }` );}async function getPerson2() { const resp = await graphql( ` query ($id: ID!) { person(id: $id) { id name address age gender mobile } } `, { id } );} 体验糟糕的查询字符串或许你觉得上面的方法还可以接受，但是如果变量中存在数组、枚举值和布尔值时，直接在 query string 中插入变量的体验就会变的很糟糕。像下面这样，我这辈子都不再想见到这样的写法。 index.js12345678async function heyGuys() { const guys = [&quot;alice&quot;, &quot;bob&quot;, &quot;carol&quot;, &quot;dave&quot;]; const resp = await graphql(` query { heyGuys(guys: [${guys.map((n) =&gt; '&quot;' + n + '&quot;').join(&quot;,&quot;)}]) } `);} 统一管理查询字符串在逻辑代码中写大量的 query string 不太利于维护，为此可以将所有字符串分类整理好统一管理。例如放在某个 documents.js 文件内，其他地方需要请求的时候直接从该文件导入即可。这样可以在真正的业务逻辑中避免大量的字符串。 api/documents.js1234567export const getPersonDoc = ` query ($id: ID!) { person(id: $id) { id name address age gender mobile } }`; 问题仍然存在上面统一管理查询字符串的体验还凑合，但是真正开发起来就会发现有以下几个问题绕不开： 我如何知道一个字符串对应的变量应该是什么？只能查看字符串本身的定义； 我写字符串的时候如何获得代码提示呢？还是说我只能对着后端的 schema 逐字段慢慢写呢？ 既然都统一管理了查询字符串，是不是还得再封装一层查询方法呢？这样业务逻辑处的代码还能更省。 如果都用字符串如何使用 Fragment 呢？（或许可以看一下 graphql-tag） 更好的方法：代码生成如果完成一个请求需要先写查询字符串，再封装一个关于这个查询字符串的请求方法，开发效率不会很高。可以看到上面的代码很多都是琐碎且平凡的，既然如此，可以尝试生成代码。为此我们需要了解以下的包或者插件： @graphql-codegen: graphql 代码生成器，一个 npm 包。通过定义的 schema 和 operation 生成包含请求方法的 typescript 文件； GraphQL: vscode 插件，用作写 .graphql 文件时的自动补全； 配置 @graphql-codegen按照该包官方文档的指示进行安装配置即可，不需要太多的配置。其官网上还有下图所示的 live example，非常容易弄懂。 我在开发中一般会配置两个代码生成配置文件，一个用于同步后端、生成代码补全所依赖的 schema 文件，一个用于生成 operation 对应的请求方法。如下是两个配置文件的大致内容。 schema.codegen.yml12345generates: ./graphql/schema.graphql: schema: &quot;BASE_URL/api/graphql&quot; plugins: - schema-ast operation.codegen.yml12345678generates: ./api/demo.ts: documents: &quot;./graphql/operations.graphql&quot; schema: &quot;BASE_URL/api/graphql&quot; # 或者直接使用 './graphql/schema.graphql' plugins: - typescript - typescript-operations - typescript-graphql-request 配置文件中的 plugins 配置项是关键。schema.codegen.yml 中的 schema-ast 是生成 schema 的 graphql 文件；operation.codegen.yml 中的 typescript-* 则是生成请求方法所依赖的插件。我这里给出的是使用 graphql-request 的例子，graphql-request 是一个轻量、简洁，支持 ts 和 promise-based API 的 GraphQL 客户端，在前后端都能使用。 生成请求方法所依赖的插件根据项目特点选定，例如 graphql-request 这个插件我用在 Vue2.x 的项目当中，而在 React 的项目中我是用的插件是 React-Query Hooks。每个插件对应的基础库的特点不一样，生成的代码风格也不尽相同，根据需要灵活选择即可。 编写 operationsoperations 顾名思义就是操作，在 GraphQL 里面操作分为 query 和 mutation，编写具名操作会被 @graphql-codegen 转换成为请求方法。下面给一个例子： schema.graphql1234567891011121314151617181920212223enum Gender { Female Male}type Person { id: ID! name: String! address: String! age: Int! gender: Gender! mobile: String!}type Query { persons: [Person!]! person(id: ID!): Person}type Mutation { setPersonGender(id: ID!, gender: Gender!): Boolean greet(id: ID!): String!} operations.graphql123456789101112131415161718192021query getPersons { persons { id name }}query getPerson($id: ID!) { person(id: $id) { id name address age gender mobile }}mutation sayHello($id: ID!) { greet(id: $id)} 因为用了 vscode 的插件 GraphQL，所以在写 operations 的时候其实是有代码补全的，开发体验比较好，下图是在 vscode 上的代码补全，在 jetBrains 的 IDE 上面的代码自动补全应该会更完善。 生成 typescript 代码写完上面的 schema 和 operations 之后，运行 graphql-codegen --config operations.codegen.yml 即在 ./api 目录下可生成一个 demo.ts 文件。其中包含了下面这样的代码： api/demo.ts123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// ... 省略了很多const defaultWrapper: SdkFunctionWrapper = (action, _operationName) =&gt; action();export function getSdk( client: GraphQLClient, withWrapper: SdkFunctionWrapper = defaultWrapper) { return { getPersons( variables?: GetPersonsQueryVariables, requestHeaders?: Dom.RequestInit[&quot;headers&quot;] ): Promise&lt;GetPersonsQuery&gt; { return withWrapper( (wrappedRequestHeaders) =&gt; client.request&lt;GetPersonsQuery&gt;(GetPersonsDocument, variables, { ...requestHeaders, ...wrappedRequestHeaders, }), &quot;getPersons&quot; ); }, getPerson( variables: GetPersonQueryVariables, requestHeaders?: Dom.RequestInit[&quot;headers&quot;] ): Promise&lt;GetPersonQuery&gt; { return withWrapper( (wrappedRequestHeaders) =&gt; client.request&lt;GetPersonQuery&gt;(GetPersonDocument, variables, { ...requestHeaders, ...wrappedRequestHeaders, }), &quot;getPerson&quot; ); }, sayHello( variables: SayHelloMutationVariables, requestHeaders?: Dom.RequestInit[&quot;headers&quot;] ): Promise&lt;SayHelloMutation&gt; { return withWrapper( (wrappedRequestHeaders) =&gt; client.request&lt;SayHelloMutation&gt;(SayHelloDocument, variables, { ...requestHeaders, ...wrappedRequestHeaders, }), &quot;sayHello&quot; ); }, };}export type Sdk = ReturnType&lt;typeof getSdk&gt;; 可以看到 getSdk 这个函数会返回一个对象，其中包含了刚刚在在 operations.graphql 中定义的几个操作。这样就从类型上锁定了这个方法的名字、参数以及返回值。这对于项目维护和开发来说无疑都是利好的。 使用生成的代码因为真正使用的请求方法肯定是要鉴权的，我们需要再调整一下生成的代码，看到上面 demo.ts 中的 defaultWrapper 函数了吗？我们只需要在调用 getSdk 时传入自定义的 Wrapper 即可。下面给个例子： api/index.ts1234567891011121314import { getSdk as getDemoSdk } from &quot;./demo&quot;;const getClientOptions = () =&gt; { return {};};const apiWrapper = async &lt;T&gt;( action: (headers?: Record&lt;string, string&gt;) =&gt; Promise&lt;T&gt;) =&gt; { const headers = { token: getYourToken() }; return await action(headers);};export const demoClient = getDemoSdk( new GraphQLClient(&quot;/api/graphql&quot;, getClientOptions()), apiWrapper); 在需要用到的地方只需要导入 demoClient 即可，我们再用几行代码重写一遍上面的 getPerson 函数： index.js1234import { demoClient } from &quot;./api&quot;;async function getPerson3() { const resp = await demoClient.getPerson({ id });}","link":"/2021/07/23/UseGraphQLElegantly/"},{"title":"Nginx 基础","text":"背景Nginx 读作 engine x，是一个高性能的 HTTP 和反向代理服务器，还能用作邮件代理服务器或是通用的 TCP/UDP 代理服务器。有过后端程序部署经历的同学应该会有所了解，用 Nginx 能够很方便地完成反向代理、服务静态文件、实现负载均衡、接入 HTTPS 协议等任务。 根据官方文档里面写到的，除了提供静态文件服务、反向代理和负载均衡等功能之外，还提供以下包括单不限于若干个方面的支持： FastCGI, uwsgi 之类服务器和反向代理服务器的缓存支持； 以域名/IP 为基础的虚拟服务器、Keep-alive 和流水线链接的支持； 访问日志、错误日志的输出和格式化，带缓存的日志写机制； 3xx-5xx 错误码重定向，根据正则表达式重写 URI (Rewrite)； 基于客户端地址的访问控制和函数调用； HTTP referer 的验证、支持除了 GET 外的几种 HTTP 方法： PUT/DELETE/COPY/MOVE/MKCOL； FLV/MP4 的流播放、响应限流、限制单点的并发连接数和请求数； 等等…（上面只列了我读得懂的） Nginx 的功能实在太多了，在这里全部列出来不太可能。一直以来我对 Nginx 都停留在“配置是什么，work 就行”的态度。因为这周程序部署时遇到的一点问题，上网搜了好多帖子、博客寻求解决方法那种捉襟见肘、有病乱投医的样子让我觉得很狼狈。借此为契机，决定周末看一下 Nginx 的官方文档。nginx 旧版的官网文档组织混乱，建议移步 Nginx Plus (Nginx 的商业版)官网。Nginx 和 Nginx Plus 的对比放在这里。 基本功能概况Name/IP-based Virtual serversNginx 接收到请求之后会首先决定让哪个 server 来接受这个请求。在 nginx 的配置文件中使用 server 这个指令可以定义“服务器”区域，当然最后到底是谁处理这个请求也是看 server 是如何定义的。下面是示例代码块： demo.conf1234567891011121314151617server { listen 80; server_name example.org www.example.org; ...}server { listen 80; server_name example.net www.example.net; ...}server { listen 80; server_name example.com www.example.com; ...} 这个配置文件定义了 3 个服务器，都监听了 *:80 端口。Nginx 接收到请求之后会转发给 server_name 字段与请求头中的 Host 字段相同的 server 进行处理。如果没有完全对应的 Server 那么请求会被转发到默认服务器进行处理。default_server 是 listen 指令的可选参数，如果在端口号后加上 default_server 表示这个服务器是默认服务器。如果没有任何 server 有 default_server，那配置文件中定义的第一个 server 就是默认的服务器。 对 listen 中的 IP 地址进行配置也是可行的。下面再看一个配置文件： demo2.conf1234567891011121314151617server { listen 192.168.1.1:80; server_name example.org www.example.org; ...}server { listen 192.168.1.1:80; server_name example.net www.example.net; ...}server { listen 192.168.1.2:80; server_name example.com www.example.com; ...} 这个配置文件下，Nginx 会先判断请求的 IP，再判断 server_name 字段。 Locations路径配置规则在 server 区域之中，可以定义若干个 location 区域。location 指令可以根据请求的 URI 将请求分发到不同的代理或者不同的静态文件目录中，后面提到的所有 location 配置都在 server 区域之中。 location prefix 匹配符合 prefix 前缀的 URI，如果一个请求的 URI 匹配了很多 location 的 prefix 那么请求会被分发到 prefix 定义最长的 location 块中。另外，location 指令还可以有标识符配置： location = path 表示 URI 需要准确的等于 path 才会落到这个 location 区域进行处理； location ~ regex 表示 URI 匹配相应的正则表达式。如果是 ~* 的话，是忽略大小写的； location ^~ prefix 表示这个 prefix 匹配的话，不考虑相应的正则表达式； 匹配优先级要找到最好的 location 块来匹配一个 URI，Nginx 会先匹配所有的 prefix，然后再匹配所有的正则表达式。但是 Nginx 会给正则表达式更高的优先级，除非有 ^~ 标识符。准确的匹配流程如下： 将 URI 与所有的 prefix 进行比对； 如果 = 标识符的 location 命中了，直接使用该 location 进行处理，匹配过程终止； 如果有 ^~ 标识符标识某个 prefix，则后续不用正则表达式检测这个 prefix 命中的 URI； 保存最长的匹配的 prefix； 将 URI 与所有的正则表达式进行比对； 如果 URI 匹配到某个正则表达式，立即停止比对，用这个正则表达式对应的 location 进行处理； 如果没有正则表达式匹配，用第 4 步保存的最长的 prefix 来处理。 那么根据这个流程，在配置 / 对应的处理方式时，使用 = 标识符能够提高 Nginx 的响应速度。 root, index, proxy_passroot, index, proxy_pass 是几个在 location 区域内常用的指令，这里专门查一下它们的用法。 root path 用文件系统路径来表示，表示在这个 location 中从哪个目录找静态文件来服务； index file ... 定义首页文件，按照给定顺序注意匹配，最后一个文件名可以是绝对路径； proxy_pass url 将请求转发到某个代理服务器上。 变量和改写变量在 Nginx 的配置文件中可以使用以 $ 开头的变量，和 linux 的 shell 变量相似。Nginx 预定义了很多变量如 $remote_addr 表示客户端的 IP 地址，$uri 表示目前 URI 的值。用户也可以使用 set 和 map 这两个指令在配置文件中自定义变量。 Nginx 内置的变量列表在这里可以查看：core HTTP 改写return 使用 return code [url/&quot;string&quot;] 可以给请求返回相应的状态码，或者直接以 30x 的状态码跳转到后续的 url，以 200 的状态码返回一个 string。 demoReturn.conf123456789location /wrong/url { return 404;}location /permanently/moved/url { return 301 http://www.example.com/moved/here;}location /text/url { return 200 &quot;hello world&quot;;} rewrite 使用 rewrite regex target cmd 可以将 regex 匹配的路径改写到 target，cmd 这个参数有 break 和 last 这两个最常用的。它们俩的区别有两点： last 会终止在当前 server 或者 location 区域当中的 rewrite 执行，但是重写到 target 之后落在新的 location 中的 rewrite（如果有的话）还是会继续执行； break 终止当前背景下的 rewrite 执行之外，新的 location 中的 rewrite 也不会执行。 rewrite 的例子可以看下面的配置： demoRewrite.conf123456789location /users/ { rewrite ^/users/(.*)$ /show?user=$1 break;}server { rewrite ^(/download/.*)/media/(\\w+)\\.?.*$ $1/mp3/$2.mp3 last; rewrite ^(/download/.*)/audio/(\\w+)\\.?.*$ $1/mp3/$2.ra last; return 403; # 可以组合使用} 总结这次走马观花看了一篇最基础的教程有了较多的新认识，服务静态文件、反向代理、压缩和解压缩、内容缓存等等内容在 Nginx Plus 的文档中都有比较系统的讲解，之后有需要或者有时间的时候再看看吧。","link":"/2021/08/15/NginxBasics-1/"},{"title":"2022年我希望...","text":"新年的元月已然过半，但直到现在我才能说我完成了上一年的工作。我一直在追求“忙的时候闲一点，闲的时候忙一点”的从容，这一年大部分时间是保持着这个状态的，让我非常欣慰。但是真正从学生身份蜕变成“开发者”“软件工程师”（或者“社畜”？其实即使是自嘲我也不太喜欢这个词），还是有着截然不同的感受和体会的。对于 2022 也有着无数的憧憬。 2021 上半年：课程时代尾声2021 年上半年我研一，为了一年上完所有的课我这个学期仍然是课程满满的，与此同时课题组的项目也开始安排给我。在课程任务和项目进度的双重压力下，我会想着“尽快结束这样的生活节奏，争取早日回归生活的正轨”，但这个状态持续一小段时间之后我逐渐与之和解了，与其认定只用考虑学习的生活是正轨，不如接受多线程地生活才是当下社会当中每个人都需要具备的基础能力这个事实。 那么话又说回来，我认为我的多线程能力还是不错的，因为在主业之外我一直以来都或多或少地参与到学院、学校的学生工作中。在相对一板一眼的计算机知识、代码编写和需要社交活动的谈话开会、学生事务之间频繁切换也没有被我归入困扰我的事情当中。这样长期以来的“训练”让我对社交行为本身脱了敏，甚至有时还会主动发起社交活动；也让我对“同时处理很多事情”这个现象习以为常。 随着课程结课，我有了更多的时间可以投入到课题组项目里。我在课题组的工作投入在一定范围内是正反馈的：投入时间增加产出增加 –&gt; 工作产出增加 –&gt; 安排给我的工作增多 –&gt; 投入时间增加。实验室有位同学开玩笑地说我是“劳模”，每天都早早来到实验室但很晚才回寝。这个状态从暑假一直持续到现在，我自己是乐在其中了。我在课题组里承担的工作让我拥有了难以替代性，在这个过程当中我收获的知识、激励还有认可都成了我齿轮转动的润滑剂——即使有的时候这种难以替代性也带来不小的压力。这个反馈过程却有点像我高中的学习历程：认真学习带来的收获主、客观上都有助于我继续认真学习，学习换成工作也受用。我觉得这样的状态是比较难得的，我享受这一状态。我觉得我很幸运，学的专业做的工作都是自己喜欢的，所以能够一定程度上保持自驱。 2021 下半年：偶然与想象喜欢记录为什么不拍照呢？我在 5 月入手了一台富士相机，并立马带上它去了青岛。不深挖构图角度也不用 Photoshop，光是拨动相机滚轮，摁下相机快门在任何一处拍下软糯或嶙峋的云彩，波光粼粼的海面还是叶影斑驳的片片树荫、铺满和煦光影的高矮楼房，都让我感受到极为愉悦。虽然没有太多机会到外面旅游，但在学校里面简单记录四时风景也是很好的体验。 8 月开始捡起闲置已久的吉他，在琴包里放了近两年，音孔部分的琴弦竟然有些锈迹斑斑。完全从零开始学，在不懂乐理的情况下，我照着吉他入门教材学习了几个基本的和弦。弹唱的曲子从教材里比较简单的《送别》到自己感兴趣的《杀死那个石家庄人》《平凡之路》等等，也逐渐意识到自己水平有限需要加强练习。下面这张图虽然没有标识我的所有练琴记录，但可以大致体现我的练琴状态：有段时间练习比较密集，有段时间又比较松懈，指尖和食指侧的茧长了又消。以前听人说吉他很简单，可能跟“Python 很简单”道理一样吧，我到目前为止我学到稍微深入一点的技巧也只有横按和泛音而已。 这半年没有课程没有作业的时间里，原本以为会枯燥乏味的生活却过得很丰富，果然生活是需要偶然与想象的。 2022 的小目标2022 年还剩下 350 天，剩余的 95%时间里我希望自己可以有更多从容不迫、处变不惊的心态和能力，有“彼可取而代也”、不惧跃入人海的傲骨和锐气；对待喜欢的人要满怀热情，对待讨厌的人要爱憎分明。之前总说“想的多了，做的就少”，务虚的话简单说说就好，人不能总活在空中楼阁。给我的 2022 定几个务实些的小目标（排序不分先后，包括但不限于）： 每个季度至少读两本非技术类书目； 每个月至少更新一次博客，类型不限； 每两周至少看一部非所谓商业片的电影； 每周至少锻炼一次，有氧/无氧、室内/室外均可； 协助所带班级内每位同学捋清未来职业发展规划； 学习乐理，能够转换吉他谱和五线谱； 开源至少3个代码仓库，做些有意思的开发。 以上。2022 来年再会。","link":"/2022/01/15/SomethingFor2022/"},{"title":"打包发布 React 组件库","text":"起因「代码写了不测等于白写」我总是跟身边的朋友这样调侃。然而我们写前端项目时很难在代码层面进行测试，大部分函数都是基于事件响应，接收用户输入的参数，并对页面组件或数据产生一定副作用，Mock 起来很麻烦。所以前端项目的测试往往都是端到端测试，即模拟用户在页面上进行操作，测试路径越离奇越好，因为无法提前预知用户会如何使用，所以最好在测试时可劲儿造。 曾经还会想着用 Cypress 等自动化工具进行端到端测试，例如用代码定义【打开某页面–&gt;拖拽滑动条至页面下方–&gt;点击输入框使之获取焦点–&gt;输入“Hello world”–&gt;按下回车–&gt;等待页面响应–&gt;观察响应是否符合预期】这个过程，但只要遇到元素稍多的页面，编写测试用例的过程就会变得机械呆板。 如果组件足够小，内容够聚焦，那么测一下也不是不可以。因为想在不同的项目中复用同一套富文本编辑组件(体积比较大，且包含机器构建的 JS)，我把它单独提出来作为 NPM 包发布以便各个项目安装使用。这当中编码和测试都遇到了一些问题。 打包经过目标既然要在不同项目之间共用，那该组件肯定至少已经应用到一个项目中。所以最终目标就是把项目中原先引入的组件完全替换成为 NPM 包版本的组件后，所有富文本编辑预览功能都照常。 原先组件结构CKEditorFormFields.tsx123456789101112131415import &quot;./some-styles.css&quot;;import React from &quot;react&quot;;import { CKEditor } from &quot;@ckeditor/ckeditor5-react&quot;;import CustomBuildEditor from &quot;@ckeditor/ckeditor5-custom-build&quot;;export const CKEditorInput: React.FC&lt;ControllableFormFieldProps&gt; = () =&gt; { /* * ... */};export const CKEditorRenderer: React.FC&lt;ControllableFormFieldProps&gt; = () =&gt; { /* * ... */}; 原先的组件定义基本上如上面片段所示，其中 CustomBuildEditor 利用了 CKEditor5 的自定义构建，算是按项目需要选取必要功能构建出来的编辑器母版，它本身的使用方法很 HTML，不太适合直接用在 React 项目里，需要用 @ckeditor/ckeditor5-react 进行包装。 而这个 CustomBuildEditor 是自定义构建工具编译好之后打包好后（后续用 ckeditor-dist 称呼）下载到本地的，如果不用 NPM 包的话需要在几个项目间复制粘贴。或许因为我们项目用的是 TS，无法直接从本地目录下直接引入，所以我们用 package.json 依赖的文件链接定义了一个叫做 @ckeditor/ckeditor5-custom-build 的假包供代码引入使用，但这个方法时而奏效时而报错，或是在张三电脑上能用而李四电脑上用不了。可以确保解决问题的方法是将该 ckeditor-dist 目录复制到 node_modules 当中，但是这样过于原始。于是决定有时间研究一下 NPM 打包。 Hello-richtext目标富文本组件包名叫做 Hello-richtext，它需要依赖我们自定义构建的富文本编辑器母版，所以首先将 ckeditor-dist 单独作为一个 NPM 包发布到我们团队的私有制品库中，就起名为 ckeditor-custom-build。 把 ckeditor-custom-build 加入到依赖中，原先组件中包含的所有文件都复制到 Hello-richtext 的代码目录中。利用 tsc 以 &quot;target&quot;: &quot;ESNest&quot; 的配置将 .tsx 格式的文件编译为 .js 和 .d.ts 文件，或许这也是最终在前端项目中被应用时的引入形式。 随后利用 Jest 和 React Testing Library 写下了如下的测试用例。 render.test.jsx12345678910111213141516171819202122232425262728293031323334import React from &quot;react&quot;;import { render } from &quot;@testing-library/react&quot;;import { Form } from &quot;antd&quot;;import { CKEditorInput, CKEditorRenderer } from &quot;../dist/CKEditorFormFields&quot;;import &quot;@testing-library/jest-dom&quot;;Object.defineProperty(window, &quot;matchMedia&quot;, { writable: true, value: jest.fn().mockImplementation((query) =&gt; ({ matches: false, media: query, onchange: null, addListener: jest.fn(), // Deprecated removeListener: jest.fn(), // Deprecated addEventListener: jest.fn(), removeEventListener: jest.fn(), dispatchEvent: jest.fn(), })),});const TestForm = () =&gt; ( &lt;Form initialValues={{ renderer: &quot;&lt;p&gt;Hello world!&lt;/&gt;&quot; }}&gt; &lt;Form.Item label=&quot;Mock Input&quot; name=&quot;input&quot;&gt; &lt;CKEditorInput data-testid=&quot;input-field&quot; /&gt; &lt;/Form.Item&gt; &lt;Form.Item label=&quot;Mock Renderer&quot; name=&quot;renderer&quot;&gt; &lt;CKEditorRenderer data-testid=&quot;renderer-field&quot; /&gt; &lt;/Form.Item&gt; &lt;/Form&gt;);test(&quot;test rendering&quot;, async () =&gt; { render(&lt;TestForm /&gt;).debug();}); 启动测试时遇到的问题只要富文本编辑器能够正常渲染就成功了，所以首次运行测试我比较保守，只定义了简单的表单并把自定义的两个组件作为表单项置入其中，并尝试将渲染结果用 render().debug() 的方式打印出来看看是否正确渲染。 但是启动 Jest 之后遇到了一系列问题，下面列举了我遇到的问题以及相关的解决方案。 无法解析 .jsx 格式文件，通过安装 @babel/preset-react 插件并创建 babel.config.js 应用该插件解决； 提示 ‘react’ 这个包没有导出 default，通过安装 @types/react 和 @babel/preset-env 解决，其中 babel 插件同样需要应用到配置文件中； 提示没有 window.matchMedia 方法，直接通过 Object.defineProperty 给 window 打上补丁（官方建议）； 无法解析 CKEditorFormField 中引入的 .css 文件，通过安装 identity-obj-proxy 依赖，并在 Jest 配置文件的 moduleNameMapper 属性中加入 &quot;\\\\.(css|less)$&quot;: &quot;identity-obj-proxy&quot; 解决； 稍微麻烦些的就是上面 4 点，当然还有一些其他的必要的依赖也是需要安装的，这里给出局部 package.json，Jest 和 Babel 的配置分别如下： package.json123456789101112131415161718192021222324252627282930313233{ &quot;name&quot;: &quot;Hello-richtext&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;scripts&quot;: { &quot;test&quot;: &quot;jest&quot;, &quot;build&quot;: &quot;tsc&quot; }, &quot;keywords&quot;: [&quot;richtext&quot;], &quot;dependencies&quot;: { &quot;react&quot;: &quot;^17.0.0&quot;, &quot;@ckeditor/ckeditor5-react&quot;: &quot;^3.0.3&quot;, &quot;@ckeditor/ckeditor5-build-classic&quot;: &quot;^31.0.0&quot;, &quot;ckeditor5-custom-build&quot;: &quot;0.0.2&quot;, &quot;antd&quot;: &quot;^4.16.13&quot; }, &quot;devDependencies&quot;: { &quot;@babel/core&quot;: &quot;^7.17.5&quot;, &quot;@babel/preset-env&quot;: &quot;^7.16.4&quot;, &quot;@babel/preset-react&quot;: &quot;^7.16.7&quot;, &quot;@testing-library/dom&quot;: &quot;^8.11.3&quot;, &quot;@testing-library/jest-dom&quot;: &quot;^5.16.2&quot;, &quot;@testing-library/react&quot;: &quot;^12.1.4&quot;, &quot;@types/jest&quot;: &quot;^27.4.1&quot;, &quot;@types/react&quot;: &quot;^17.0.39&quot;, &quot;babel-jest&quot;: &quot;^27.5.1&quot;, &quot;identity-obj-proxy&quot;: &quot;^3.0.0&quot;, &quot;jest&quot;: &quot;^27.5.1&quot;, &quot;react-dom&quot;: &quot;^17.0.2&quot;, &quot;react-test-renderer&quot;: &quot;^17.0.2&quot;, &quot;ts-node&quot;: &quot;^10.7.0&quot;, &quot;typescript&quot;: &quot;^4.1.2&quot; }} jest.config.ts12345678export default { coverageProvider: &quot;v8&quot;, moduleNameMapper: { &quot;\\\\.(css|less)$&quot;: &quot;identity-obj-proxy&quot;, }, testEnvironment: &quot;jsdom&quot;, // 其他均为默认}; babel.config.js123456module.exports = { presets: [ [&quot;@babel/preset-env&quot;, { targets: { node: &quot;current&quot; } }], [&quot;@babel/preset-react&quot;, { targets: { node: &quot;current&quot; } }], ],}; 测试补全解决上一节遇到的问题之后，测试脚本可以顺利运行了，需要稍微补全一下测试用例。因为 CKEditorInput 从 ckeditor-custom-build 中加载 CKEditor 时采用了异步加载，所以用 rlr 的 render() 第一时间拿到的页面源码内显示该方法仍在加载中。通过 screen.logTestingPlaygroundURL() 方法可以获取 Debug Playground 的访问链接，用浏览器打开可以看到下图的内容，清楚明了。利用它还能获取如何查询页面元素的提示。 我期望两个组件都正常渲染，如果 CKEditorInput 组件需要异步加载，那么设置等待即可。测试用例补充为下面的样子： render.test.jsx (2)12345678910111213141516171819202122// 防止加载时间稍长引发 jest timeout 的问题jest.setTimeout(60000);// ...test(&quot;test rendering&quot;, async () =&gt; { render(&lt;TestForm /&gt;).debug(); screen.logTestingPlaygroundURL(); await waitFor(() =&gt; screen.getByText(/hello world!/i), { timeout: 30000, }); await waitFor( () =&gt; { // 根据 debug() 的返回结果发现: // 可以通过获取 &quot;段落&quot; 这个工具栏提示字样来判断是否已经渲染出富文本编辑器 screen.debug(); screen.getByText(/段落/i); }, { timeout: 30000, } );}); 打包、发布完成测试完成后，利用 npm publish 命令将 Hello-richtext 发布到团队的私有制品库中，完成打包流程。在两个实际项目中安装 Hello-richtext@0.0.1 后，组件切实可用。 这次打包发布 React “组件库”的经历给了我几点体会， 首先是学习和实践了 React 组件的测试流程，加深了自己对于前端项目测试的理解； 其次让自己构建 NPM 包的流程更规范，之前发布的几个 NPM 包因为图方便求速度都没有写测试，没测等于没写； 在目前两个可能未来更多的项目开发中事实上地提升了代码复用度和开发体验，原先富文本编辑在各个项目间维护…挺麻烦的。","link":"/2022/03/12/PublishNpmPackageOfReactComponents/"},{"title":"好久不见，前端再见","text":"断断续续写前端项目也有好几年了，从大二接触 Javascript 和 Vue 时的兴奋，到接触小程序和 React 时的“渐入佳境”，再到这段时间的感到无比疲惫，我希望我在前端开发上大规模投入的阶段先暂告一段落了。为什么有这样的疲惫感呢？原因总结起来有以下几点： 前端开发面临的大多不是技术问题，是产品问题或美学问题，最终目标是让用户满意； 前端开发过于琐碎，需要处理的细枝末节极多，每个页面元素都有相应的状态需要管理； 前端难于抽象，对应到用户上则表现为具体业务需求是千变万化的。 写前端的疲惫感从技术上来说绝大部分前端应用并不需要考虑代码的优化，而是需要“取悦”用户，这应该是最让我无法忍受的了。因为前端应用面向最终用户，看得见摸得着的应用就会让用户有点评的冲动，而除了基本的程序正确性问题外，诸如“我觉得这里不好看”这样主观且不确定的问题占绝大多数。相较于后端而言，前端应用面向的用户不知道也不关注技术问题，他们对应用提出的问题是不收敛的，且提出问题的同时往往伴随着需求变动。 而回归到技术上，与其说是我选择了前端框架，不如说是我选择的 UI 组件库替我焊死了框架的门，真正做到 面向 UI 组件库的编程 。目前我们普遍使用的 React + AntDesign 的组合已经成了标准配置，图快写出来的所有的应用外观上和代码上都几乎一样，让我有些视觉疲劳和精神疲劳。 疲劳状态下需要转移注意力，换换脑子接触新的东西，近期发现了重编译时的轻量前端框架 Svelte 的确让我耳目一新，简单清晰的 API 和朴素的文件布局和它的名字完全吻合，让人觉得 Web 开发又回到了似曾相识的从前的模样，熟悉又清新。但很快美梦就破碎了，当我准备用它重构自己用 React 写的网站时发现消息提示，模态对话框，表单验证等前端开发常见场景问题都没有太好的解决方案。回顾了 Bootstrap，有刀耕火种的味道。可能我属于“急于求成”“基础不扎实”的前端开发者吧，如果有时间和机会好好打磨 UI 组件，调整样式和布局应该能够达到非常好的效果。但 React + AntDesign 就跟吃快餐一样，风味欠佳但是效率很高。 现在我觉得是实际业务选择了我，前端框架选择了我。 当我谈写代码时我谈些什么当我提到写代码，脑海里想的是借助程序充分利用计算机的能力，达到我需要完成的计算任务。程序的状态应被准确地刻画，程序的输入输出应被良定义，程序的边界应当较为清晰，程序的正确性可以被验证…而这些与前端开发不能说是毫不相干，也可以说是相去甚远。 人是会犯错的，用动态语言写代码错误频出，而因为我能力有限，用动态语言写前端代码只会使得错误跟我玩捉迷藏，更何况还得面对各路用户提出的需求和美学问题。我希望我写代码时能够保持足够长的、不被打扰的专注时间，让我可以设计并实现程序，而不是时不时考虑表格 A 缺了字段，按钮 B 应当换成主色等琐碎的问题。下面这张图就是上个月某两周的代码提交次数统计情况，在前端应用的开发中需要处理的事情和写的代码都是极其琐碎且没有局部性的。我不认为这么多次的提交都具备平均的单位价值，但可以体现写的代码是东一榔头西一棒子的。 当然这些都是我的主观感受，无知者无畏，我只是窥到了前端的冰山一角便自觉全面接触后难以接受想着要逃。我很喜欢 Vue React Svelte 这些项目，也希望有朝一日可以自己可以搭建出这样的框架。用框架写前端应用和实现前端框架是完全不一样的概念，前者针对业务，后者面向系统。我计划在之后的时间里深入到软件系统的设计与实现当中，在可接触的项目范围内开始转向后端于我而言是理想的开端。编写相似的业务代码给我带来的技术提升和成就感与日俱减，尤其是我饶有兴致地用颇为巧妙的方法实现的前端应用被用户主观地认为“不好看”或者“不易用”时，很难避免这些对应用本身的批评转嫁到自己身上，实现得不好可能也有，但是这种因素占比是多少呢？ 希望一人独处的念头，始终不变地存于心中。所以一天跑一小时，来确保只属于自己的沉默的时间，对我的精神健康来说，成了具有重要意义的功课。至少在跑步时不需要和任何人交流，不必听任何人说话，只需眺望周围的风光，凝视自己即可。这是任何东西都无法替代的宝贵时刻。 —— 村上春树 写前端的收获抱怨了这么多，不是想表达我对前端开发的深恶痛绝，而是“爱之深责之切”(?)。我仍会关注前端领域的发展，甚至还会在自己的网站里尝试新的前端技术，但对于我而言它已经成为“庐山烟雨浙江潮”。它给我带来的收获有几方面，最主要的就是产品思维、审美提升和巩固 Web 基础。 首先产品思维就是站在用户的角度思考产品的问题，以用户为中心。而用户就是更注重产品的美观性、易用性而不注重实现方法和程序效率，软件产品本身不仅是代码和程序，美术素材、按钮图标、菜单样式等等都是其中重要的部分。因为前端应用直接面向最终用户，所以前端开发其实也是在做产品，当然如果纯粹机械地将原型图还原成前端应用则另当别论。 审美提升则是主观和隐性的。因为我所做的前端应用大多是 ToB 模式，不同于电商类应用，没有引流新用户的需要，所以几乎所有界面都是简约风格的，久而久之我对应用的设计风格就有了简约的偏好。最直接的影响是发现“丑”的应用数量增多，间接影响是当我听完一段应用的功能描述后可以在脑海里描摹出大致的页面设计方案，何处需要做一些留白，元素如何合理布局，如何让页面观感舒适等。审美的提升不仅体现在页面实现上，也体现在代码风格上，能写美观的地方我会尽量优化，目录结构、文件命名、甚至是注释的写法也要统一(/** */、//、/* */)。 巩固 Web 基础的巩固则直截了当，DOM API、Typescript、浏览器兼容性 甚至是 jQuery 等杂七杂八的东西都有接触，也开始封装发布自己的 UI 组件和实用工具 NPM 包，成为 NPM 包玩家。Svelte 的出现也把基于 Virtual DOM 前端框架的势头稍微拉回了经典的开发风格，让我体验了一把文艺复兴。 后续计划前段时间业余时间学习了 Rust，希望能够借助 Rust 的学习路径复习操作系统、计算机组成原理和数据库原理等基础知识。阅读 Rust 的中英文文档及博客后深感深切感受到了大一从 C/C++ 上没有感受到的魅力，Rust 编译器像严格的编程教练，用极为严格的语法要求我们写出合规合法的代码，这样一来通过编译器检查的代码就已经达到较高的标准；Rust 中 所有权 和 生命周期 的机制也十分巧妙，能够最大限度地避免内存泄漏和野指针的问题；作为 Mozilla 推出的现代编程语言，内建的类库和原生的 API 都比较趁手。 3月末 Golang 发布了 1.18 正式版，终于推出了泛型编程的特性，到现在还没有真正上手写呢，是时候体验一番了。 后续计划主要用 Golang 或 Rust 这两种编程语言，配合 GraphQL 搭建可拓展的后端服务；自己整体的工程方向朝着微服务、云原生的方向靠拢，尽量扎进计算机系统的核心和底层。","link":"/2022/05/09/GoodbyeFrontEnd/"},{"title":"为什么前端开发要选择 GraphQL","text":"这篇博客其实是以一次公司内的技术分享为基础做的调研和总结归纳，包含了我自己很多不成熟的观点看法。这次分享主要是想向新同事介绍为什么我们选择在项目中大规模使用 GraphQL 而不是更传统更简单的 RESTful API。 GraphQL 是什么在上一篇有关 GraphQL 的博客里，我简单地说明了 GraphQL 的定义及其大致用途，贴了官网链接就开始介绍我使用 GraphQL 的“更优雅的”方式，对 GraphQL 本身描述得并不多。这里又贴一下英文的定义：A query language for your API GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. 划重点，它是一种查询语言和使数据可查询的运行时。作为一种语言，它有自己的语法，能够定义Type, Enum, Input, Fragment, Query, Mutation等元素，熟悉 Typescript 的朋友会对此感到亲切。而在前端开发中需要重点关注的是以下几个部分： Schema: 可以类比为整个 API 的全貌，通过类型系统定义了所有的类型和方法，描述了如何进行数据的查询和修改； Query: 是 Schema 中定义的方法，用于查询数据，无副作用，在后端并行执行； Mutation: 是 Schema 中定义的另一类方法，用于修改数据产生副作用，在后端串行执行； Operation: 是客户端定义的方法，命名和参数自定义，用于调用 GraphQL 的「查询」 根据以上主要元素，使用 GraphQL 进行开发的大致范式如下图所示，当然在这里我简化了后端的工作。首先后端（至少）需要将 API Schema 定义并提供到指定端点上，前端开发者将 Schema 拉取到本地当作接口文档或者是编写 Operation 时代码补全的蓝本，需要调用接口时向指定端点传递 Operations 及其对应参数即可，整个过程十分丝滑。 虽然但是，上述过程本质还是向某个 URL 地址以某种 HTTP 方法传递了某些参数且得到了某些返回数据，和传统的 RESTful API 有什么区别呢？ RESTful API 的劣势RESTful API 是什么下面这是 REST 的定义，比较晦涩难懂。符合 REST 规范的 API 被称为 RESTful API，特点是使用 URI 和 HTTP Method 来区分接口方法。 REST is an acronym for REpresentational State Transfer and an architectural style for distributed hypermedia systems. 规范的 RESTful API 将资源放在 URI 中，如 /api/v1/articles 表示「文章」资源；用 HTTP Method 方法来表示动作语义，如 GET 表示获取，POST 表示更新，PUT 表示新增，DELETE 表示删除等，但是 RESTful 的规范很难实现或维持。我见过很多公开的 API 都直接将动作语义包含在 URI 里，而真正应该用来表达动作的 HTTP Method 则只用 GET 和 POST。最近刚好在接微信的接口，正好拎出来批评一下。 不规范使用 RESTful API 的问题因为 RESTful API 规范很难在开发过程中贯彻执行，所以下面我就归纳了目前常见 RESTful API 的缺陷： 容易过度获取数据或获取数据不足 不支持一次网络请求调用多个接口 前端需要知道所有接口服务的地址，强依赖于接口文档 导致 HTTP Method 滥用，如查询表单字段稍多就写成了 POST 接口 前端开发强依赖于后端，后端接口变化前端反应剧烈 当增加新功能时后端需要整合多个微服务的数据提供给前端，即需要数据网关 接口返回值没有类型，无法利用 TS 的优秀特性 难以编写接口文档，虽然有 OpenAPI/Swagger 等工具，但编写起来仍然很麻烦 API 升版本会直接改变端点 URL，对前端开发影响较大 GraphQL 的改进GraphQL 的诞生可以说是刀法精准，每一处都正中 RESTful 的弱点： 需要什么信息就定义什么字段，不多不少 支持一次请求调用多个接口 所有接口的 URL Endpoint 统一 进一步促进前后端解耦和 API 迭代优化，消除 API 版本 给接口赋予类型系统，代码即文档，无需 OpenAPI 式的补丁式接口说明 支持在线调试和文档查询 IDE 如 GraphiQL 和 GraphQL Playground 目前越来越多公司都开始使用 GraphQL API，包括但不限于 Meta，Github，PayPal，Netflix，Airbnb 还有国内的快手等。 GraphQL 给前端的新可能获得更大的自主性 数据存取上比起 RESTful API 完全隶属于后端，使用 GraphQL 给前端开发更多的自主性 需要用什么数据，用多少数据，怎么用数据以前端为主；后端只需要将 Schema 这个「全集」定义好、测试好 页面增加或减少信息呈现、数据字段改名等操作都可以在前端独立完成 GraphQL Schema 应当是前后端开发者共同制定出来的，强化前端开发在数据层面的重要性 充分利用类型系统的优势 利用代码生成工具，前端开发时可以将 GraphQL 的 Schema 拉取下来，并且参照其类型系统生成对应的 TS 代码，此类工具集大成者是 GraphQL Code Generator 兼容性好：只生成 API 代码，不涉及页面模板，React、Vue 和微信小程序都可以支持，基本涵盖目前需要接触所有类型的前端开发 自由度高：code generator 支持多种插件，可根据开发习惯和偏好进行选择 可维护性强：使用 TS 调用接口后不会在代码逻辑中出现任何与接口名称、URL 地址、HTTP 方法相关的任何硬编码字符串；TS 指定接口参数返回值，避免许多 JS 中常见的运行时错误 声明式的语言提高确定性GraphQL 的查询语言是 type { …fields } 这样声明式的，在 Operations 中定义了什么字段，一切正常的情况下就会返回相应结构的字段，所见即所得。而且，声明式的查询使得接口的确定性更高，消除歧义，前端开发不用猜接口返回的信息包含什么数据。 总结因为从一开始接触 Web 应用开发以来我就开始使用 GraphQL，所以反而是 RESTful API 没有大规模的使用过（只用在自己的玩具应用），对它的认识没有特别深入。但是直观感受是 GraphQL 对前端开发来说只有好处没有坏处，倒是后端实现起来 GraphQL 成本比 RESTful 高一些，毕竟要定义 Schema 和各式各样的方法、类型和参数，比直接定义 URI + Method + Params 的方式要复杂一点。但是一旦 RESTful API 要规范化规模化的话，应该也需要投入大量时间编写接口文档，这个时候两种 API 实现方式的成本就相差无几了。而开发体验上 GraphQL 是完胜的，所以我希望越来越多企业转向 GraphQL API。","link":"/2022/06/03/WhyWeChooseGraphQL/"},{"title":"再见了，兵荒马乱的 2022","text":"今天是2022年12月31日，2022年的最后一天，先给年初的自己说一声对不起：年初立的 Flag 基本没有一条是顺利达成的，我以为目标量化以后就可以逐项实现，但还是高估自己的执行力和时间充裕程度了。今年是我第二个本命年，本应过的虎虎生风，生龙活虎，但是现实没有我想象中那么精彩，但有一些事情还是值得记载。这一年我在专业上经历了迷茫，在情感上遇到波折，在工作上也遭遇了挑战，大环境上也在年底迎来了疫情政策的转折。我想对这一年做个小小的总结，也对明年做些展望和规划。 面对未知要乐观现在回看今年写的几篇博客发现自己还是比较幼稚，喜欢用学生思维来思考问题：“我不想写前端代码了”“我为什么要用 GraphQL 来写前端代码”云云，当时除了觉得前端代码比较聚焦视觉和交互以外，还认为总是写前端代码对之后找基础研发相关的工作不是很有利。现在回过头来看，与其花时间纠结自己一直写前端代码对未来有什么不利的影响，不如把这部分时间投入到自己想学、想做的事情当中。 畏畏缩缩地权衡着每一种选择的利弊，却从未迈出哪怕一步。这或许可以说是谨慎，但是过于谨慎只会让人止步不前。然而我不是自发主动地参悟到这一点，而是参加了今年的秋招后被迫察觉的。我当然没有参加过往年的秋招，但是今年整个社会都在传递就业方面的焦虑情绪，可以大体了解到今年秋招应该是比前两年困难些许的。 暑假本来可能有机会可以去字节跳动实习，但当时因为公司研发任务较重没办法抽出时间参与，错过了一段具有较高含金量的实习经历，我不以为然，觉得没有暑期实习没什么大不了。想着公司手头的事情忙完了再拿出一段时间好好准备秋招，但手头的事情永远忙不完，我就永远没有抽出时间进入准备秋招的“状态”。七月底投了字节的提前批，在 LeetCode 上刷了几十道题就开始面了，第一次面试过于紧张，基础知识回答得还算满意，写代码环节脑子一片空白。稀里糊涂到了三面主管面，这回更紧张了，表现一般最后给挂了。 通过字节提前批的经历我有了心理预期：没有完完整整的时间窗口可以用来专心致志地准备一件事情，不能再用准备高考的状态来准备秋招了。真如鲁迅先生所说，时间就像海绵里的水，只要愿意挤总会有的。那之后我白天去公司上班，晚上回学校刷题复习，八月底九月初密集投递简历，不断润色打磨自己的面试状态，经常一天面试2～3轮，着实是没有硝烟的战场。近三个月的秋招让我产生了线上面试 PTSD，戴上耳机打开会议软件就会自动联想到面对面试官做自我介绍的场景。当然这种 PTSD 不是害怕，是厌倦。 毛主席说过要在战略上藐视敌人，战术上重视敌人，我一直秉持这样的行事态度，力求对待一切事情都“抱最大希望，做最坏打算”，这也要求我对待事情做最好的准备，我喜欢准备 Plan B，在原定计划无法顺利开展时拥有备选方案是令人安心的。秋招对我来说很重要，所以准备需要更充分。（虽然最了最坏打算，但是本能地还是希望有好的结果）一直在准备，从来没行动，这是我对失败的不确定性的恐惧，是害怕失败的二阶导数。以前常听罗翔老师讲段子，他曾提到一句“对待已知的事情要谨慎，对待未知的事情要乐观”。自己熟悉或擅长的东西自己不上心往往容易出差错，我家这边有句谚语的意思大概就是“淹死的往往都是会游泳的”也讲得是这个意思，所以面对已知的事情要敬畏，要谨慎；面对未知的事情自己没有什么办法，只能尝试不同的方法来面对未知，消除未知带来的恐惧，在原地踌躇不前是无法解决问题的。 不过好在我开始行动的时间也不算晚，面试到九月下旬时渐入佳境，也陆陆续续收到了一些大小厂的 Offer（哈哈也包括字节的正式批），结果还算满意吧。 重度“效率瘾”患者“效率瘾”是我在调研笔记软件时看到一篇博客里提到的名词，我觉得很有趣。其大概含义是指如今不少人的最大爱好就是追求高效：今天在应用商店打开“效率”类目查看是否有新的效率提升软件可以帮助自己提升效率，明天在网上调研一番市面上哪款笔记软件比较适合构建知识库，后天开始思考应该如何分配笔记的段落结构使得内容详略得当，信息密度提升。他们执行效率最高的事情就是寻找提高效率的方法，自己却从未尝试或坚持将方法付诸实践，相当于提效的二阶导数。这些人可以说是“差生文具多”，可以说是学习上的“工具党”，甚至可以说是学习之前要做好万全的准备，争取一步到位。 最惊悚的是看到这个名词之后我发现自己就是”效率瘾“患者，而且是重度。我潜意识里希望自己行动之前做好万全的准备，开始时就是最好的也是最终的状态，这很理想化，没有可行性。 工具是次要的，内容是重要的。调研了若干笔记软件，找到了当前最先进的又如何，其中存放着几篇笔记呢？看到这篇博客我被吓了一跳，赶紧关掉所有介绍和对比笔记软件的浏览器标签页。经过一些思考，我总结了两条克服“效率瘾”的办法： 重视内容轻视工具，集中精力投入生产，学习是知识在大脑里的 I/O 过程，看到重要的内容记录下来，之后复习时再整理，整理的过程也是知识碎片重组的过程； 用软件工程的思维看待事物，软件需要不断的迭代更新，成功不是一蹴而就的。生活、工作也一样，不完美和不如意是必然的，有迭代的勇气和能力就足够了。 愿自己能够尽早戒除“效率瘾”，把时间精力用在做有用功上。 对 2023 年的期待今年五月开始打比赛，六月底恋爱分手，紧接着七月底开始了长达三个月的秋招历程，再伴随着疫情抬头我十二月初就早早回到家里远程办公，刚到家就得知疫情防控政策开始放宽，于是等待变阳。今年下半年没有太多“自己的时间”，什么是自己的时间我也说不准，只是一直这么觉得。我无比期待 2023 年能够恢复到 2019 年的生活，我现在已经想象不到出门不用戴口罩的感觉，回忆不起来自由出入学校、出入北京的体验了。疫情三年抹去了研究生阶段诗与远方中“远方”的部分，这诗也唱得呕哑嘲哳，疫情让我本科线上毕业，千万让我毕业时能和北京当面好好道个别。 至于计划，争取每个月发布至少一篇博客吧。 Bye～","link":"/2022/12/30/Goodbye2022/"},{"title":"rFTP - 用 Rust 实现简单的 FTP Server (1)","text":"开发动机核心动力是我有学会一门系统级编程语言的梦想。所以计划用 Rust 为开发语言（手段）完成本科三年级计算机网络专业课上的 FTP 大作业（目标），学习 Rust 的同时巩固计网的基础知识。 虽然大一刚入学就开始接触 C/C++，但是对于当时没有任计算机知识何积累的我来说用这样的方式开始我的编程入门实在是颇为残忍。或许我当时连内存大小和磁盘容量都分不清，不知堆栈为何物，也搞不懂什么编译链接，让我去理解指针实在是有点为难。现在回过头来看，当时的教学顺序对零基础的学生来说是不太友好的：老师在讲指针结构的内存优化时我甚至还写不出像样的符合语法的程序，课程内容就自然也就无法很好地消化了。 如果由我来制定教学计划，我一定将最开始的编程入门课定为使用 Python 教学而不是 C/C++，在知道如何写出鲁棒、高效、优雅的代码前，先要做到能写代码，就好似学会跑步之前需要先学会走路；等学生们了解了计算机组成原理、操作系统等计算机基础知识之后（或同时），再教授 C/C++ 等较低层的、系统的编程语言了。话扯太远了，就此打住。 因为当时的无知无能，我在大学前两年的专业课学习中并没有把基础打牢，现在希望可以通过恶补计算机知识来抢救一下，而与此同时我也希望选择一门系统级语言来作为接触计算机底层的抓手，我对 C/C++ 有些 PTSD，Golang/Java 又太面向应用构建了，所以选择了社区里比较火的 Rust 来入门，可能本质上这也是一种跟风吧。 Rust 基础Rust 的所有权和生命周期机制是它很重要的亮点，同时也是难以上手的概念；Rust 拥有强大的枚举值机制，配合上全面的模式匹配，使得程序控制流和错误处理变得灵活优雅，Golang 没有枚举类型的情况有时显得捉襟见肘；另外和 C++ 不同，Rust 没有继承的概念，而是和 Golang 类似采取“组合大于继承”“组合而非继承”的方式来达到面向对象的目标，以取得更高的编码灵活度。 所有权Rust 所有权三条基本规则： 每个值都有一个所有者； 同一时间只允许存在一个所有者； 当所有者离开作用域，值会被抛弃(drop)。 离开作用域之后，变量会被释放掉(drop)；直接赋值会导致右值的所有权被剥夺；如果只希望借用变量的值，可以使用 &amp; 符号进行借用，如 &amp;var 表示变量的只读借用，而 &amp;mut var 表示变量的可写借用，同一个作用域中只允许存在一个可写借用；可写借用创建出来且仍在存活时，后续再创建的只读借用就会失效（或者编译器编译失败）。 存放在栈上的基本类型可以通过 Copy 特征自动的进行复制，而不是转移所有权。Rust 不允许在实现了 Drop 特征的类型上标注 Copy 特征，编译器会提示编译错误。也就是说 Copy 特征只复制栈上的数据。 除了完全存放于栈上的基本数据类型以外，大部分类型都是主体数据存放于堆上，索引（或者叫指针）在栈上，此时通过 Copy 只会实现指针的复制，堆上的主体数据不会有变化。这时就要借助 Clone 特征来实现堆栈上数据的完全克隆。 利用切片机制可以引用一个连续元素集合当中的部分内容，需要注意的是切片是引用，并没有值的所有权。特别地，String 类型的切片类型写作 &amp;str。 生命周期Rust 生命周期基本三条准则： Rust 编译器会给函数所有的输入变量赋予一个生命周期 如果只有一个输入变量，Rust 编译器会给输出的值赋予这个输入变量的生命周期 如果函数的参数当中有 &amp;self 或者 &amp;mut self，说明这个函数是个方法，此时会给所有的输出值赋予 &amp;self 相同生命周期，以方便编写方法函数，因为这样可以少写很多生命周期符号 结构体与特征在 Rust 中没有 class 关键字，而是不约而同地和 Golang 一样选用了 struct 结构体作为数据整合与面向对象的主要载体。通过 struct typeName {} 代码块可以定义一个结构体，通过 impl typeName {} 代码块可以为该结构体定义和实现内部的方法(methods)。在方法中定义第一个参数为 &amp;self 表示只读引用实例，如果定义为 &amp;mut self 则表示可编辑地引用实例，如果没有传入 self 参数则表示为类方法，需要用 typeName::methodName 的形式调用。 为了将多个结构体之间可能存在的共有方法进行抽象，Rust 提供了特征(trait)机制。Rust 的 trait 与 Java/Golang 当中的 interface 类似，结构体通过实现特征，在某些函数中能够当作该特征的实例变量使用，为结构体实现特征的代码片段为 **impl traitName for typeName {}**。值得注意的是，在定义 trait 时可以给方法提供默认实现，结构体需要把特征当中的不包含默认实现的方法全都实现才能算作实现了该特征，不能部分实现。 在入参和返回值类型中标记实现某个特征的语法如下，配合泛型一起食用口感更佳： 12345678910111213141516171819202122232425262728// 使用 impl 关键字说明特征pub fn notify(item: &amp;impl Summary) {}// 使用泛型约束pub fn notify&lt;T: Summary&gt;(item: &amp;T) {}// 多个特征的约束pub fn notify(item: &amp;(impl Summary + Display)) {}// 使用 where 关键字说明类型特征fn some_function&lt;T, U&gt;(t: &amp;T, u: &amp;U) -&gt; i32where T: Display + Clone, U: Clone + Debug,{}// 泛型结构体针对特定特征的实现struct Pair&lt;T&gt; { x: T, y: T,}impl&lt;T&gt; Pair&lt;T&gt; { fn new(x: T, y: T) -&gt; Self { Self { x, y } }}impl&lt;T: Display + PartialOrd&gt; Pair&lt;T&gt; { // some implementations} 枚举类型及模式匹配Rust 有很强的枚举类型机制，感觉是另一类的结构体，配合 match, if let 等关键字可以优雅地实现控制流和错误处理。定义枚举类型的方式和定义结构体几乎完全一致，使用枚举可以使得相同用途的类型在逻辑上和空间上两个维度上更加紧凑。从各种角度来看枚举和结构体都非常相似，我们甚至可以给枚举类型实现方法。 模式匹配主要依赖 match 和 if let 两个关键字。其中 match 和其他语言中的 switch/case 语法是类似的，只不过 Rust 要求 match 必须穷尽所有的枚举情况，即强制写出 default 分支。这也合理，以编译严格著称的 Rust 不允许潜在的不可达状态。 12345678910match dice_roll { 3 =&gt; add_fancy_hat(), 7 =&gt; remove_fancy_hat(), _ =&gt; reroll(), // 抛弃默认值}match dice_roll { 3 =&gt; add_fancy_hat(), 7 =&gt; remove_fancy_hat(), other =&gt; move_player(other), // 使用默认值} if let 是单分支、更精准的 match，如果匹配的枚举值符合预期就会进入到相应的语句块里执行相应的语句，其代码块为 **if let Some(var) = some_option {}**。 rFTP 开发开发计划因为自己还有课题组的开发任务，业余时间也不多，所以 rFTP 的开发计划也比较宽松随性，突出一个“重在参与”。计划迭代两期，其中第一期包括基本的 FTP 指令如： USER/PASS，指定用户名和密码登录 PORT/PASV，主动和被动模式的数据端口指定 RETR/STOR，存取文件 ABOR/QUIT，断开连接 SYST/TYPE，获取服务器信息 RNFR/RNTO，文件重命名 PWD/CWD/MKD/RMD，切换当前会话的所在目录 LIST，列出当前目录文件列表 迭代的第二期计划加入相对高级的指令： REST，断点续传 DELE，删除文件 STOU，唯一存储 APPE，追加写 ALLO，预留存储空间 I/O 多路复用本科期间用 C 写 FTP Server 时跟风使用 epoll 来达到 I/O 多路复用的效果，当时对于 I/O 多路复用处于完全不明白的状态。现在对其一知半解，也打算在 rFTP 中引入这样的机制。因为是在 macOS 上开发，尝试引入 epoll 后代码无法正常编译运行，查阅相关资料才发现 macOS 并不支持 epoll，而是单独开发维护一个文件系统事件库叫 kqueue 来实现类似的功能。 为了 rFTP 的可移植性，同时也为了省心，我自然地选择引入了 Rust 的异步运行时 Tokio 来达到 I/O 多路复用、异步编程、多线程的实现目标。目前对 Tokio 的认识还停留在上手阶段，没有做深入了解和 Benchmark，不太敢说其性能如何如何，以下是 Tokio 文档中的介绍，看起来让人安心。 A runtime for writing reliable network applications without compromising speed.Tokio is an event-driven, non-blocking I/O platform for writing asynchronous applications with the Rust programming language. At a high level, it provides a few major components: Tools for working with asynchronous tasks, including synchronization primitives and channels and timeouts, sleeps, and intervals. APIs for performing asynchronous I/O, including TCP and UDP sockets, filesystem operations, and process and signal management. A runtime for executing asynchronous code, including a task scheduler, an I/O driver backed by the operating system’s event queue (epoll, kqueue, IOCP, etc…), and a high performance timer. 一个疑难问题Rust 如何实现 TcpStream 在生产者、消费者不同的作用域之间的传输？ 预期状态：服务器 listen 进入 accept loop 之后，每次接收到一个 socket，我希望可以通过 mpsc::channel 将这个 socket 和 addr 信息传输到另外一个事件循环中进行处理，但在把 socket 传到 Task 当中时，客户端出现断连的现象，暂时还没发解决。大致的代码如下： main.rs123456789101112131415161718192021222324252627282930fn main() { let (tx, rx) = mpsc::channel(N); for i in 0..N { tokio::spawn(async move { loop { let task = rx.recv(); let mut socket = task.socket; let mut buf = vec![0u8; 1024]; loop { let n = socket.read(&amp;buf); if n == 0 { break; } // do something with buf and respond to peer socket } } }); } loop { let (socket, addr) = listener.accept(); tx.send(Task{ socket, // socket closed here // Client meet error: Connection reset by peer addr, }); }} 疑难问题的权宜解决方法经过一两天的思考，觉得跨作用域传递 socket 变量是不好的实践方式，于是将上述代码改为了下面的样子：直接在服务器监听方法当中对来到的 socket 进行读取和相应处理，不再跨作用域转移变量。 main.rs1234567891011fn main() { loop { let (mut socket, addr) = listener.accept(); tokio::spawn(async move { loop { // do something with socket and addr } }) }} 参考资料 rfc959 epoll(7) - Linux manual page Mac OS X Manual Page for kqueue(2) Kernel Queues: An Alternative to File System Events The Rust Programming Language Tokio - An Asynchronous Rust Runtime","link":"/2023/01/20/RFTP-1/"},{"title":"用两年读完《白鹿原》","text":"陈忠实先生花了六年时间著出《白鹿原》这部巨作，而我却前前后用了近两年时间才读完这部作品，实在惭愧。因为用的是“微信读书”读的电子书，阅读记录都有存储在其平台上，才能得知我原来在 2022 年看完《杀死一只知更鸟》之后就开启了《白鹿原》。然而，过去在学校因为事情众多难以聚焦到“读闲书”上，所以 2022 年的 2 月开启这部书的阅读，走入白鹿原的广袤天地之后就一度搁置。 直到 2023 年毕业工作后的某个瞬间，决定要重拾读书计划之后才想起在一年前的某个时间我曾读过这本书的三分之一。过去了这么长时间，我还能明确地记得我读到三分之一的节点是白灵和鹿兆海用掷铜钱的形式分别投身国共两党追求理想而愉快分别的场景，再次拾起此书时两人分别时欣喜的场景还历历在目，但后来的结局竟是如此，实在令人惋惜… 过去没有写读后感的习惯，最多在中学时因为看书中人物罹难后过于激动而在 QQ 空间发动态抒怀。但不得不说毕业后独居的生活给予了我更多的思考时间，让我能够在品读作品之余思索其中深意，感慨书中角色之悲壮或卑劣，也算是双面硬币里好的一面了。 向外输出是向内消化的高级姿态，我希望通过写出自己的感受加深自己对作品的理解。 白鹿原之白鹿白鹿精魂《白鹿原》中最具神话色彩的便是多次出场的白鹿精魂，每次露面总能带来祥瑞之兆：是白嘉轩看中天字宝地时的灵光乍现，是久旱逢甘露的峰回路转，是瘟疫肆虐时的杀毒祛邪，更是白灵托梦时的凄惨和不舍。白鹿是白鹿原的神，“白毛白腿白蹄，那鹿角更是莹亮剔透的白”，如此洁白无瑕的形态本身就预示着其极其圣洁的存在。 因为白嘉轩是主角，所以白鹿总是围绕着他腾挪欢跃，事实上每个人都有自己的白鹿，都有属于自己的精灵。白鹿的存在是预示着灾厄、苦难和煎熬终将抵达终点，上天会眷顾勤劳勇敢的人们。白嘉轩一辈子行事光明磊落，从不堕入歪门邪道，坚毅刚强，从不落人把柄，成人后作为一族之长秉持着家族的尊严和荣耀，尽管有时其作为族长摆出的架子让人看起来封建古板、不尽人意，但其行为处事最后往往能够收获不坏的结果，足以深孚众望，让他在几十年的族长位子上坐得稳稳当当。贯穿全书的是白嘉轩如同上帝一般的全局视角，无论经历什么灾祸或是幸事，总能够达到波澜不惊、宠辱偕忘的境地，只有在亲人遭遇苦难和不幸时才会流露出一丝失措和动容。我无法想象他是用一种怎样强大的心态来面对周遭世事的，是半生的经历让他有了足够的见识阅历不至于慌张，还是心中一直装着何种信仰？我不认为我在知天命之年能有他那般姿态，或者永远不会。或许白嘉轩和白鹿一样，他们的存在是这整部作品里唯一不现实的意象吧。 白和鹿白嘉轩和鹿子霖的家族恩怨贯穿了整部作品，虽然众多事件仅发生在白鹿原上的小村落里，但其中的复杂程度远超我所看过的宫斗剧。族长白嘉轩取了七房妻子才最终留下后人，令人可笑又可叹；他为了得到天字号宝地，不惜跟鹿家以多换少，双方都以为自己稳赚不赔······ 白嘉轩总是以德报怨，以德服人，不求功名，爱护族人，有底线有原则，虽然思想保守不求变革，但把众多家风传统很好地传承给了下一代，文武义三兄弟都沐浴在这种家风之中四平八稳地生活着，少走弯路。 而鹿子霖则是在相反的方向几乎走到了极端。因为族长位置始终被白家占据，鹿子霖为了“平衡”这种局势，始终极为贪恋权力，从乡约做到保长，坏事做尽。欺上瞒下，狐假虎威，没有底线，贪恋财色，色厉内荏，可以形容他的贬义词汇实在太多，但不得不认清的是：或许他的子集才是彼时此地较为常见且现实的封建家长形象。乐观来讲，我相信不是所有人都有他那么坏，但他的坏是众人之恶的集合，把他拆开揉碎了，就是白鹿原上的所有普通人。 鹿子霖始终期望能够在面子和里子上赢过白嘉轩，但结果总是过犹不及。直到最后鹿子霖被白孝文主持的枪毙大会吓疯后在路边捡拾吃食，用疯癫的语气询问路过的白嘉轩是否要尝尝他的珍馐美味，这场持续半个世纪的与白嘉轩的争斗才遗憾告负。此时的白嘉轩也叹息坦白道他这辈子光明磊落，唯独只背着鹿子霖做过一件对不起他的事，也不过是买下鹿家一块地用来做坟。鹿子霖父亲被土匪杀害时他没疯，鹿子霖二儿子鹿兆海战死时他没疯，鹿子霖锒铛入狱坐了两年大牢他没疯，偏偏在解放后白孝文作为县长主持的批斗会后发疯，不是因为他不够坚强或是胆小如鼠，而是他奋斗了一生的、希望赢过白家的目标，被白嘉轩的从容不迫、白孝文的位高权重和靠山田福贤的倒塌而变得不可实现。压在鹿子霖心中的大山没有移走反而更加厚重，常年如履薄冰的他终于堕入了冰冷的水中。 人与原“白嘉轩就是白鹿原”，这是陈忠实先生在《白鹿原》的后记中提到的。这一说法也自然地解答了我的疑惑，为什么我认为白嘉轩是如同神一样的存在，正因为他在任何时候都能足够稳定、冷静和从容地分析决断，体面地处理家里和族中的大小事务，令人心悦诚服，到头来发现其所做出的决断带来的结果总是不赖。此般沉着稳重和波澜不惊俨然如黄土高原积攒千万年巍然不动的黄土，深厚且沉静。虽然没有去过西北，没亲眼见过黄土高原，但凭借零星的影视作品和我单薄的想象力，也能够在脑海中描绘其壮观雄浑的景象。 田小娥和他的四个男人我讨厌田小娥。她是个不知廉耻、人人唾弃的“婊子”，是宁静乡村里和谐生活的毒药，是男人淫欲贪欲的放大器。他一生和很多男人有过纠葛，吸引和释放着男人的欲望，自由且作恶的一生被公公残酷终结，最终化作魂魄也被镇压在高塔之下，不得安生。田小娥是自由的，是进步的，是开放的，但在那个年代自由、进步、开放的就是“婊子”。而封建社会的女人家最好就是安安分分嫁出去，在婆家勤勤恳恳服侍丈夫和公婆。可恨之人亦有可怜之处，田小娥错就错在于那样的年代投胎成为了白鹿原上的女子。 郭举人田小娥作为郭举人的小妾，并没有得到郭举人的一丝宠爱或“临幸”，而被当做了工具——“泡枣”的容器。读到这里我感到非常震撼，一是感叹旧社会底层的人不配做人，二是认为这样的食物不健康不卫生，让我作呕。我起初其实非常不理解家境不差的田秀才为什么会把自己的女儿委身到年近半百的郭举人家中去，读书人应当是知书达理的，是思想进步开放的，为什么会做出这样不可理喻的决定的？后来我明白了，田秀才读的书不能让他体恤他人、理解女儿，只是让他成为庸儒，成为唯上的学问家，宁可和其他读书人向上兼容地“联姻”，也不愿向下从庄稼汉里找亲家。这样迂腐的学问早就应当和这样迂腐的王朝一同被推到历史的垃圾桶里。 黑娃前半生坏事做尽，后半生浪子回头。 黑娃，鹿兆谦，从小到大没干过一件体面、冷静的事情，但在百年前的农村环境下，其敢于和田小娥私奔回家甚至期望和田小娥拜堂成亲，已经表现出其独特的勇气和魄力。黑娃拥有一种未经驯化的、没有规则的、充满棱角的勇气和魄力，这样的特质也让他之后闹农协、搞暴动、入军队、当土匪等一些行为都变得顺理成章自然而然。他的勇敢是值得赞赏的，但其对田小娥始乱终弃的行为间接导致了田小娥的进一步堕落。黑娃闹农协出走之后几乎没有回过白鹿原，更不用说再回到那个冰冷又温暖的窑洞。他们的窑洞今后会有若干的、爱黑娃的、憎黑娃的男人会踏足，可黑娃在田小娥死之前却始终不曾回来过。 黑娃从山里被招安到保安团后发生的转变我愿称为奇迹，突然从直率莽撞、目无法纪的土匪变成了文质彬彬、一心想学的温柔君子。向贤者朱先生认真求学并得到了朱先生的高度赞扬，“我最好的弟子竟然是一个土匪”。而我以为黑娃会有个很好的结局，可是没想到和白灵一样，好人没有落到好的下场。属实让我觉得惋惜。 鹿子霖鹿子霖与白鹿村及周边村子的几十名女子有染，据说如果其私生子、干儿子齐聚一堂的话需要坐好几桌。田小娥便是这些与鹿子霖有染的女子之一。但鹿子霖对田小娥完全没有情感，就是嫖客和妓女的关系，或者是工人和工具的关系，虽然在这样的关系当中他们也曾和谐和快乐过。鹿子霖周期性地来到田小娥的窑洞与其偷情，顺便施以小恩小惠，让田小娥糊涂地以为这段关系能长久持续。快乐并不长久，美人计得逞之后的鹿子霖得意忘形：田小娥在她眼里从来都不是能够平等交流的人。“兔死狗烹”的想法流露出来之后田小娥给了他一巴掌，这整本书唯一的一次女人扇向男人的耳光，竟然是田小娥挥向鹿子霖的。 白孝文白孝文完全沦陷于荡妇田小娥没有底线没有原则的攻势，这是被包办婚姻的他没有品尝过的激情与爱意。白嘉轩给儿子们娶媳妇时看重的是这位女子能够给家庭带来什么，并不会考虑儿子们的喜恶，娶回来的媳妇自然就不一定得到儿子的疼爱。所有的包办婚姻都是如此。先结婚生子再谈感情，或者不用谈感情的情况比比皆是，两人举案齐眉、相敬如宾，客客气气地深受家长喜爱，但就是不讨白孝文喜欢。而田小娥的身体和话语则让白孝文魂牵梦绕挥散不去，以至于最后跟家里闹得妻离子散，分家后变卖家产也不给他的原配夫人一分一毫，甚至其饥荒饿死也不曾引来白孝文的一丝悔恨和一滴泪水。 白孝文是田小娥罪恶的巅峰，也是其性命走向终点的引线。忠厚善良的鹿三为了结这一切荒谬的闹剧，在一个夜里把田小娥给杀死了… 鹏海之于白灵白灵的死让我意难平但又难以评价，但写到这里其实已经没有太多表达欲了。不知道为什么在阅读白灵、鹿兆海和鹿兆鹏的故事情节时我总能联想到几年前玩过的《隐形守护者》，可能国共两党从合作到决裂，国民党反动派肆意屠杀异见者的情节总是让人想到谍战吧，但不同的是白灵是这部谍战片中的女主角。鹿兆海像个小孩，革命上和感情上跟白灵打了一辈子的赌；白灵却早就长大成人，在战乱的年代选择了革命爱情。白灵是崭新的、向上的、自由的、进步的、顽强的、不屈的、喷薄的、新中国的新一代的力量，她的选择必定成功。 最后，为什么鹿兆鹏与怀孕中的白灵告别后再也没有出现，没有救下因猜忌而惨死的白灵，是我读这部作品的不解和遗憾。 总结《白鹿原》描绘了从清末到新中国近半个世纪的时间跨度下，白鹿原中白家和鹿家两代人的牵扯和纠葛，刻画了白嘉轩、白孝文、鹿子霖、田小娥、鹿兆鹏、鹿兆海、黑娃等栩栩如生的形象，展现了国共两党从北伐蜜月期到国民党临阵倒戈的暗流涌动和真刀真枪，揭示了旧社会旧中国把女人当工具，把穷人当牲口的丑恶面目，由点及面地从白鹿原小家族入手讲述了整个中国农村环境的变迁，是一部恢弘的史诗，也是一部写实的影集，值得思索和品味。 题外话，看完《平凡的世界》和《白鹿原》两部作品后，真心希望自己能够抽出时间游览大西北，领略大家笔下的黄土高原是何等的伟壮。","link":"/2023/12/31/WhiteDeerPlain/"},{"title":"当我执行 kubectl create 时发生了什么[译]","text":"翻译自 What happens when … Kubernetes edition!。我认为这篇文章写得生动有趣，且在关键位置都给出了有价值的链接，引导进一步的阅读学习，让我有了重读并翻译的冲动。 如果我希望往 Kubernetes 集群当中部署 nginx，我大概率会在命令行输入下面这样的命令并敲下回车键： 1kubectl create deployment nginx --image=nginx --replicas=3 几秒之后，我应该能看到三个 nginx 的 pod 散布在集群的工作节点上。这很神奇！但这个过程背后究竟发生了什么？ 关于 Kubernetes 的惊人的特点是，它通过用户友好的 API 处理工作负载的部署。其中的复杂性被简单的抽象隐藏起来。但为了充分理解它所提供的价值，了解其内部工作原理也是很有用的。本指南将引导您了解从客户端 kubectl 到 kubelet 的请求的完整生命周期，并在必要时链接到源代码（或者相关文档和博客）来进一步说明正在发生的事情。 这是一份不断修订的文档。如果您发现可以改进或重写的地方，欢迎贡献！ kubectl校验和生成器好的，我们开始吧。我们刚刚在终端里敲击了回车键，现在会发生什么？ 首先 kubectl 会进行客户端校验，该过程保证了应当出错的请求尽早地出错，而不是在发送给 kube-apiserver 之后再返回错误，例如创建了一个不支持的资源或者一个异常的镜像名。这个校验过程通过减少不必要的负载提升了系统的性能。 校验完成后，kubectl 开始组装将要发送到 kube-apiserver 的 HTTP 请求。任何希望访问或者改变 Kubernetes 系统状态的请求都会经过 API server 并最终与 etcd 进行交互。kubectl 也一样，为了构建这样的 HTTP 请求，kubectl 使用了名为生成器的抽象来完成序列化过程。 可能不太明显的是，我们实际上可以使用 kubectl run 指定多个资源类型，而不仅仅是 Deployments。为了实现这一点，如果没有使用 --generator 标志显式指定生成器名称，kubectl 将推断出资源的类型。 例如，具有 --restart-policy=Always 标志的资源被视为 Deployments，而具有 --restart-policy=Never 的资源被视为 Pods。kubectl 还会确定是否需要触发其他操作，例如记录命令（用于滚动更新或审计），或者该命令只是通过 --dry-run 标志来指定的模拟运行。 在意识到我们想要创建一个 Deployment 之后，kubectl 将使用 DeploymentAppsV1 生成器根据我们提供的参数生成一个运行时对象。“运行时对象”是一个通用术语，用于表示资源。 API 组别和版本的协商在继续之前需要指出的是，Kubernetes 使用 “API 组” 这样的版本化 API。API 组的目的是将相似的资源进行分类，以求更容易理解。它还提供了一个比单一的单体 API 更好的选择。Deployment 的 API 组名为 apps，最新的版本是 v1。这就是为什么在 Deployment 的清单的顶部需要使用 type apiVersion: apps/v1。 无论如何，在 kubectl 生成运行时对象之后，它开始查找适当的 API 组和版本，并组装一个版本化的客户端，该客户端知道资源的各种 REST 语义。这个发现阶段被称为版本协商，其中 kubectl 扫描远程 API 上的 /apis 路径，取回所有可能的 API 组。由于 kube-apiserver 在此路径上以 OpenAPI 格式公开其模式文档，所以客户端可以轻松完成 API 发现。 为了提高性能，kubectl 还将 OpenAPI 模式缓存到 ~/.kube/cache/discovery 目录中。如果你想看到这个 API 发现的过程，可以尝试删除该目录，并运行一个带有最大值 -v 标志的命令，之后可以看到所有试图找到这些 API 版本的 HTTP 请求。有很多！ 最后一步是实际发送 HTTP 请求。一旦发送请求并收到成功的响应，kubectl 将根据预期的输出格式打印出成功消息。 客户端认证在之前的步骤中我们没有提到客户端认证，这是在发送 HTTP 请求之前处理的，所以现在让我们来看看这个过程。 为了成功发送请求，kubectl 需要进行身份验证。用户凭据基本上都存储在位于磁盘上的 kubeconfig 文件中，但该文件可以存储在不同的位置。为了定位它，kubectl 执行以下操作： 如果提供了 --kubeconfig 标志，则使用该文件。 如果定义了 $KUBECONFIG 环境变量，则使用该变量。 否则，查找推荐的主目录，如 ~/.kube，并使用找到的第一个文件。 解析文件后，kubectl 确定要使用的当前上下文、要指向的当前集群以及与当前用户关联的任何身份验证信息。如果用户提供了特定标志的值（例如 --username），则优先使用这些值，并将覆盖 kubeconfig 中指定的值。一旦获得这些信息，kubectl 将补全客户端的配置，以便适当地组装 HTTP 请求： x509 证书使用 tls.TLSConfig 发送，这也包括根 CA Bearer token 放置在 “Authorization” HTTP 头中发送 用户名和密码通过 HTTP 基本身份验证发送 OpenID 身份验证过程由用户在之前手动处理，会生成一个像 Bearer token 一样发送的 token kube-apiserver身份验证我们的请求已经发送出去了，太棒了！接下来呢？该轮到 kube-apiserver 出场了。正如我们之前提到的，kube-apiserver 是客户端和系统组件用来持久化和获取集群状态的主要接口。为了开展其工作，它需要能够验证请求者的身份。这个过程被称为身份验证。 kube-apiserver 如何对请求进行身份验证呢？当服务器首次启动时，它会查看用户提供的所有 CLI 标志，并组装一个合适的身份验证器列表。举个例子：如果传入了 --client-ca-file 参数，它会添加 x509 身份验证器；如果看到 --token-auth-file 参数，它会将 token 身份验证器添加到列表中。每次接收到请求时，它会通过身份验证器链验证请求，直到有一个成功为止： x509 验证处理程序将验证 HTTP 请求是否使用由 CA 根证书签名的 TLS 密钥进行加密。 bearer token 验证处理程序将验证 HTTP 请求中提供的令牌（在 HTTP Header 中 Authorization 字段中指定）是否存在于 --token-auth-file 参数指定的磁盘文件中。 basic auth 验证处理程序将类似地确保 HTTP 请求的基本身份验证凭据与其自身的本地状态匹配。 如果每个身份验证器都失败，该请求将失败，并返回一个聚合错误。如果身份验证成功，Header 中 Authorization 字段将被删除，并将用户信息添加到其上下文中。这使得之后的步骤（例如鉴权和准入）能够访问先前确立的用户身份。 请求鉴权好的，请求已经发送出去，kube-apiserver 已成功验证我们的身份。松了一口气！然而，我们还没有结束。我们可能是我们自己说的那个身份，但我们是否有权限执行此操作呢？毕竟，身份和权限是不同的。为了让请求继续执行，kube-apiserver 需要对请求进行鉴权。 kube-apiserver 处理鉴权的方式与身份验证非常相似：根据输入的标志，它将组装一个鉴权器链，针对每个传入的请求依次运行。如果所有鉴权器都拒绝请求，请求将导致 Forbidden 的响应，并且不再继续处理改请求。如果单个鉴权器批准请求，请求将继续进行。 Kubernetes v1.8 提供的一些鉴权器示例包括： webhook，与集群外的 HTTP(S) 服务进行交互； ABAC，强制执行在静态文件中定义的策略； RBAC，强制执行由管理员作为 k8s 资源添加的 RBAC 角色； Node，确保节点客户端（即 kubelet）只能访问托管在自身上的资源。 可以通过查看每个鉴权器的 Authorize 方法，了解它们的工作原理。 准入控制好的，到目前为止，我们已经通过了 kube-apiserver 的身份验证和请求鉴权。那接下来呢？从 kube-apiserver 的角度来看，它相信我们是谁并允许请求继续执行，但在 Kubernetes 中，系统的其他部分对于什么应该和不应该发生有严格的要求。这时候准入控制器就开始发挥作用了。 虽然鉴权的重点是判断用户是否有权限，但准入控制器拦截请求以确保其符合集群的更大范围的预期和规则。它们是对象持久化到 etcd 之前的最后一道控制屏障，因此它们封装了剩余的系统检查，以确保操作不会产生意外或负面的结果。 准入控制器的工作方式类似于验证器和鉴权器，但有一个区别：与验证器和授权器链不同，如果单个准入控制器校验失败，整个链条将中断，请求将失败。 准入控制器设计的真正精妙之处在于其专注于促进可扩展性。每个控制器都存储为 plugin/pkg/admission 目录中的插件，并且被设计为满足一个小接口。然后，每个控制器都被编译到主要的 kubernetes 二进制文件中。 准入控制器通常按照功能分为资源管理、安全性、默认设置和引用一致性几类。以下是一些负责资源管理的准入控制器的示例： InitialResources：根据过去的使用情况为容器的资源设置默认限制。LimitRanger：为容器的请求和限制设置默认值，或对某些资源配置上限（例如内存不超过 2GB，默认为 512MB）。ResourceQuota：在命名空间内统计或拒绝分配一定数量的对象（pod、rc、service 负载均衡器）或总消耗的资源（CPU、内存、磁盘）。 etcd到目前为止，Kubernetes 已经完全检查了传入的请求，并且允许其继续执行。接下来，kube-apiserver 对 HTTP 请求进行反序列化，从中构建运行时对象（类似于 kubectl 的生成器的逆过程），并将它们持久化到数据存储中。让我们来详细解析一下这个过程。 kube-apiserver 怎么知道接受我们的请求时该做什么呢？在任何请求被处理之前都有一系列复杂的步骤。让我们从起点开始，也就是二进制文件首次运行时： 当 kube-apiserver 二进制文件运行时，它创建一个服务器链，用于支持 apiserver 的聚合。这只是支持多个 apiserver 的一种方式，我们不需要担心这个。 在这个过程中，会创建一个通用的 apiserver 作为默认实现。 生成的 OpenAPI 模式补充了 apiserver 的配置。 kube-apiserver 然后遍历模式中指定的所有 API 组，并为每个 API 组配置一个存储供应器作为通用的存储抽象，kube-apiserver 在访问或修改资源状态时需要与其进行交互。 对于每个 API 组，它还会遍历每个组版本，并为每个 HTTP 路由安装 REST 映射。这让 kube-apiserver 能够正常映射请求，并在找到匹配项后把请求代理给正确的逻辑处理。 对于我们的特定用例，会注册一个 POST 处理程序，该处理程序将进一步代理给一个创建资源的处理程序。 截至目前，kube-apiserver 已经完全了解了存在的路由和内部映射，在请求到来时能将其转发到正确的处理程序和存储供应器。现在设想我们的 HTTP 请求已经到达： 如果处理程序链能够将请求与一组模式匹配（即我们注册的路由），它将把请求分发到为该路由注册的专用处理程序。否则，它将回退到基于路径的处理程序（例如调用 /apis 时的情况）。如果没有为该路径注册处理程序，则会调用一个未找到的处理程序，导致返回 404 错误。 幸运的是，我们有一个名为 createHandler 的注册路由！它的工作原理是什么呢？首先，它会解码 HTTP 请求并执行基本验证，例如确保提供的 JSON 与我们对版本化 API 资源的预期相符。 进行审计和最终的准入。 通过代理给存储供应器将资源保存到 etcd 中。通常，etcd 键的形式为 &lt;namespace&gt;/&lt;name&gt;，但这也是可配置的。 捕获任何创建时的错误，最后存储供应器执行 get 调用以确保对象实际上已创建。然后，如果需要进行其他的最终处理，它会调用任何创建后(Post-create)处理程序和装饰器。 构建并返回 HTTP 响应。 步骤很多！通过追溯这些步骤，我们能看到 apiserver 实际上做了多少工作。所以总结一下：我们的 Deployment 资源现在存在于 etcd 中。但是其中还有一些尚未完成的流程，所以目前我们还没办法看到它… 初始化器在将对象持久化到数据存储中后，只有在一系列初始化器运行完成之后，该对象才会对 apiserver 或调度程序完全可见。初始化器是与资源类型相关联的控制器，在资源对外界可见之前对资源执行相关逻辑操作。如果某个资源类型没有注册任何初始化器，则会跳过此初始化步骤，资源会立即对外可见。 正如许多优质的博客文章介绍的，这是一个强大的功能，因为它让我们能够执行通用的引导操作。例如： 将代理边车容器注入到公开端口 80 的 Pod 中，或者具有特定注释的 Pod 中。 向特定命名空间中的所有 Pod 注入带有测试证书的卷。 如果一个 Secret 的长度小于 20 个字符（例如密码），阻止其创建。 initializerConfiguration 对象允许我们声明哪些初始化器应该针对特定的资源类型运行。想象一下，如果我们希望在每次创建 Pod 时运行自定义的初始化器，我们可以这样做： 12345678910111213apiVersion: admissionregistration.k8s.io/v1alpha1kind: InitializerConfigurationmetadata: name: custom-pod-initializerinitializers: - name: podimage.example.com rules: - apiGroups: - &quot;&quot; apiVersions: - v1 resources: - pods 创建完这个配置后，它会将 custom-pod-initializer 添加到每个 Pod 的 metadata.initializers.pending 字段中。初始化器控制器会定期扫描新的 Pods。当初始化器检测到一个 Pod 的 pending 字段中有自己的名称时，它会执行相应的逻辑。完成逻辑处理后，它会从 pending 列表中移除自己的名称。只有列表中第一个名称的初始化器才能对资源进行操作。当所有的初始化器完成逻辑处理并且 pending 字段为空时，该对象将被认为已经初始化。 细心的你可能已经发现了一个潜在的问题。如果资源在 kube-apiserver 中不可见，用户自定义的控制器如何处理这些资源呢？为了解决这个问题，kube-apiserver 提供了一个 ?includeUninitialized 查询参数，它返回所有对象，包括未初始化的对象。 控制循环Deployments 控制器现在我们的 Deployment 记录已存储在 etcd 中，并且任何初始化逻辑都已完成。接下来的步骤涉及设置 Kubernetes 所依赖的资源拓扑结构。我们可以这样想，一个 Deployment 实际上只是一组 ReplicaSet，而一个 ReplicaSet 是一组 Pod。那么 Kubernetes 是如何通过一个 HTTP 请求来创建这样的多层级结构的呢？这其实是 Kubernetes 内置的控制器的作用。 Kubernetes 在整个系统中广泛地使用“控制器”。控制器是一个异步逻辑，用来将 Kubernetes 系统的当前状态与期望的状态进行协调(reconcile)。每个控制器都有自己的任务，并和 kube-controller-manager 组件一起并行运行。让我们先介绍接管工作的第一个控制器，即 Deployment 的控制器。 在 Deployment 的记录存储到 etcd 并初始化后，kube-apiserver 使其对外可见。当这个新的资源可用时，它会被 Deployment 控制器检测到，Deployment 控制器的工作是监听对 Deployment 记录的变动。在我们的例子里，控制器通过 informer 为资源新建的事件注册了一个特定的回调函数（有关此内容的更多信息，请参见下文）。 当我们的 Deployment 首次可用时，这个回调处理程序将被执行，并首先将对象添加到内部工作队列中。当控制器处理我们的对象时，它通过标签选择器查询 kube-apiserver 检查出我们的 Deployment 没有与之关联的 ReplicaSet 或 Pod 记录。有趣的是，这个同步过程是与状态无关的：新的记录和老的记录协调方式相同。 在发现没有对应的 ReplicaSet 或者 Pod 记录后，它会通过一个弹性进程创建一个 ReplicaSet 资源，为其分配一个标签选择器，并给它分配版本号为 1。ReplicaSet 的 PodSpec 是从 Deployment 的配置清单中复制过来的，当然也包括其他相关的元数据。在此之后，有时还需要更新 Deployment 记录（例如，如果设置了处理截止时间）。 之后会更新 Deployment 的状态，并重新进入相同的协调循环，直到 Deployment 达到期望的完成状态。由于 Deployment 控制器只关注创建 ReplicaSet，因此这个协调阶段需要由下一个控制器继续进行，也就是 ReplicaSet 控制器。 ReplicaSets 控制器在前面的步骤中，Deployment 控制器为我们的 Deployment 创建了第一个 ReplicaSet，但我们还没有看到 Pod。这之后 ReplicaSet 控制器将发挥作用！它的任务是监听 ReplicaSet 及其依赖资源（Pod）的生命周期。与大多数其他控制器一样，它通过在特定事件上触发处理程序来实现这个功能。 首先我们来看资源创建事件。当创建了一个 ReplicaSet（由部署控制器负责）时，ReplicaSet 控制器会检查新 ReplicaSet 的状态，并发现当前状态与预期状态之间存在的差异。然后它尝试通过增加 ReplicaSet 的 Pod 数量来调解这个状态。它非常谨慎地创建这些 Pod，确保 ReplicaSet 的突发计数（它从其父级部署继承的）始终保持匹配。 Pod 的也是批量创建的，从 SlowStartInitialBatchSize 开始，每次成功创建后扩大一倍，以一种类似于“慢启动”的方式进行。这样做的目的是减轻同时出现大量 Pod 启动失败时（例如，由于资源配额不足）引发 kube-apiserver 负载过高，同时能够减少不必要的 HTTP 请求。如果组件会失败报错，我们最好以对其他系统组件的影响最小的方式来优雅地失败！ Kubernetes 通过 Owner References（这是子资源中的一个字段，用来引用其父级的 UID）来保证对象的层级结构。这不仅确保一旦由控制器管理的资源被删除，子资源就会被垃圾回收，还为父资源提供了一种有效的方式以避免它们争夺子资源（设想一下两个父级认为它们拥有同一个子资源的情况！）。 Owner Reference 设计的另一个微妙好处是它是有状态的：如果任何控制器重新启动，由于资源拓扑结构独立于控制器，它的宕机状态不会影响更多的组件。这种对隔离的关注也包含在控制器本身的设计中：它们不应该管理它们没有明确声明拥有的资源。控制器应该在所有权的声明中进行选择，并且不干扰、不共享。 无论如何，回到 Owner Reference！有时系统中会出现“孤立”(orphaned)的资源，该情况通常由以下原因导致： 删除了父资源，但没有删除其子资源。 垃圾回收策略禁止删除子资源。 发生这种情况时，控制器将确保孤立资源被一个新的父级资源接管。多个父级资源可以竞争接管子资源，但只有一个会成功（其余的父级资源将收到验证错误）。 Informers正如你可能已经注意到的那样，一些控制器（例如如 RBAC 鉴权器或 Deployment 控制器）需要查询集群状态以正常工作。以 RBAC 鉴权器为例，我们知道当请求到达时，验证器将保存用户状态的初始信息以备后用。然后，RBAC 鉴权器将使用改信息来查询用户在 etcd 中关联的所有角色以及角色绑定。控制器应该如何访问和修改这些资源？在 Kubernetes 中往往通过 informer 来解决。 informer 是一种允许控制器通过简单的订阅存储事件来获取它们关注的资源的设计范式。除了提供良好的抽象外，它还处理了许多细节，例如缓存（缓存很重要，因为它减少了与 kube-apiserver 的不必要的连接，并减少了服务器和控制器端重复序列化的开销）。通过该设计，控制器还可以用线程安全的方式进行交互，而不必担心干扰其他任何人。 有关 informer 在控制器中的工作方式的细节，可以参阅这篇博客。 调度器在上述所有控制器运行完成后，我们在 etcd 中存储了一个 Deployment、一个 ReplicaSet 和三个 Pod，并且可以通过 kube-apiserver 查询到它们。然而我们的 Pod 仍处在 Pending 状态，因为它们尚未被调度到节点上。解决这个问题的最后一个控制器是调度器(Scheduler)。 调度器作为控制平面的一个独立组件运行，并且以与其他控制器相同的方式工作：它监听事件并尝试调解状态。在这种情况下，调度器筛选出 PodSpec 中 NodeName 字段为空的 Pod，并尝试寻找一个适合该 Pod 的节点。 为了找到一个适合的节点，调度器使用特定的调度算法。默认调度算法的工作方式如下： 当调度器启动时，会注册一系列默认的谓词(Predicates)。这些谓词实际上是函数，这些函数根据节点是否适合托管 Pod 来进行过滤。例如，如果 PodSpec 明确要求一定的 CPU 或 RAM 资源，容量不足而无法满足这些要求的节点将被排除在 Pod 之外（资源容量计算为总容量减去当前运行容器的资源请求总和）。 一旦选择了合适的节点，会对过滤后的节点运行一系列优先级函数，以对它们的适合程度进行排序。例如，为了在系统中分散工作负载，调度器会倾向于资源富裕的节点（因为这表示较少的工作负载正在运行）。在运行这些函数时，它会为每个节点分配一个数值等级。然后选择排名最高的节点进行调度。 调度算法找到节点后，调度器会创建一个绑定对象，其 Name 和 UID 与 Pod 匹配，其 ObjectReference 字段包含所选节点的名称，然后通过 POST 请求将其发送到 apiserver。 当 kube-apiserver 接收到此绑定对象时，注册表会反序列化对象并更新 Pod 对象上的以下字段：将 NodeName 设置为 ObjectReference 中的节点名称，添加相关的注解，并将其 PodScheduled 状态条件设置为 True。 一旦调度器将 Pod 调度到节点上，该节点上的 kubelet 就可以开始接管并进行部署。真是令人兴奋！ 提示：自定义调度器时谓词和优先级函数都是可扩展的，并且可以使用 --policy-config-file 标志进行定义。这提供了一定程度的灵活性。管理员还可以在独立的 Deployment 中运行自定义调度器（具有自定义处理逻辑的控制器）。如果 PodSpec 包含 schedulerName，Kubernetes 将把该 Pod 的调度交给已注册在该名称下的调度器。 kubeletPod 同步好的，主要的控制器循环已经完成，呼！总结一下：HTTP 请求通过了身份验证、鉴权和准入控制阶段；一个 Deployment、一个 ReplicaSet 和三个 Pod 资源被持久化到了 etcd 中；一系列初始化程序已经运行；最后，每个 Pod 被调度到了一个合适的节点上。然而到目前为止我们所有推演的状态完全存在于 etcd 中。接下来的步骤涉及将状态分发到工作节点上，这是 Kubernetes 这样的分布式系统的核心目标！接下来的过程是通过一个叫做 kubelet 的组件来实现的。我们开始吧！ kubelet 是在 Kubernetes 集群的每个节点上运行的代理程序，负责管理 Pod 的生命周期等任务。这意味着它处理了从 Pod（实际上只是 Kubernetes 的一个概念）到其构建块（容器）的所有转换逻辑。它还处理与挂载卷、容器日志、垃圾回收等相关逻辑以及许多其他重要事项。 一个便于理解的方法是：可以把 kubelet 看做一个控制器！它会每隔 20 秒（可配置）从 kube-apiserver 查询 Pod，过滤出 NodeName 与该 kubelet 所在节点名称匹配的 Pod。一旦获得该 Pod 的列表，它会通过与自己的内部缓存进行比较来检测新增的 Pod，当比较存在差异时开始同步状态。我们来看看这个同步过程是什么样的： 如果正在创建 Pod（我们的 Pod 正在创建中！），kubelet 会注册一些用于在 Prometheus 中跟踪 Pod 延迟的启动指标。 然后，它生成一个 PodStatus 对象，表示 Pod 当前阶段的状态。Pod 的阶段是其生命周期中的高度总结。阶段包括 Pending、Running、Succeeded、Failed 和 Unknown。生成这个状态相当复杂，所以我们来详细了解一下具体发生了什么： 首先，按顺序执行一系列同步处理程序 PodSyncHandlers。每个处理程序都检查 Pod 是否仍应驻留在节点上。如果它们中的任何一个决定该 Pod 不再属于该节点，Pod 的阶段将变为 PodFailed，并最终被从该节点中驱逐出去。例子包括在超过 activeDeadlineSeconds 后驱逐 Pod（在 Job 资源中常用）。 接下来，根据其初始化和实际容器的状态确定 Pod 的阶段。由于我们的容器尚未启动，容器被归为等待状态。Pod 在拥有等待容器时的阶段为 Pending。 最后，根据容器的状态确定 Pod 的条件。由于我们的容器尚未由容器运行时创建，它将把 PodReady 条件设置为 False。 生成 PodStatus 后，它将被发送给 Pod 的状态管理器，后者负责通过 apiserver 异步更新 etcd 记录。 接下来，一系列准入处理程序会确保 Pod 具有正确的安全权限。这包括校验 AppArmor 配置文件和 NO_NEW_PRIVS 等。在此阶段被拒绝的 Pod 将永远保持在 Pending 状态。 如果指定了 cgroups-per-qos 运行时标志，kubelet 将为 Pod 创建 cgroups 并应用资源参数。这是为了给 Pod 提供更好的服务质量(QoS)。 为 Pod 创建数据目录。这包括 Pod 的目录（通常为 /var/run/kubelet/pods/&lt;podID&gt;）、卷目录（&lt;podDir&gt;/volumes）和插件目录（&lt;podDir&gt;/plugins）。 卷管理器将绑定并等待 Spec.Volumes 中定义的所有相关卷。根据要挂载的卷的类型的不同，某些 Pod 可能需要等待更长时间（例如云存储或 NFS 卷）。 从 apiserver 中查询在 Spec.ImagePullSecrets 中定义的所有密钥，以便后续注入到容器中。 最后由容器运行时(CRI)来运行容器（下面将详细描述）。 CRI 和暂停容器我们现在已经完成了大部分的配置工作，容器已经准备好启动了。负责启动容器的软件被称为容器运行时（例如 Docker 或 rkt ）。 为了更好的扩展性，自 Kubernetes v1.5.0 以来，kubelet 一直在使用称为 CRI（Container Runtime Interface）的概念与具体的容器运行时进行交互。简而言之，CRI 提供了 kubelet 与特定运行时实现之间的抽象接口。通信通过 protocol buffers 完成（类似于更快的 JSON），并使用 gRPC API（一种非常适合执行 Kubernetes 操作的 API 类型）。这是一个非常酷的想法，因为通过使用 kubelet 和容器运行时之间的定义合约，容器编排的具体实现细节变得相对不重要，唯一重要的是合约。这使得可以用最小的开销添加新的运行时，因为我们不需要更改核心 Kubernetes 代码！ 话题岔开太远了，让我们回到部署容器的过程本身。当一个 Pod 首次启动时，kubelet 调用 RunPodSandbox 远程过程调用（RPC）。沙盒 Sandbox 是 CRI 术语，用来描述一组容器，在 Kubernetes 术语中就是一个 Pod。这个术语被刻意地设计得比较模糊，以便对于其他可能实际上不使用容器的运行时（比如基于虚拟化的运行时，其中的沙盒可能是一个虚拟机）也适用。 在我们的例子中，我们使用的是 Docker。在这个运行时中，创建一个沙盒其实是创建一个暂停容器。暂停容器作为 Pod 中所有其他容器的父容器，承载了许多工作负载容器将要使用的 Pod 级资源。这些“资源”是 Linux 的命名空间（IPC、网络、PID）。如果你对 Linux 中容器的工作原理不熟悉，我们进行一个简短的复习。Linux 内核具有命名空间的概念，允许主机操作系统划分出一组专用资源（例如 CPU 或内存），并将其提供给一个进程，就好像它是世界上唯一使用这些资源的进程一样。在这里，Cgroups 也很重要，因为它们是 Linux 管理资源分配的方式（有点像监管资源使用的警察）。Docker 使用这两个内核特性来运行具有足够资源和强制隔离的进程。要了解更多信息，请查看 b0rk 的精彩文章《容器到底是什么》。 暂停容器提供了一种托管所有这些命名空间并允许子容器共享它们的方式。处在同一个网络命名空间的好处是同一个 Pod 中的容器可以使用 localhost 相互引用。暂停容器的第二个角色与 PID 命名空间的工作原理有关。在这些类型的命名空间中，进程形成一个层次树，顶部的初始化进程负责“清理”已经退出的进程。要了解这是如何工作的更多信息，请查看这篇精彩的博客。在创建完暂停容器后，它会被存档到磁盘上，并启动运行。 CNI 和 Pod 通信现在，我们的 Pod 已经有了个基本的骨架：一个承载所有命名空间以实现跨 Pod 通信的暂停容器。但是其中的网络是如何生效的，又该如何进行设置呢？ 当 kubelet 为一个 Pod 设置网络时，它将任务委派给一个名为 CNI 的插件。CNI 代表容器网络插件 Container Network Interface，其工作方式类似于 Container Runtime Interface。简而言之，CNI 是一个抽象层，允许不同的网络供应程序使用不同的容器网络实现。kubelet 通过将 JSON 数据（配置文件位于 /etc/cni/net.d）经过 stdin 传送给相关的 CNI 二进制文件（位于 /opt/cni/bin）与注册好的插件进行交互。这是一个 JSON 配置的示例： 12345678910111213{ &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;bridge&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;cnio0&quot;, &quot;isGateway&quot;: true, &quot;ipMasq&quot;: true, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;ranges&quot;: [[{ &quot;subnet&quot;: &quot;${POD_CIDR}&quot; }]], &quot;routes&quot;: [{ &quot;dst&quot;: &quot;0.0.0.0/0&quot; }] }} 它还通过环境变量 CNI_ARGS 指定了 Pod 的附加元数据，例如它们的名称和命名空间。 接下来发生的步骤取决于具体的 CNI 插件，我们先来看看网桥(bridge) CNI 插件的工作流程： 首先，网桥插件将在主机的根网络命名空间中设置一个本地 Linux 网桥（bridge），以服务于该主机上的所有容器。 然后，它将在暂停容器的网络命名空间中插入一个接口（一对 veth 的一端），并将另一端连接到网桥（bridge）上。最好将一对 veth 想象成一个大管道：一端连接到容器，另一端在根网络命名空间中，使得数据包在两者之间进行传递。 接下来，网桥插件应该为暂停容器的网卡分配一个 IP 地址并设置路由，这使得 Pod 拥有了自己的 IP 地址。IP 地址分配是委派给在 JSON 配置中指定的 IPAM 提供程序来完成的。 IPAM 插件与主要的网络插件类似：它们通过二进制文件调用，拥有标准化的接口。每个 IPAM 插件必须确定容器接口的 IP/子网，以及网关和路由，并将这些信息返回给主插件。最常见的 IPAM 插件称为 host-local，它从预定义的地址范围中分配 IP 地址。它将状态存储在主机的文件系统上，以确保在单个主机上 IP 地址的唯一性。 至于 DNS，kubelet 将向 CNI 插件提供内部 DNS 服务器的 IP 地址，CNI 插件将确保容器的 resolv.conf 文件设置正确。 上述步骤都完成后，CNI 插件会向 kubelet 返回 JSON 数据说明操作的结果。 跨节点通信到目前为止，我们已经解释了容器如何连接到主机，但主机之间如何通信呢？当位于不同机器上的两个 Pod 想要通信时，就自然会涉及节点间的通信。 通常，节点间通信是通过一种称为覆盖网络（overlay networking）的概念实现的，它是一种在多个主机之间动态同步路由的方式。一个流行的覆盖网络供应者是 Flannel。安装完成后，它的核心任务是在集群中的多个节点之间提供第 3 层的 IPv4 网络。Flannel 不控制容器如何与主机进行网络连接（这是 CNI 的工作，请记住），而是控制主机之间的流量传输。为此，它为主机分配一个子网，并在 etcd 中注册这个子网。随后，它保持集群路由的本地表示，并将传出数据包封装在 UDP 数据报中，确保其到达正确的主机。要了解更多信息，请参阅 CoreOS 的文档。 容器启动所有关于网络的过程都已经介绍了。接下来还剩下什么呢？好吧，我们需要真正地启动工作负载容器。 一旦沙盒初始化完成并处于活动状态，kubelet 就可以开始为其创建容器。它首先启动在 PodSpec 中定义的所有 init 容器，然后再启动主要的容器本身。具体过程如下： 拉取容器的镜像。PodSpec 中定义的加密信息（secrets）都将用于私有的镜像仓库； 通过 CRI 创建容器。沙盒会先从父 PodSpec 中生成一个 ContainerConfig 结构体（其中定义了命令、镜像、标签、挂载点、设备、环境变量等），然后通过 protobuf 将其发送给 CRI 插件。以 Docker 举例，它会反序列化有效的负载并生成自己的配置结构以发送到 Docker 的守护进程 API。在此过程中，它会向容器中注入一些元数据标签，例如容器类型、日志路径、沙盒 ID。 然后，它将使用 CPU 管理器（CPU manager）注册容器。CPU 管理器是 1.8 中的一个新的 alpha 功能，它使用 UpdateContainerResources CRI 方法将容器分配到本地节点上的一组 CPU。 接下来容器被启动。 如果注册了任何后启动（post-start）的容器生命周期钩子，它们将被执行。钩子可以是 Exec 类型（在容器内执行特定命令）或 HTTP 类型（针对容器端点执行 HTTP 请求）。如果 PostStart 钩子运行时间过长、挂起或失败，容器将永远无法变成 running 状态。 回顾好的，终于完成了。 经过所有这些步骤，我们应该在一个或多个工作节点上运行着 3 个容器。所有的网络、卷和加密信息都已由 kubelet 配置完成，并通过 CRI 插件转换为容器。","link":"/2024/01/13/WhatHappensWhenK8s/"},{"title":"Controller-runtime 源码阅读","text":"Operator 是 Kubernetes 用来拓展其 API 的一种开发范式（Pattern），其核心是定义若干的自定义资源及其对应的资源控制器，当这些资源发生变化时其对应的控制器对变化进行调解（Reconcile），最终使得实际状态与预期状态达成一致。K8s-sigs 推出的 kubebuilder 是一个用于构建 Operator 应用的框架，和 Operator-SDK 一样都依赖了 controller-runtime，提供了高级 API 和抽象，让开发者更直观地编写操作逻辑，并提供用于快速启动新项目的脚手架和代码生成工具。 截至目前我已经参与了两个 Operator 项目的搭建和维护，均采用了 kubebuilder 做基础脚手架。我目前对 CRD 的设计生成、Webhook 的校验机制和、Controller 的控制循环机制有了一定认识，接触时间稍长后在日常开发 Operator 项目时难免出现缺乏新意的情况，Operator 模式看久了和 CURD 之于后端有些许相似。但我乐观地估计通过观察下层实现可以收获一些启发。既然 kubebuilder 和 Operator-SDK 都依赖了 controller-runtime，那么就先从它出发吧。 SummaryMindMapcontroller-runtime 这个包的内容不少，为了在一篇文章里能覆盖到，本文仅选取在构建 Operator 过程中起关键作用的包进行介绍，envtest、scheme、certwatcher 等同样重要的包就不在此提及。下面是我读代码时画的意识流思维导图，有的子项目是组成结构，有的子项目是工作角色，比较自由灵活。我选取了 Cache,Source, Handler, Client, Controller 和 Manager 这六个包。 WorkFlow对于控制器而言，资源发生变动的信息均来自于 API Server，从资源发生变动到控制器完成调解需要经过多个模块的处理，大体来说可以用下面的图来表示。 CacheCache 包通过 Informers 接口封装了 client-go 中的缓存机制 SharedInformer，为每个资源类型都创建对应的 Informer，通过它们的缓存避免所有请求都直接访问 API Server 导致其可能的不堪重负。SharedInformer 机制在 client-go 中定义，它采用增量同步的方式从 API Server 处“订阅”某类资源的事件，并且将事件的增量更新保存在本地存储（Store）当中，其中典型的存储是 DeltaFIFO。SharedInformer 是 k8s client-go 中的核心机制，几乎所有的客户端应用都绕不开它，之后有空再阅读查看其中细节，这里我们不再展开。 控制流路径大致为 cache.New -&gt; newCache -&gt; internal.NewInformers -&gt; sharedInformers，其中： cluster 初始化时通过 cache.New 创建集群资源缓存，默认的创建缓存方法可以通过传入自定义的缓存初始化函数进行 Mock，大多数情况下不需要传入自定义的函数； 在 Cache 的初始化函数当中可以为每个类型的资源定义细粒度的缓存策略，通过 cache.Options 中的 ByObject 字段进行配置。在 manager.Options 中的 Cache 就是负责控制缓存的行为的字段。 internal.Informers 提供了 Get、Peek 和 Remove 方法。其中 Get 方法中调用 Peek，若没有获取到指定的 sharedInformer，会根据配置参数中的 newInformer 方法创建出来并且添加到 map 当中留作后用；Peek 方法若无法从 map 中获取到也不会自动启动新的 sharedInformer。 通过 internal.Informers 获取到 sharedInformer，后续的 Source 包能够将事件处理器与其绑定，将从 API Server 处同步到的对象变更事件转化为控制器循环当中的 reconcile.Request 对象。 SourceSource 顾名思义是来源，但准确来讲这个来源是请求的来源，也就是在 kubebuilder 中所有的控制器需要实现的 Reconcile 方法的 requests.Request 这一参数的生产者。在 Source 有三种类型，Channel，Informer 和 Func。 其中 Channel 类型主要用于外部事件的处理，例如 Github 的 Webhook，需要用户自行编写外部的 Source 来将通用事件写入到内部的 Channel 当中。 Informer 类型的 Source 在控制器当中最常用，它封装了 client-go 的 cache.Informer 接口，将事件处理器与 informer 进行绑定，用于产生源于集群内部的事件，例如 Pod 的创建等。 pkg/source.go:184-2061234567891011121314151617181920212223// Informer is used to provide a source of events originating inside the cluster from Watches (e.g. Pod Create).type Informer struct { // Informer is the controller-runtime Informer Informer cache.Informer}var _ Source = &amp;Informer{}// Start is internal and should be called only by the Controller to register an EventHandler with the Informer// to enqueue reconcile.Requests.func (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface, prct ...predicate.Predicate) error { // Informer should have been specified by the user. if is.Informer == nil { return fmt.Errorf(&quot;must specify Informer.Informer&quot;) } _, err := is.Informer.AddEventHandler(internal.NewEventHandler(ctx, queue, handler, prct).HandlerFuncs()) if err != nil { return err } return nil} 因为 Source 接口只有一个 Start() 方法，所以 Func 类型只是为了方便将单个函数实现成为这个接口而封装出来的类型，在此不再作更多介绍。 在 Source 的内部实现 (pkg/internal/source) 中，它将从 Informer 中获取到的对象转换为 Create，Update，Delete 和 Generic 四类事件，四类事件分别由相应的事件处理器进行下一步的处理。其中，事件处理器在 Handler 包中定义，Informer 在 Cache 包中定义。 Handlerpkg/handler.go:44-571234567891011121314type EventHandler interface { // Create is called in response to a create event - e.g. Pod Creation. Create(context.Context, event.CreateEvent, workqueue.RateLimitingInterface) // Update is called in response to an update event - e.g. Pod Updated. Update(context.Context, event.UpdateEvent, workqueue.RateLimitingInterface) // Delete is called in response to a delete event - e.g. Pod Deleted. Delete(context.Context, event.DeleteEvent, workqueue.RateLimitingInterface) // Generic is called in response to an event of an unknown type or a synthetic event triggered as a cron or // external trigger request - e.g. reconcile Autoscaling, or a Webhook. Generic(context.Context, event.GenericEvent, workqueue.RateLimitingInterface)} Source 将事件处理器 EventHandlers 和 Informers 进行绑定，Handlers 将某个某类型资源 A 的事件 Event 转化为某类型资源 B 的事件请求 Request 推入工作队列（workqueue.RateLimitingInterface，定义在 client-go 当中），其中 A 通常等于 B，但也存在 A 不等于 B 的情况。下面将两种情况区分介绍。 1. A == B A == B 也就是说产生事件的资源和需要调解的资源类型是相同的，例如用户提交了一个 Pod，那 Pod 的控制器就会接收到这个 Pod 被创建的事件，并对该事件进行调解。这是最普遍的情况，在 pkg/handler/enqueue.go 中有该情况的实现。 2. A != B A != B 说明在类型 A 产生的事件要发送给类型 B 的控制器进行调解，这在单一资源/控制器的语境下没有太大的意义，但如果将资源的从属关系也纳入其中就很好解释了：父级资源在子资源发生变更时收到相应的事件，级联地调解自身的状态，进而加速多级资源结构整体的调解速度。例如 ReplicaSet 资源应该监听其拥有的 Pod 资源的事件，当 Pod 状态发生变化时，ReplicaSet 控制器也应该调解 ReplicaSet 资源的状态或配置，以求符合预期。 在 pkg/handler/enqueue_owner.go 和 enqueue_mapped.go 中有 A != B 时的 handler 方法实现。其中 enqueue_owner.go 中为我们实现了“子资源变更，父资源调解”的逻辑，在 kubebuilder 中在 builder 方法下使用 Owns() 方法可以声明从属关系，从而让我们的控制器能够调解拥有的其他资源的“此类资源”。 而 enqueue_mapped.go 则封装了更为通用的事件处理器方法，能够让用户自定义从 client.Object 到 reconcile.Request 的映射，实现更为灵活的事件入队逻辑。 ControllerController 控制器是我们要补充编码并最终运行的若干实体，它们负责从 K8s 的控制循环中取回对应资源的事件，并且调用自身的调解函数（也就是我们在编写 Operator 时补充的 Reconcile 函数）完成资源状态对齐的任务。如开头的思维导图所示，我列出了 Reconcile，Workqueue，Watches() 和 Metadata Projection 这些子项目，下面分别就这些内容进行介绍。 pkg/internal/controller/controller.go1234567891011121314151617181920212223type Controller struct { // Name is used to uniquely identify a Controller in tracing, logging and monitoring. Name is required. Name string // MaxConcurrentReconciles is the maximum number of concurrent Reconciles which can be run. Defaults to 1. MaxConcurrentReconciles int // Reconciler is a function that can be called at any time with the Name / Namespace of an object and // ensures that the state of the system matches the state specified in the object. // Defaults to the DefaultReconcileFunc. Do reconcile.Reconciler // MakeQueue constructs the queue for this controller once the controller is ready to start. // This exists because the standard Kubernetes workqueues start themselves immediately, which // leads to goroutine leaks if something calls controller.New repeatedly. MakeQueue func() workqueue.RateLimitingInterface // Queue is an listeningQueue that listens for events from Informers and adds object keys to // the Queue for processing Queue workqueue.RateLimitingInterface // ...} ReconcileReconcile 函数也就是控制器结构体（pkg/internal/controller）当中 Do 这个字段的具体实现，它接收 reconcile.Request 返回 reconcile.Result，这两个参数类型都极为简单，从中可以表现出 controller-runtime 的设计者们希望把最简单的接口留给开发者。reconcile.Request 其实就是 NamespacedName，reconcile.Result 则包含了两个字段 Requeue 和 RequeueAfter，分别表示是否重新入队和多久后重新入队。这与后续的工作队列模块相互配合，支持我们实现有计划、有规律的调解重试。 WorkqueueWorkqueue 顾名思义是工作队列，与 Controller 控制器和 Source 事件源相互配合，完成对资源变更事件的有序处理过程。workqueue 是 client-go 中的 workqueue.RateLimitingInterface 接口，也就是速率受限的工作队列，限定速率的工作由 rateLimiter 接口完成，一个对象需要先经过 rateLimiter 同意才能够顺利入队，速率限定器的逻辑可由用户自行定义，但大部分 K8s 客户端的场景当中，使用默认的速率限定逻辑即可。速率受限的工作队列也在 client-go 中完成定义，之后的文章中有机会再探讨。 WatchesWatches 方法将某一类对象包装成为 Source，并将其通过事件处理器 Handler 与工作队列进行关联。在 Kubebuilder 当中我们直接使用的方法是 ControllerManagedBy，它采用构建者模式返回一个 Builder 类型的结构，支持我们链式调用配置方法，最终通过 Complete 方法完成控制器的构建。在 Builder 结构体下暴露了若干的方法，其中有 For，Owns 和 Watches 这三个方法用于绑定 Source 和 Handler。For 和 Owns 其实是 Watches 的语法糖，它们分别表示监听某类资源和监听拥有的某类资源（从属关系通过 OwnerReferences 构建），都可以通过 Watches 方法来实现。 Watches 方法接受 client.Object，handler.EventHandler 和 WatchesOption 作为参数，从集群的缓存中拿到某类资源的 Informer 封装为 Source，绑定上事件处理器。handler 包中提供的两个现有的方法分别构成了 For 和 Owns 两个方法对 Watches 封装的语法糖。 Watches 的行为还会收到 Predicates 的影响，Predicates 起过滤作用，用来决定什么事件应该进入工作队列，什么事件不应该进入工作队列。刚开始接触 Controller runtime 时许多开发者经常会遇到资源 Spec 变更后触发调解，控制器更新资源 Status 之后再次触发调解的莫名其妙的死循环，这个情况就是 Predicate 没有正确设置，当资源（包括 Status）发生更新后，资源的 ResourceVersion 会发生变更，但如果不希望 Status 更新后触发调解，可以在 builder.WithEventFilter() 中传入预先定义好的 predicate.GenerationChangedPredicate{}，这样会过滤掉 ResourceVersion 发生变更的事件。 Metadata Projection在 Controller Builder 包中有个类型是 objectProjection 表示对象的投影。在调用 For，Owns 和 Watches 三个方法时可以通过末尾的不定长选项参数传入有关投影的配置，builder.OnlyMetadata 就是这样的配置。OnlyMetadata 用来告诉控制器只需要缓存元信息，并且只通过 MetadataClient Watch 元信息格式的资源对象。这对于某类资源对象众多、资源占据空间极大或者只知道资源的 GVK 不知道资源的具体结构等情况是非常有用的。 Controller ManagerController Manager 控制器管理器管理了包括控制器在内的若干可运行接口（Runnable），只要实现了方法 Start(context.Context) error 就能够成为 Runnable，上述介绍的若干模块都实现了这个方法，例如 Cache, Source, Controller，还有未提及的 Webhook，HttpServer，LeaderElection 等。管理器自身也实现了 Start 方法，用于在我们的主程序中调用运行。上述所有模块的配置也都可以通过 Manager 的配置进行传入，换句话说，Manager 的配置整合了所有其他模块的配置信息。 Manager 还封装了 Cluster 这个接口，cluster 包含了 rest.Config, runtime.Scheme, Cache, client.Reader 和 meta.RESTMapper 等包含集群信息的重要字段，Cluster 接口所有的方法都是只读的，也确定了该结构就是单纯用于“信息查阅”的。 ClientClient 封装了常用的客户端功能，Get 和 List 操作优先从缓存中读取，Create，Update 和 Delete 等写入操作直接与 API Server 进行通信。当然可以在初始化客户端时通过 client.Options.Cache.DisableFor 字段配置禁用某些资源类型的缓存，直接从 API Server 读取。 启发对外的 API 保持简单如果不读源码，不仔细读文档，我们能够接触到的接口无非就是 Controller Builder 的使用和 Reconcile 方法，了解 K8s 的控制循环逻辑就可以开始编写 Operator 的代码。Controller-runtime 高内聚的特点将简单留给了开发者，复杂性由设计者和维护者承担。 同时也可以看到为了保持依赖的简洁，Controller-runtime 从其他地方(k8s.io/kubernetes)复制了一些文件到本地，从而避免了直接依赖整个包。虽然不优雅，但不得不说实用主义在这里再次赢得了胜利。 多利用编译阶段的静态检查123456789type Source interface { // Some methods}type kind struct { // Some fields}var _ Source = kind{} 上述代码段定义了一个公开接口和一个内部结构，通过 var _ Source = kind{} 这个语句在编译阶段保障我们的结构实现了指定的接口，而因为变量名为空，在编译完成后该变量会被抛弃，对程序的运行状态没有影响。这个小技巧可以在之后的开发中多加使用，尽早避免可能出现的未完全实现某接口导致的运行时错误。 Option 接口在 Controller runtime 中广泛出现 Option 接口的实现，各种配置都通过传递 Option 接口来完成，下面是其中一处源码片段： 123456789101112131415161718192021222324252627type ForOption interface { // ApplyToFor applies this configuration to the given for input. ApplyToFor(*ForInput)}type Predicates struct { predicates []predicate.Predicate}// ApplyToFor applies this configuration to the given ForInput options.func (w Predicates) ApplyToFor(opts *ForInput) { opts.predicates = w.predicates}func (blder *Builder) For(object client.Object, opts ...ForOption) *Builder { if blder.forInput.object != nil { blder.forInput.err = fmt.Errorf(&quot;For(...) should only be called once, could not assign multiple objects for reconciliation&quot;) return blder } input := ForInput{object: object} for _, opt := range opts { opt.ApplyToFor(&amp;input) } blder.forInput = input return blder} 当我们为多个类型实现了 Option 接口的方法，这样多个类型可以作为相同的类型传递给同一个函数，例如 For 当中的 Predicate 和 ObjectProjection 等。通过 Option 接口可以获得更高的配置灵活性。","link":"/2024/02/03/ReadSourceOfControllerRuntime/"},{"title":"VSCode 插件 - YAI","text":"介绍这又是一次目标回收计划，早在 2021 年我还在广泛地写 TypeScript 代码时就想完成这样一个插件来满足我“不打断心流地引入模块”的需求，但“新建文件夹”之后我一直没有实际的迭代动作。直到最近高频写 Go 代码时，才真正意识到这个需求的重要性。于是我又重新打开了这个项目的代码仓库。 这是一个 VSCode 插件，叫做 YAI，全称 Yet Another Importer，英文项目命名的 Yet Another 数不胜数，我也随波逐流一次。这个插件是用来帮助开发者方便地引入模块的，它可以自动识别当前项目中的依赖，并且扫描项目本地的文件，统计当前项目中引入模块的规律和频次，在需要引入模块时给出相应提示，并且以编程语言“原生”的方式将模块引入到代码中。何为“原生”，也就是适应当前项目编程语言的引入方式，比如在 JavaScript 项目中，它会使用 import 或 require 语句引入模块，而在 Python 项目中，它会使用 import 语句引入模块，在 Go 项目中，它会使用别名来引入模块等。 目前这个插件还处于开发阶段，但是已经可以在 Go 语言中使用了。目前规划的编程语言还有 ECMAScript、Python、C/C++ 这几种。该插件的代码仓库在 Github 代码仓库中，如果你也对这个插件感兴趣，欢迎使用或者参与开发。 插件已发布，可以在 yai - VSCode Marketplace 查看安装。 这篇文章可以算作插件的设计文档，我会在这里记录一些关于这个插件的设计思路和实现细节。 开发初衷从代码仓库的提交日期可以看到，我三年前就想实现这样的小插件，当时主要编程语言是 TypeScript，主要写 React 项目，虽然 VSCode 对 TypeScript 的模块引入支持得还算不错，但有时它也会不及预期： 在实现的具体代码逻辑中键入一个模块名，有时 VSCode 能够自动提示，有时则不能，例如引入 React； 在已经部分引入模块的情况下，再想引入模块中其他导出变量，有时 VSCode 能够自动提示，有时则不能（大多时候不能），例如已经从 antd 中引入了 Button, Tooltip, message 再想引入 Select 时，往往无法得到编辑器的提示； 在引入本地定义的模块时，VSCode 有时无法正确判断导出方式，export default 和 export 有时会混淆。 为了解决上述 VSCode 模块引入的一些问题，我需要暂停手头的工作，把视窗划到文件的头部（通常情况下可以使用 Cmd + Up Arrow 组合键来完成），然后键入 import 或 require 等关键字，再键入模块名，最后再键入可选的分号。这样的操作虽然看似简单，但是在频繁引入模块的情况下，会打断我的心流，让我无法专注于当前的工作。 这个情况在写 Go 代码的时候也时常出现，虽然 Go 语言的模块引入方式相对简单，但是在引入第三方模块时，我还是需要打开浏览器，查找模块的文档，然后复制粘贴模块的引入语句。而且在 Go 代码中会广泛地使用别名来引入模块，若一个项目 Go 文件稍微多些，同一个模块会出现多个引入别名（例如 VSCode 有时将 k8s.io/apimachinery/pkg/apis/meta/v1 和 k8s.io/api/core/v1 都引入为 v1，有时又分别引入为 metav1 和 corev1），为了保持各个文件中该模块的语义和含义一致，我不仅需要跳转到文件头部，甚至需要打开其他文件将别名引入语句复制粘贴到当前文件中。这种操作无疑是对“流畅编写代码”目标的一次沉重打击。 设计思路和实现细节根据开发初衷，我给这个插件制定了如下几个目标，优先级从高到底： 模块引入：需要提供不打断心流的引入模块方式，即在键入模块名时，插件应该能够自动提示当前项目中的可引入的模块； 模块索引：需要支持本地文件中引入模块的索引，即插件应该能够扫描当前项目中的文件，统计当前项目中引入模块的规律和频次； 多语言支持：需要支持多种编程语言，即插件应该能够根据当前项目的编程语言，使用该编程语言的原生引入方式引入模块； 为了实现这三个目标，结合近期编写 Go 代码的经历，我给 YAI 插件制定了如下设计思路。 模块引入模块引入是插件的核心功能，结合我的日常使用习惯，开发者通常会引入的模块有三种： 标准库模块：Go 语言的标准库模块，例如 fmt, os, io 等； 第三方模块：Go 语言的第三方模块，例如 github.com/gin-gonic/gin, github.com/spf13/viper 等； 本地模块：项目中的本地模块，例如 pkg/utils, internal/config 等。 为了识别这三个类型的模块，插件需要扫描当前项目中的 go.mod 文件，以识别第三方模块；扫描当前项目中的 go 文件，以识别本地模块；最后至于标准库模块，Go 语言的标准库模块是有限的，插件可以直接内置这些模块。例如执行 go list std 命令就可以把所有的标准库都列出来。 秉持着先能跑再优化的开发理念，该模块引入目标的迭代计划是： 第一步：所有的模块引入都需要开发者提供完整的模块名，例如 github.com/gin-gonic/gin，pkg/utils 等，不提供补全提示； 第二步：在键入模块名时，插件应该能够自动提示当前项目中的可引入的模块，例如在 Go 项目中，键入 gin 时，插件应该能够提示 github.com/gin-gonic/gin； 第三步：在键入模块名时，插件应该能够自动提示当前项目中的可引入的模块，同时在键入 . 时，插件应该能够自动提示当前模块的导出变量。 模块索引模块索引需要扫描本地所有源文件，解析其中的模块引入语句，统计当前项目中引入模块的规律和频次。这个功能的目的是为了在模块引入时，给出更加智能的提示。例如在 Go 项目中对于第三方模块而言，有时开发者会引入模块的子模块，例如 github.com/gin-gonic/gin 中的 github.com/gin-gonic/gin/render，有时开发者会引入模块的别名，例如 github.com/gin-gonic/gin 有时会引入为 gin，有时会引入为 g，有时会引入为 g1 等。这些引入方式都是合法的，但是在一个项目中应该保持一致，这样可以提高代码的可读性和可维护性。 模块索引是为了能在模块引入时提供更多的信息，以更好地满足模块引入的需求。在 YAI Go 的第一个版本中，模块索引所实现的功能就是把本地文件中所有的引入路径都扫描出来，然后在模块引入时给出提示，这很好地补充了仅扫描 go.mod 文件获取第三方模块根路径的不足。同时，Go 项目的模块索引可以能获取到模块的别名，在模块引入时能让开发者直观地看到这个模块的所有别名以及各个别名的引入频次。 多语言支持最初的开发动力来自于 TypeScript 编写的 React 项目，但是在写 Go 项目的时候，我发现不打断心流地模块引入是一个更加普遍的需求。因此，我决定在 YAI 插件中支持多种编程语言，因为当前接触得比较多的是 Go 语言和 ECMAScript 语言，所以我决定先支持这两种语言。 我定义了一个名为 InnerProcessor 的接口，其中提供了两个方法 index() 和 import()，由各个语言的处理器分别实现该接口，例如 Go 语言的 GolangProcessor。外层定义 LanguageProcessor 接口拓展了 InnerProcessor 额外提供获取当前项目编程语言的方法 getLanguageId()，再定义类 YAIProcessor 实现这个接口，用于根据当前项目的编程语言选择合适的处理器。 每当激活插件后打开某个项目，若插件检测到当前项目的编程语言是已经支持的，则会触发一次 index() 方法的调用(YAI: Index Modules)。而模块引入功能则需要用户主动触发，即通过快捷键或者命令面板调用执行，命令名称为 YAI: Import Module。执行模块引入命令后会通过一系列输入选择的交互引入用户所指定的模块。 实现效果下面是 YAI 插件在 Go 项目中实现功能的一些实现效果。 首先通过 YAI: Index Modules 命令来索引当前项目中的模块，索引建立成功之后会有提示： 打开命令面板(F1 或 Ctrl/Cmd + Shift + P) 通过 YAI: Import Module 命令来开启引入模块功能。 进入命令后会展示当前可以引入的模块，这里可以通过键盘上下键选择模块，也可以通过输入文本进行筛选，然后回车确认选择： 选择模块后插件会提示用户是否需要引入该模块的子模块，此处如果不输入（直接按回车或 ESC 键）则表示不引入子模块： 下一步插件会提示本项目中该模块使用的所有的别名，用户可以选择其中一个别名，也可以手动再输入一个别名，然后回车确认选择： 最后插件会在当前文件中插入引入语句，模块引入成功： 总结和不足YAI 的开发占据了我近期工作日的业余时间和周末一整天，不过这个插件的开发过程还是很有意思的。在开发过程中，我学到很多关于 VSCode 插件开发的知识，也大致明白了 VSCode Extension API 能够实现的功能。不得不说 VSCode 对插件开发的支持还是很好的，它提供了丰富的 API，让开发者可以很方便地实现自己的想法。 就 YAI 目前的功能而言，它已经能够满足我在 Go 项目中引入模块的需求，让我不打断心流地引入我指定的模块。但它是还有很多可以迭代的方向： 交互流程有些冗长，需要用户多次输入和选择，未来可以考虑通过配置项的方式优化交互流程，例如省略输入子模块的步骤等； 目前只支持 Go 语言，未来可以考虑支持更多的编程语言，例如 ECMAScript、Python、C/C++ 等； 可以针对每个语言推出特定的功能，例如统一 Go 项目中的模块别名等。 插件已经发布，可以在 yai - VSCode Marketplace 查看安装。","link":"/2024/02/27/YetAnotherImporter-VSCode/"},{"title":"在 Kubernetes 中安全地运行应用","text":"背景在近期的工作当中我大量接触 Kubernetes 集群以及容器化应用，在完成 Operator 拓展的开发和发布后，有海外用户反映在 OpenShift 平台上运行我们的容器化应用会遇到安全性问题，具体而言是我们的容器化应用默认需要 root 用户运行，而 OpenShift 平台如果不进行专门的设置是不允许容器使用 root 用户的。为了解决该用户的问题我们花费了一些功夫，正好我也想以此为契机进一步了解如何在 Kubernetes 中安全地运行应用。本文将记录我在调研和学习过程中的一些心得体会。 容器安全Docker Init CLIDocker Init 命令是用来创建遵循最佳实践的 Docker 配置文件的命令行工具。在使用时通过选择需要运行的应用类型（例如 Go、Python、Node、Rust 等），Docker 会自动帮助用户创建出符合最佳实践的 Dockerfile 和 compose.yaml 文件。 我的操作系统是 macOS, Docker 版本为 25.0.5，后续的版本中支持的应用类型可能会更多。 docker init12345678910111213141516171819$ docker initWelcome to the Docker Init CLI!This utility will walk you through creating the following files with sensible defaults for your project: - .dockerignore - Dockerfile - compose.yamlLet's get started!? What application platform does your project use? [Use arrows to move, type to filter] Go - suitable for a Go server application Python - suitable for a Python server application Node - suitable for a Node server application Rust - suitable for a Rust server application&gt; Other - general purpose starting point for containerizing your application Don't see something you need? Let us know! Quit 以 Go 语言应用为例通过 Docker Init 创建出 Go 语言程序的 Dockerfile 示例如下， Dockerfile123456789101112131415161718192021222324252627282930313233343536373839# syntax=docker/dockerfile:1ARG GO_VERSION=1.22FROM golang:${GO_VERSION} AS buildWORKDIR /srcRUN --mount=type=cache,target=/go/pkg/mod/ \\ --mount=type=bind,source=go.sum,target=go.sum \\ --mount=type=bind,source=go.mod,target=go.mod \\ go mod download -xRUN --mount=type=cache,target=/go/pkg/mod/ \\ --mount=type=bind,target=. \\ CGO_ENABLED=0 go build -o /bin/server ./k8s-safetyFROM alpine:latest AS finalRUN --mount=type=cache,target=/var/cache/apk \\ apk --update add \\ ca-certificates \\ tzdata \\ &amp;&amp; \\ update-ca-certificatesARG UID=10001RUN adduser \\ --disabled-password \\ --gecos &quot;&quot; \\ --home &quot;/nonexistent&quot; \\ --shell &quot;/sbin/nologin&quot; \\ --no-create-home \\ --uid &quot;${UID}&quot; \\ appuserUSER appuserCOPY --from=build /bin/server /bin/EXPOSE 8080ENTRYPOINT [ &quot;/bin/server&quot; ] 首先设置好 Go 的版本和工作目录后，通过 cache 和 bind 两种挂载将 go mod download -x 命令所依赖的文件缓存和 go.mod、go.sum 文件挂载到容器中，然后通过 go build -o /bin/server ./k8s-safety 命令编译出二进制文件。这里使用了 --mount=type=cache 和 --mount=type=bind 两种挂载方式，前者是将缓存文件挂载到容器中，后者是将本地文件挂载到容器中。这样做的好处是可以减少容器构建时间，提高构建效率。如果直接使用 COPY 或者 ADD 命令将本地文件拷贝到容器中，每次构建都会重新拷贝一次，效率较低，而且这两个命令可能会在镜像中残留一些不必要的信息，增加了潜在的安全风险。 该文件使用了两阶段构建，这是生产环境镜像常用的构建方法。如果直接采用 golang 的镜像作为运行镜像，其体积大不说（282 MB 左右），因为其中包含了编译环境，还会暴露一些不必要的信息，增加了潜在的安全风险。使用 alpine 镜像作为运行镜像，体积小（5.6 MB 左右），减小镜像体积的同时减小了攻击面。 在 alpine 镜像中通过 apk --update add 命令安装了一些必要的软件包，涉及了 TLS 证书和时区信息。再通过 adduser 命令创建了一个非 root 用户 appuser，并将其设置为容器运行时的用户。该用户没有设置密码、Home 目录和登录 shell，拥有最小的权限和资源，提高了容器的安全性。最后通过 COPY 命令从构建镜像中把编译好的二进制文件拷贝到运行镜像中，设置了容器监听的端口和启动命令。 通过 Docker Init 创建的这个 Dockerfile 遵循了一些最佳实践，例如使用多阶段构建、使用最小化的基础镜像、设置非 root 用户等，提高了容器的安全性。 配置管理ConfigMap 和 Secret在 Kubernetes 中，配置管理是一个非常重要的环节。在应用部署时，我们需要将应用的配置信息注入到容器中，以便应用能够正常运行。在配置管理中，我们需要合理的使用 ConfigMap 和 Secret 两种资源对象。其中 ConfigMap 用来存储应用的配置信息，Secret 用来存储应用的敏感信息，例如密码、证书等。它们都可以通过环境变量或者存储卷挂载的方式进行注入。 在我们的开发中常用 Write Ahead ConfigMap/Secret 的形式来存储配置信息，在应用启动时从 ConfigMap 或者 Secret 中读取配置信息到内存中，运行时若有修改，先修改 ConfigMap 或者 Secret，再修改内存中的数据，这样可以在应用重启时也能保持原有的配置，相当于用 K8s 的 Etcd 来存储配置信息。但需要注意的是 ConfigMap 和 Secret 一般有内容大小限制（1MB 左右），如果配置信息过大，可能会导致存储失败。 另外，我们还可以通过 ConfigMap/Secret 的更新机制来实现配置的热更新，避免了应用重启的问题。例如 local-path-provisioner 采用了轮询的方式来检测挂载的 ConfigMap 是否变化。 命名空间Kubernetes 中的命名空间是用来对集群中的资源进行逻辑隔离的资源。通过命名空间，我们可以将集群中的资源划分为不同的逻辑单元，提高了资源的管理和安全性。在实际的应用部署过程中，我们可以根据业务需求和安全要求，将不同的资源放置在不同的命名空间中，以便更好的管理、控制资源和隔离风险。最直观的是 ConfigMap、Secret、Service 等都是按照命名空间划分的资源，分别决定了应用的配置、密钥和服务访问等。 通过命名空间的划分，将开发、测试和生产的资源分开，将提供不同服务的应用分开，将不同业务线的应用分开，有条不紊地将集群中的资源组织起来是非常重要的。 资源配额Pod 的模板中可以为每个容器设置资源请求（Request）和限制（Limit）。资源请求是容器启动时所需的资源，资源限制是容器能够使用的资源的上限。通过设置资源请求和限制，我们可以更好的控制容器的资源使用，避免资源的浪费和滥用。如果使用 VSCode 进行开发并且安装了 Kubernetes 插件，在识别到 Pod 模板没有填写 Request 和 Limit 时会有警告提示，这也一定程度上说明了最佳实践是如何。当然如果是部分设置，需要区分情况： 没有设置 Limit，Pod 可以使用集群中的所有资源，可能会导致节点上其他 Pod 无法正常运行。 没有设置 Request，K8s 无法决定在哪个节点上调度 Pod，可能会导致该 Pod 无法正常运行。 总之，Pod 中资源请求和限制的设置是非常重要的，是 Kubernetes 正确调度 Pod 的基础。 访问控制Kubernetes 内置的 RBAC（Role-Based Access Control）是一种基于角色的访问控制机制，用来控制用户对集群资源的访问权限，是一种成熟的访问控制方案。通过 RBAC，我们可以为用户（ServiceAccount）分配不同的角色，不同的角色拥有不同的权限，从而实现对集群资源的精细化控制。其核心是通过 Role、RoleBinding、ClusterRole 和 ClusterRoleBinding 四种资源对象来实现。在 Role 中定义一组权限，包括对何种资源能够采取何种操作，然后通过 RoleBinding 将 Role 绑定到用户上，从而实现对用户的授权。ClusterRole 和 ClusterRoleBinding 与 Role 和 RoleBinding 类似，只是作用于集群级别。 RBAC 相关的资源在 rbac.authorization.k8s.io API 组中，通过 kubectl api-resources 命令可以查看到。下面这是一个简单的 RBAC 角色配置。 rbac-role.yaml123456789101112131415161718192021apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: default name: pod-deployment-managerrules: - apiGroups: [&quot;&quot;] # 空字符串表示 core API 组 resources: - pods verbs: - get - list - watch - create - update - patch - delete - apiGroups: [&quot;apps&quot;] resources: - deployments verbs: [&quot;*&quot;] # * 表示所有操作 上述配置定义了一个名为 pod-deployment-manager 的 Role，该 Role 具有对 Pod 和 Deployment 资源的 get、list、watch、create、update、patch 和 delete 操作权限。然后通过 RoleBinding 将该 Role 绑定到用户上。 rbac-role-binding.yaml1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: namespace: default name: pod-deployment-manager-bindingroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-deployment-managersubjects: - kind: User name: pod-deployment-manager - kind: ServiceAccount name: pod-deployment-manager-sa 上述配置定义了一个名为 pod-deployment-manager-binding 的 RoleBinding，将 pod-deployment-manager Role 绑定到 pod-deployment-manager 用户和 pod-deployment-manager-sa ServiceAccount 上。这样，pod-deployment-manager 用户和 pod-deployment-manager-sa ServiceAccount 就具有了对 Pod 和 Deployment 资源的 get、list、watch、create、update、patch 和 delete 操作权限。将具有权限的 ServiceAccount 分配给 Pod，就可以实现对 Pod 的授权。 在生产环境的部署中，需要按照“最小权限”地原则给每个应用和用户单独分配权限，避免权限过大导致的安全风险。 准入控制准入控制是 Kubernetes 中一种用来控制集群中资源的创建和修改的机制，可以通过准入控制器来实现。准入控制器对请求的资源进行验证和审批，只有通过了验证和审批的资源才能被创建或修改。准入控制器可以通过 Webhook 的方式实现，也可以通过 Admission Controller 的方式实现。在自定义 Kubernetes 拓展时往往涉及到引入自定义资源 CRD（Custom Resource Definition），同大多数 K8s 资源会对配置进行校验一样，我们也可以通过准入控制器对 CRD 的配置进行校验，保证配置的正确性和安全性。 准入控制器的工作原理是当 K8s APIServer 收到请求时，会将请求发送给准入控制器，准入控制器对请求进行验证和审批，然后返回给 APIServer，APIServer 根据准入控制器的结果决定是否允许请求。准入控制器可以对请求的资源进行各种验证，例如验证资源的名称、标签、注解、配置等，在资源不符合校验条件时拒绝请求；也可以对请求的资源进行变更，例如自动添加标签、注解、配置等。这两种准入控制逻辑在 K8s 中分别由 validatingwebhookconfigurations 和 mutatingwebhookconfigurations 实现。API Server 通过调用 Webhook 服务来实现准入控制逻辑，Webhook 服务可以是内部服务也可以是外部服务，如果使用 kubebuilder 等 Operator 构建框架来进行 Kubernetes 的拓展，Webhook 服务是自动集成到 Operator 中的。 小结本文从容器安全、配置管理、访问控制和准入控制几方面介绍了在 Kubernetes 中安全地运行应用的一些方法和最佳实践。目前涉及到的实体交互尚停留在容器本身的运行和配置管理层面，后续将在另外的文章中继续对 Kubernetes 中的网络安全、存储安全、日志安全进行学习。 参考 Docker Init What is Docker Init &amp; When to Use It – Best Practices Kubernetes API v1.25 Kubernetes Security: 9 Best Practices for Keeping It Safe","link":"/2024/05/19/RunAppSafelyInK8s/"},{"title":"rFTP - 用 Rust 实现简单的 FTP Server (2)","text":"前情提要在上一篇文章中，我讲述了我为什么选择 Rust 作为学习计算机底层知识的工具，一些 Rust 的基础知识和我当时所遇到的困难。在这篇又是属于目标回收的文章中，我将介绍最近的进展、Rust 开发的体验和下一步的计划。总的来说，用 Rust 写项目体验尚可，通过与编译器博弈而最终通过“考试”后，自己对 Rust 的理解也有了些许提升。 异步编程在 Rust 中，异步编程是通过 Future 特征和 async/await 语法糖来实现的。Future 是 Rust 中的异步编程的基础，它代表了一个异步计算的结果或者异步任务的“承诺”，可以通过 poll 方法来获取计算的结果。async/await 语法糖则是为了让异步编程更加友好，通过 async 关键字来定义异步函数，通过 await 关键字来等待异步计算的结果。 越来越多的语言采纳异步编程机制，比如 Python 的 asyncio、JavaScript 的 Promise（或者是同样的 async/await）、Golang 的 goroutine 等等。异步编程的优势在于可以提高程序的并发性能，因为异步编程可以让程序在等待 I/O 操作的时候不阻塞，可以继续执行其他任务。Rust 的 Tokio 是一个基于 Future 的异步编程框架，它提供了很多异步编程的工具，比如 tokio::spawn、tokio::net::TcpStream 等等。 Future可以看到，Future 是一个特征，它有一个关联类型 Output，代表了异步计算的结果类型。Future 只有一个 poll 方法，这个方法接受一个 Pin&lt;&amp;mut Self&gt; 类型的参数，返回一个 Poll&lt;Self::Output&gt; 类型的结果。Poll 是一个枚举类型，它有两个成员 Ready(T) 和 Pending，分别代表了异步计算已经完成和异步计算还在进行中。 future_poll.rs123456789pub trait Future { type Output; fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt;;}pub enum Poll&lt;T&gt; { Ready(T), Pending,} 通过 async 关键字修饰函数后，正常的函数将转化为一个异步函数，调用后返回一个实现了 Future 特征的类型，这个类型可以通过 await 关键字来等待异步计算的结果，而 await 关键字则会调用 poll 方法来获取计算的结果。在 poll 方法中还可以通过 cx.waker().wake_by_ref() 来唤醒任务，这样可以让任务在等待 I/O 操作时交出控制权而不阻塞主流程，在 I/O 操作完成时唤醒主流程继续执行。这样等待和唤醒的机制能够精确地控制任务的执行。 Rust 异步编程与其他语言的异同惰性求值 (Evaluation)Rust 的异步编程和 Python 的 asyncio 较为相似，调用 async 关键字修饰的函数后会返回一个“执行器”，直到调用 .await 后才开始执行异步函数和获取结果。Python 的 asyncio 也是这样的，调用 async 修饰的函数后会返回一个协程对象，需要调用 await 来执行协程。这种实现是惰性求值的风格，只有在需要的时候才执行任务，有助于不必要的资源消耗和任务调度，可以更灵活地控制异步任务的执行时机和顺序（例如通过 waker 控制何时唤醒）。 立即求值JavaScript 和 Golang 的异步编程风格则属于立即求值，即调用异步函数后立即执行其中的语句，这种风格更加直观、符合直觉。而它们的具体方案也有所差异： JavaScript 中的异步函数执行完成后是返回一个 Promise 对象，通过对其调用 .then() 或者 .catch() 方法可以获取其执行状态和结果，JavaScript 的实现就相当于调用异步函数后返回一个标识符，用以检查异步任务完成情况； Go 的异步函数 go func() 就显得更加简单粗暴了，没有返回任何标识符，开发者不需要知道协程本身的信息，“任务能跑就行”。当要从协程中返回信息或者是要控制协程的状态时则需要通过传递 channel 进行通信的方式来完成。这可能也是 Go 简单哲学的一处体现吧。 Do not communicate by sharing memory; instead, share memory by communicating.– from Andrew Gerrand proms.js123456789101112131415161718192021(async () =&gt; { const proms = new Promise((resolve, reject) =&gt; { console.log(&quot;Hi, I'm out of setTimeout!&quot;); setTimeout(() =&gt; { console.log(&quot;Hi, I'm in setTimeout!&quot;); resolve(); }, 1500); }); console.log(&quot;Hi, I'm on the root.&quot;); await proms; console.log(&quot;Hi, everything is done!&quot;);})();// Run this script by node proms.js, you will get:// Hi, I'm out of setTimeout!// Hi, I'm on the root.// Hi, I'm in setTimeout!// Hi, everything is done! goroutines.go1234567891011121314151617func main() { go func() { // Simple task }() complexTask := func(done chan struct{}, results chan Result) { for { select { case &lt;-done: // Clean and exit default: // Do something } } } // go func() has no return value. go complexTask(done, results)} 错误处理Rust 的错误处理机制是通过 Result 和 Option 类型来实现的，Result 代表了可能出现错误的结果，Option 代表了可能为空的结果。Result 和 Option 都是枚举类型，Result 有两个成员 Ok(T) 和 Err(E)，Option 有两个成员 Some(T) 和 None。虽然和 Go 一样是通过返回值而非 try-catch 来传递错误信息，但是 Rust 的错误处理更加严谨细致，配合模式匹配和 ? 操作符可以更方便地处理错误。 result_option.rs12345678910111213141516171819202122232425enum Result&lt;T, E&gt; { Ok(T), Err(E),}enum Option&lt;T&gt; { Some(T), None,}fn helper_option() -&gt; Option&lt;i32&gt; { // Do something}fn helper_result() -&gt; Result&lt;i32, String&gt; { // Do something}fn process() -&gt; Result&lt;i32, String&gt; { // Option.ok_or() will return the value if it is Some(T), otherwise return the error message. let value = helper_option().ok_or(&quot;No value found&quot;)?; // Result? will return the value if it is Ok(T), otherwise return the error message. let result = helper_result()?; Ok(value + result)} 当 Option 或者 Result 的值为 None 或者 Err 时，可以通过 ? 操作符来提前返回错误，这样可以减少代码的嵌套和提高代码的可读性。而且不必像 C++、Java 一样在 catch 语句块中单独处理错误信息，打乱原本代码的逻辑结构。 实话说，因为之前写过一段时间的 TypeScript，对其中的 ? 操作符比较有好感的，在 Rust 中使用这样的操作符也是一种愉悦的体验。这是 Rust 错误处理相对 Go 的一个重大优势。在 Go 里面，错误通过返回值来传递，而且没有类似 ? 操作符这样的语法糖，所以在处理错误时需要显式地检查错误并返回，这样会导致代码的嵌套和可读性下降。而且嵌套盘空也是影响代码可读性的一个重要问题。 nested_nil.go1234567891011121314151617181920type A struct { // ...}type B struct { A *A}type C struct { B *B}func (c *C) DoSomething() error { if c.B == nil { return errors.New(&quot;B is nil&quot;) } if c.B.A == nil { return errors.New(&quot;A is nil&quot;) } // Do something with c.B.A return nil} FTP 服务器在 Rust 中实现一个简单的 FTP 服务器是一个不错的练习，可以通过实现 FTP 协议来学习网络编程和异步编程。FTP 协议是一个比较古老的协议，它是基于文本的协议，通过控制连接和数据连接来实现文件的上传和下载。FTP 服务器的实现可以分为两个部分，一个是控制连接的处理，另一个是数据连接的处理。控制连接用于接收客户端的命令和发送响应，数据连接用于传输文件数据。 主流程前面也提到过，我使用了 Rust 中的异步编程框架 Tokio 来实现 FTP 服务器。通过 Tokio::net::TcpListener 绑定通过参数指定的主机和端口后，通过 listener.accept() 方法来接受客户端的连接请求，返回一个 tokio::net::TcpStream 类型的流。随后通过 tokio::spawn 方法来创建一个异步任务 handle() 来专门处理来自该客户端的连接请求，这样可以让主线程继续接受其他客户端的连接请求。 在 handle() 中，首先会将 TcpStream 通过 into_split() 方法分割为读取流和写入流，然后通过读取流来接收客户端的命令，通过写入流来发送响应。在处理命令的过程中，会根据命令的类型来分发调用不同的处理函数，比如 USER 命令会调用 user() 函数，LIST 命令会调用 list() 函数等等。在我最初的实现中，在处理完完成一条命令后，服务器才会通过 writer.write_all() 方法来发送响应，然后继续等待下一个命令。但这种实现方式会在传输数据时阻塞主线程，导致中断传输命令 ABOR 无法被及时处理。于是再后来的优化里，我将发送响应的操作也放到了异步任务中，这样可以让主线程继续接受其他客户端的连接请求，而不会被阻塞。 下面的代码片段就是这个 FTP 处理客户端请求的主要流程。 ftp_server.rs12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849impl Server { pub async fn listen(&amp;self) { loop { if let Ok((socket, addr)) = self.listener.lock().await.accept().await { // 收到客户端连接请求后，创建一个异步任务来处理 let shared_self = self.clone(); tokio::spawn(async move { shared_self.handle(socket, addr).await; }); } else { continue; } } } pub async fn handle(&amp;self, socket: TcpStream, addr: SocketAddr) { let user_map = self.user_map.clone(); // 将 TcpStream 分割为读取流和写入流 let (mut reader, mut writer) = socket.into_split(); { // 检查用户是否已经登录，如果是新用户，则发送欢迎信息 let mut user_map_locked = user_map.lock().await; if !user_map_locked.contains_key(&amp;addr) { if let Err(e) = writer .write_all(b&quot;220 xxx.xxx.xxx.xxx FTP server ready.\\r\\n&quot;) .await { // ... } } } // 创建一个 Arc&lt;Mutex&gt; 来保护写入流，避免多个异步任务同时写入 let writer_guard = Arc::new(Mutex::new(writer)); loop { let mut buf = vec![0; 2048]; // 读取客户端发送的命令 let req = { let n = match reader.read(&amp;mut buf).await { // ... }; String::from_utf8_lossy(&amp;buf[..n]).to_string() }; // ... // 分发命令并进行错误处理 tokio::spawn(async move { let cloned = cloned_writer.clone(); cloned_self.dispatch(cloned_writer.clone(), cmd, user).await; }); } }} 主要涉及的结构体服务器 Server服务器结构体 Server 很简单，只包含了服务器的地址、端口、根目录、监听器、用户映射等信息。其中监听器是一个 Arc&lt;TcpListener&gt; 类型的字段，给 TcpListener 加上了 Clone 特征以满足在 Future 间传递的要求。用户映射是一个 Arc&lt;Mutex&lt;HashMap&lt;SocketAddr, Arc&lt;Mutex&lt;User&gt;&gt;&gt;&gt;&gt; 类型的字段，用于保存用户的信息。实现了 new() listen() handle() dispatch() 等方法和 FtpServer 特征。 server.rs1234567pub struct Server { pub host: String, pub port: u16, pub root: String, pub listener: Arc&lt;TcpListener&gt;, pub user_map: Arc&lt;Mutex&lt;HashMap&lt;SocketAddr, Arc&lt;Mutex&lt;User&gt;&gt;&gt;&gt;&gt;,} 用户 User用户结构体 User 用于保存用户的信息，包括用户名、密码、当前目录、数据连接等信息。用户结构体还包括了一个 PathGuard 类型的字段，用于保护用户的当前目录，避免用户越权访问文件系统。用户结构体还包括了一个 TransferSession 类型的字段，用于保存数据连接的信息，包括数据连接的类型（主动或被动）、数据连接的地址等。 user.rs123456789pub struct User { pub username: String, pub status: UserStatus, pub addr: SocketAddr, pub session: Option&lt;Arc&lt;Mutex&lt;TransferSession&gt;&gt;&gt;, pub trans_type: TransferType, // ASCII or Binary path: PathGuard, // 保护 PWD 的工具类型} 传输会话 TransferSession传输会话结构体 TransferSession 用于保存数据连接的信息，包括数据连接的类型（主动或被动）、数据连接的地址、文件的总大小、已传输的大小、文件名等信息。传输会话结构体还包括了一个 offset 字段，用于指定文件的传输偏移量，以便在传输中断后继续传输（实现了 REST 命令）。 session.rs123456789pub struct TransferSession { pub mode: TransferMode, // PORT/PASV pub total_size: u64, // 文件总大小 pub finished_size: u64, // 已传输大小 pub file_name: String, // 文件名 pub finished: bool, // 是否传输完成 pub aborted: bool, // 是否传输中断 pub offset: u64, // 传输偏移量} 开发经验使用标准工具对拍在开发过程中，我发现了一个很好的测试方法，那就是使用标准的 FTP 客户端来对拍自己的 FTP 服务器。通过使用 Python 的 ftplib.FTP 和 macOS 上的 Transmit 这个标准的 FTP 图形化客户端，我可以看到客户端和服务器之间的交互过程，可以更好地发现自己的错误。这种对拍的方法可以让开发工作更有目标，因为我知道自己的 FTP 是符合标准的。 使用 {} 来缩小作用域在 Rust 中，可以通过 {} 来缩小变量的作用域，这样可以避免变量的生命周期过长，提高代码的安全性。在开发过程中，因为通过 Mutex.lock() 方法拿到的是一个 MutexGuard 类型，它的生命周期和得到的锁的生命周期是一样的，所以可以通过 {} 来缩小锁的作用域，避免锁的生命周期过长而导致可能的死锁。 多看编译器的提示Rust 是以编译时检查为主的语言，编译器会给出很多有用的提示，比如未使用的变量、未处理的错误、不安全的代码等等。在开发过程中，我发现多看编译器的提示可以帮助我更好地理解 Rust 的语法和规则，提高代码的质量。通过解决编译器提出的问题，有种准备考试时专项训练的感觉，完成这些测试就能拿到不错的成绩，这个过程是一种挣扎也是一种解谜，我的直观感受是节省了不少查阅文档的时间。 当前进展和下一步计划已实现功能已经实现的功能包括了 FTP 服务器的基本功能，包括用户登录、文件传输、目录操作、文件操作等。下面是在开发过程中定义的命令枚举类型，包括了目前 FTP 服务器支持的所有命令。 commands.rs1234567891011121314151617181920212223242526272829303132pub enum FtpCommand { USER(String), PASS(String), PORT(SocketAddr), PASV, RETR(String), STOR(String), ABOR, QUIT, SYST, TYPE(String), RNFR(String), RNTO(String), PWD, CWD(String), MKD(String), RMD(String), LIST(Option&lt;String&gt;), REST(u64), // To be tested DELE(String), STAT(Option&lt;String&gt;), STOU, APPE(String), ALLO(u64), NOOP, NLST(Option&lt;String&gt;), CDUP, FEAT, MDTM(String),} 下一步计划从 2023 年写毕业论文思绪万千时想用 Rust 写 FTP 服务器来放松心情，到 2024 年的今天终于算是完成了这个玩具工具的开发。从 Git 提交记录来看其间搁置了接近 17 个月，6 月终于是有两三周的时间和动力来完成这个小项目。下一步的计划是继续完善该项目的功能，包括但不限于下面的几项。最终要是能让这个工具变得真正有用就再好不过了。 FTP 客户端: 实现一个简单的 FTP 客户端，配套测试服务器的功能 优化代码: 优化代码结构和性能，提高代码的可读性和可维护性 补全测试: 增加更多的测试代码，提升代码质量；进行压力测试，检查性能效果 制品发布: 使用 Docker 和 Github Action 构建和发布制品，便于部署和使用 这篇文章里实现的 FTP 服务器 “rFTP” 的代码可以在我的 Github 仓库 powerfooi/rftp 中查看。 参考资料 rfc959 The Rust Programming Language Tokio - An Asynchronous Rust Runtime asyncio — Asynchronous I/O ftplib — FTP protocol client","link":"/2024/06/22/RFTP-2/"},{"title":"一起来做类型体操吧！😊","text":"类型重要吗？长话短说，类型很重要。 类型的重要性静态检查在编程中，类型往往是我们的第一道防线，它可以帮助我们在编译阶段就发现一些潜在的问题，避免一些不必要的错误。在过去，由于 JavaScript 是弱类型、解释型语言，所以在编译阶段无法发现一些类型相关的问题，这就需要我们在运行时进行一些类型检查，这样就会增加一些不必要的开销。而 TypeScript 则是由微软推出的、基于 JavaScript 的强类型语言，它可以在编译阶段就发现一些类型相关的问题，这样就可以避免一些不必要的错误。 当然也不是说用 JavaScript 就一定会出问题，但这要求编程者有更高的责任心和编程能力，能够在编码阶段就提前规避问题，但对于大型项目来说，这是不现实的。不要完全相信任何人的代码，即使是自己的代码。因为人是会犯错的，需要加以约束。在 Web 应用开发过程中，JavaScript 代码如果访问了一个空对象的字段则会导致异常，如果程序有限定错误边界，那么这个错误可能会被忽略，但是如果没有限定错误边界，那么这个错误可能会导致程序崩溃，也就是页面白屏。而使用 TypeScript 配合 IDE 的类型检查、其他静态检查工具，可以在编码、编译和代码合并时就发现并修复这些问题。 类型即文档另外，用弱类型语言编写的项目一旦涉及到多人协作（甚至是对于现在的自己和过去的自己来说也是如此），就会变得难以维护和协作，因为弱类型语言无法提供足够的信息，所以在多人协作时，很容易出现一些问题。而强类型语言则可以提供足够的信息，帮助我们更好地理解代码，提高代码的可维护性。所以我也认为“类型即文档”。 现在很多语言都有相关工具可以通过类型信息来生成文档，例如 openapi-generator、TypeDoc、JSDoc 等等，通过代码自动生成文档可以帮助开发者省去很多写文档的时间，同时也可以极大程度地提高交流效率。 脚本语言的类型脚本语言通常以小巧方便为优势，在最初设计时没有考虑到类型检查的问题，所以在设计时没有引入类型系统，这样可以减少一些不必要的开销。但越来越多人用脚本语言来开发大型项目，这时就需要引入类型系统来帮助我们更好地维护代码。例如纯粹的 JavaScript 有 JSDoc 和 Flow、Python 有 MyPy、Ruby 有 Sorbet、PHP 有 Hack、Lua 有 Typed Lua 等等。 JavaScript在纯粹的 JavaScript 中，我们可以使用 JSDoc 来对函数的参数和返回值进行类型注解，这样可以起到类型标注的作用，这无法起到类型检查的作用，但是可以使得部分的 IDE 在编码时提供更好的提示。 test-js-docs.js1234567/** * @param {number} a * @param {number} b */function add(a, b) { return a + b;} Facebook (现在的 Meta) 的 Flow 是一个 JavaScript 静态类型检查工具，它有着和 TypeScript 类似的功能（甚至是极其类似的语法，TypeScript 的语法设计应该很大程度上借鉴了 Flow 的设计），可以在编码阶段就发现一些类型相关的问题。 test-flow.js1234// @flowfunction add(a: number, b: number): number { return a + b;} PythonPython 3.5 开始引入了类型提示，可以通过 typing 模块来对函数的参数和返回值进行类型注解，这样可以起到类型标注的作用，这无法起到类型检查的作用，但是可以使得部分的 IDE 在编码时提供更好的提示。 python-typing.py1234567from typing import Listdef add(a: int, b: int) -&gt; int: return a + bdef concat(a: List[int], b: List[int]) -&gt; List[int]: return a + b Python 中的类型系统要比 TypeScript 弱一些，只能起到标注和提示的作用，无法进行类型检查，但是可以通过一些工具来进行类型检查，例如 mypy。我极为乐于看到的是，目前有些 Python Web 框架已经在利用类型提示来生成文档，例如 FastAPI。 什么是类型体操？在 TypeScript 内置的类型中，有很多工具类型，比如 Partial、Required、Readonly、Record、ReturnType、Parameters 等等，这些工具类型可以帮助我们更好地操作类型。所谓类型体操，就是仅基于 TypeScript 的各种内置的类型和类型操作符如infer, typeof, keyof, extends 等来实现一些工具类型，使用 TypeScript 的类型推导能力来运行具体的逻辑、消除 IDE 的警告、提高代码的可读性等。Python 的类型体操也是类似的，只不过 Python 的系统类型较弱，不能够实现像 TypeScript 那样灵活的类型操作。 类型挑战题库说到这里不得不提一下 type-challenges 这个 Github 仓库，其中有很多关于 TypeScript 的类型挑战，有需要时可以进行检索查阅，有空闲时间的话也可以到其中进行解题挑战，帮助自己更好的掌握 TypeScript 中的类型系统和编写类型的技巧。推荐在完整了解和使用过 TypeScript 中的基础工具类型和类型操作符之后再上手进行挑战，不然会有较高难度。当然最好不要过于钻牛角尖，毕竟类型体操有千千万万种题面，且为了设计成挑战题目，有些需求是不切实际的，所以不要过于纠结于题目的细节（有些题目为了难而难），而是要关注练习和学习的初衷。 一些例子和个人理解DeepReadonly这道题目的要求是实现一个 DeepReadonly 工具类型，使得所有的属性都变成只读的，包括嵌套的属性。其中有个测试用例是这样的： deep-readonly-case.ts12345678910111213141516171819202122232425262728293031323334353637383940414243444546type X1 = { a: () =&gt; 22; b: string; c: { d: boolean; e: { g: { h: { i: true; j: &quot;string&quot;; }; k: &quot;hello&quot;; }; l: [ &quot;hi&quot;, { m: [&quot;hey&quot;]; } ]; }; };};type Expected1 = { readonly a: () =&gt; 22; readonly b: string; readonly c: { readonly d: boolean; readonly e: { readonly g: { readonly h: { readonly i: true; readonly j: &quot;string&quot;; }; readonly k: &quot;hello&quot;; }; readonly l: readonly [ &quot;hi&quot;, { readonly m: readonly [&quot;hey&quot;]; } ]; }; };};type cases = [Expect&lt;Equal&lt;DeepReadonly&lt;X1&gt;, Expected1&gt;&gt;]; 如果仅仅是递归地将属性变成只读的话，那么这个题目就太简单了，但是这个题目的难点在于如何处理数组，因为数组是一个特殊的对象，它的属性是数字，而且数组是可变的，所以我们需要将数组的属性也变成只读的，同时也需要将数组的元素变成只读的。这个题目能通过测试用例的解法是这样的，首先判断一个类型是否拓展了 Function，如果是的话就直接返回原类型，否则就递归地将其中的属性变成只读的。 deep-readonly.ts1234567// 能通过测试用例的解法type DeepReadonly&lt;T&gt; = T extends Function ? T : { readonly [k in keyof T]: DeepReadonly&lt;T[k]&gt; };// 我认为正确的写法type DeepReadonly&lt;T&gt; = { readonly [k in keyof T]: DeepReadonly&lt;T[k]&gt; }; 但我认为这个解法有问题，例如上面的用例片段中 X1['b'] 是一个 string 类型的字段，我判断 X1['b'] 是否是 Function 类型时会返回 false，但因为这个写法通过了测试用例，说明在执行判断时 DeepReadonly&lt;X1['b']&gt; = DeepReadonly&lt;string&gt; = string，这是不对的，因为 string extends Function 的结果为 false，就不应该会返回原来的类型。但如果去掉了这个终止条件的判断，仅通过递归的调用，如何达到终止条件呢？ 原来 never 类型是 TypeScript 中的底类型，它是所有类型的子类型，所以 never 类型可以赋值给任何类型，任何类型都无法赋值给 never。所以我们可以通过 never 类型来终止递归的调用，这样就可以达到终止条件。 recursive-exit-point.ts1234type cases = [ Expect&lt;Equal&lt;DeepReadonly&lt;never&gt;, never&gt;&gt; // 该断言是通过的，任何涉及 never 的类型都会相等，这也是隐藏的递归终止条件]; TupleToObject这道题目要求将元组转换为键值相等的对象，解法中利用了元组可以通过数字下标来访问属性的特点: tuple-to-object.ts123456type TupleToObject&lt;T extends readonly (keyof any)[]&gt; = { [k in T[number]]: k;};type tuple = readonly (keyof any)[]; // readonly (keyof any)[] 可以表示任何类型的元组type tuple = readonly (string | number | symbol)[]; MyAwaited这道题目要求实现一个异步函数递归等待的工具类，这个题目的难点在于如何递归地等待 PromiseLike 类型的结果，通过 infer 关键字来获取 PromiseLike 的结果类型，然后判断这个结果类型是否 也是 PromiseLike 类型，如果是的话就继续递归地调用 MyAwaited，否则就返回结果类型。在 extends 类型判断语句中能够使用 infer 关键字来获取类型，这是 TypeScript 中的一个高级特性，能够玩出很多花样。 my-awaited.ts12345type MyAwaited&lt;T extends PromiseLike&lt;any&gt;&gt; = T extends PromiseLike&lt;infer U&gt; ? U extends PromiseLike&lt;any&gt; ? MyAwaited&lt;U&gt; : U : never; Chainable这道题目要求实现一个链式调用设置对象参数的工具类，而且要求不能重复设置相同的键。题目的难点在于如何判断一个键是否已经被设置过，通过 K extends keyof T ? never : K 可以判断一个键是否已经被设置过，如果已经被设置过就返回 never 类型（并导致调用时报错），否则就返回原本的键类型。 chainable.ts1234567type Chainable&lt;T = {}&gt; = { option: &lt;K extends string, V&gt;( key: K extends keyof T ? never : K, value: V ) =&gt; Chainable&lt;Omit&lt;T, K&gt; &amp; Record&lt;K, V&gt;&gt;; get: () =&gt; T;}; 项目中的案例类型体操并不完全是为了脑筋急转弯，它在实际的项目中也是可以发挥作用的，适当地进行类型编程可以在适当的场合节省较多的工作量。 需求背景在我参与的项目几个月前引入 OpenAPI Generator typescript-axios 后涉及了 API 请求响应类型的变化。具体来说生成的客户端代码中 Axios 请求方法返回的是 AxiosResponse，而原有代码中的请求方法返回的都是 Promise。这一转变要求在所有前端组件中都需要修改处理 API 返回结果的逻辑，这是一个非常繁琐的工作，因为项目中有很多组件，而且每个组件中都有很多请求方法，如果工作量过大会导致协作者不愿意引入新的工具 OpenAPI Generator。所以我想到了通过包装一个 TypeScript 转换器来解决这个问题。既要保证生成的 API 工厂类型能够在 IDE 中得到正确的代码提示，又要保证转换后的代码能够正确地将返回结果从 AxiosResponse 转换为 Promise 类型。 解决方案wrapper.ts123456789101112131415161718192021222324import { ClusterApiFactory, Configuration } from &quot;./generated/index&quot;;const config = new Configuration({});type factoryFunction&lt;T&gt; = ( configuration?: Configuration | undefined, basePath?: string | undefined, axios?: AxiosInstance | undefined) =&gt; T;const wrapper = &lt;T&gt;( f: factoryFunction&lt;T&gt;, ...args: Parameters&lt;factoryFunction&lt;T&gt;&gt;): PromiseWrapperType&lt;T&gt; =&gt; { return f(...args) as any;};type PromiseWrapperType&lt;T&gt; = { [K in keyof T]: T[K] extends (...args: infer P) =&gt; AxiosPromise&lt;infer R&gt; ? (...args: P) =&gt; Promise&lt;R&gt; : never;};export const clusterApi = wrapper(ClusterApiFactory, config); 参考资料 TypeScript Handbook Type Challenges What is “extends never” used for OpenAPI Generator FastAPI","link":"/2024/07/27/TypeGymnastics/"},{"title":"记黑神话悟空云通关☁️","text":"第一印象游戏科学在 2020 年 8 月放出黑神话悟空第一个实机演示视频以吸引游戏人才，“被迫”开始了对外宣传，到今年 2024 年 8 月 20 日正式发售，经历了四年的时间。这四年里，每年的 8 月 20 日都会释放出新的实机演示视频，赚足了单机游戏玩家的期待，其中自然也包括了我。 我在小学时期玩过很多游戏，各种各样的游戏都爱玩，仙剑奇侠传、波斯王子、英雄无敌、魔兽争霸、鬼泣等，对单机游戏是有足够好感的。中学之后娱乐时间变少后玩游戏的时间就趋近于零了，这个习惯一直延续到了大学毕业。在研究生阶段中期，多多少少受到些黑神话悟空的影响，我又开始断断续续接触了几款动作 RPG 游戏，例如古剑奇谭三、塞尔达传说、只狼、黑魂三、刺客信条·奥德赛等。 从释出的实机演示视频来看，黑悟空与近期接触过的只狼比较相似，战斗系统、画面风格、故事背景（都具有东方特色）等，而只狼是我玩过类似游戏里最喜欢的。黑悟空刺激的战斗系统搭配上西游记的故事背景，进一步拔高了我对黑悟空的期待。 八十一难之第零难：启动游戏我手头的设备是 MacBook Pro 和 Switch，并且 Switch 长时间吃灰，彻底沦为了有氧拳击启动器。同时因为没有业余时间打游戏的习惯，没有再增配主机，近期只对黑悟空这一款游戏感兴趣，不打算单单为了它添置设备，所以我开始摸索如何在 Mac 上运行黑悟空。 Parallel Desktop ❌Parallel Desktop 是 macOS 上一款虚拟机软件，用来在 Mac 上运行 Windows 系统。我在 Parallel Desktop 17 上安装了 Windows 11，下载和安装 Steam 后再下载了黑神话悟空，抱着期待启动游戏后发现在头箍加载进度条界面会闪退，无法正常运行。用兼容模式启动亦是如此。尝试在网上搜索解决方案无果，遂放弃该路径。 CrossOver ❌CrossOver 是一款 macOS 上的软件，可以在 Mac 上运行 Windows 程序。我在 CrossOver 21 上安装了 Steam，再下载了黑神话悟空，启动游戏后仍然出现闪退的情况，查看网上的攻略后发现在 CrossOver 上运行黑神话悟空需要用到苹果的游戏转译工具包 GPTK (Game Porting Tool Kit) 2.0，而安装 GPTK 2.0 则需要 macOS 升级到最新的 15 beta 版本。因为踩过苹果产品操作系统测试版本的坑，觉得太过折腾遂也放弃了该方案。 云游戏 ✅我打电话咨询了附近的网咖询问他们店里是否支持游玩黑神话悟空，他们都表示暂不支持，而且就算支持了我也比较担心网咖里二手烟的状况，遂作罢。 之前玩原神时曾尝试过米哈游的云游戏，体验尚可。黑神话悟空推出之后很多游戏厂商和云计算厂商都宣传了自己云游戏的服务，出于对游戏的期待和对云游戏当前发展状况的“信赖”，我开始了云游戏的尝试。如果是在 WeGame 上购买的黑神话悟空，可以尝试腾讯的 Start 云游戏；因为我是在 Steam 上购买的游戏，所以选择了网易云游戏。 云游戏简单来说就是把用户电脑当做视频播放器和指令发送器，游戏实际上是运行在云游戏厂商所管理的主机上的。只要网络速度足够快，用户指令和远端画面能够进行低时延地双向传输，游玩的体验能够做到与本地运行别无二致。 抱着试一试的心态打开了网易云游戏上专门为黑悟空开设的游戏项目。打开之后实际上是一台 Windows 云电脑，上面安装了 Steam 和相应的加速器，Steam 中也预装了黑悟空，只需要登录自己的 Steam 账号即可开始游玩。我对什么样的显卡能得到什么样的画面效果不太了解，但我觉得网易云游戏上的“高配”机器运行黑悟空的体验还不错，所有的画面效果都能达到全高，600M 带宽的情况下大部分时候画面传输延迟在 20-30ms，仅需要约 2.4 元/小时的价格对我而言是极其划算的，毕竟我只打算短期内玩这一款游戏。 游玩体验初上手开局扮演大圣大战天兵天将，跟二郎神单挑作为新手教程还是很不错的，相当于游戏初期让玩家接触到了后期会拥有的技能，让玩家心里有底并且对未来的角色的发展抱有期待。主角每一个技能的获取都有一段小故事，每获取一个技能都会让玩家有种成就感，让玩家探索欲望更强。 但因为没有内置的地图，在黑风山幽魂地图里我迷路了许久，上来的第一个 Boss 就是幽魂，在这里应该卡了有 1 小时，作为第一个 Boss 我觉得稍微有些难度，难怪网上好多玩家反映说这个 Boss 有些劝退。后来找到路先打了广智获取了变身技后，再来打它就变得很容易了。后来看了网上的攻略发现幽魂原来是可以先跳过不打的，确实多花费了不少时间…以至于后来我基本都是一边看着小黑盒的章节地图，一边探索和找寻各章的物品和首领。 中间两天出门旅游只带了笔记本电脑，闲暇时候打开了云游戏，意外发现仍然能够正常游玩，锁定敌人目标之后完全不需要鼠标或者手柄就可以打败大部分体积较小的 Boss 了，在此向白衣秀士、金池长老、黑风大王、黑熊精等 Boss 表示抱歉，你们都是我用触控板打的。 前四章的战斗和剧情都酣畅淋漓，每一章都各有特色，黑风山的绿水青山、黄风岭的飞沙走石、小西天的天寒地冻、盘丝岭的盘丝错节…章节通关动画短片也质量不俗，其中第二章末书生南柯一梦后残害白狐的情节和第三章的故事紧密相连给了我较强的代入感。让本来基本只做主线任务的我在小西天不得不去把“不空”教训了一顿，为白狐伸张正义。 到了第五、六章的游玩体验就没有前四章那么好了。首先是剧情不是那么连贯和吸引人，其次是地图设计变成了线性推进，另外 Boss 的来历也没有过多介绍，见面就是战斗，而且有很多体型奇怪（急如风、碧水金睛兽、王灵官等）、攻击欲望极强（赤金战车、碧水金睛兽）的怪物，让我屈死了很多次… 但所有磨难带来的苦楚最终都在我凑齐大圣套装，走上天真顶时烟消云散了，击败杨戬、石猿和大圣残躯后观看不戴金箍的通关结局动画时心中激起了万般感慨，希望找寻大圣六根的旅途能够持续进行下去，希望能够在未来的拓展内容中探索更多章节，挑战更多首领。这种心情如同回到了四年前刚看到宣传动画时，又充满了同样的期待。 几场最喜欢的战斗虎先锋虎先锋是我的闪避和识破老师，虽然有着庞大的身型和暴躁的脾气，但他还是很讲武德的，不管是拳打脚踢还是舞刀弄剑都富有节奏，练就了我闪避和识破的基本功，让我对阵后续的首领时更加游刃有余。作为早期试玩阶段就释放出的首领，它本身就自带了很多梗。虽然首发日当天很多玩家都被卡在第一章幽魂处，但猴头外卖使命必达，该上的课一节也不会落下。我遭遇虎先锋时我并没有身外身法这个技能，所以一对一地打败它的，给了我很强的正向反馈。 同时，虎先锋的战斗场景血池的设计也非常精美，十分符合虎先锋残忍暴戾的性格特点。角色在血池里奔跑、翻滚和打斗都会激起池面的波纹，让我有了更多的代入感。虚幻 5 引擎的实力在这些细节上体现出来了，同样让我觉得精美的场景还有后面的沙漠和雪地，人物行走时腿部和棍棒接触地面都会画出痕迹，效果逼真令人赞叹。 黄眉“既见未来，为何不拜。”如果说跟虎先锋的战斗是磨砺了基本功，那么和黄眉的战斗则是一同演绎了一场对角戏。严格来说与黄眉的战斗有三个阶段，每个阶段的难度都不是很高，但游玩时可以从黄眉的台词中得到很多背景信息，了解到他是如何妖言惑众，蛊惑人心的。“信什么如来，不如我自己来”这样倒反天罡的话从他嘴里说出来却不觉得违和，因为他就是这样一个自视甚高、不知天高地厚的角色。 在第二阶段的人种袋里，天命人被蛊惑着击杀了赤尻马猴，随后索性被化身成了赤尻马猴。在黄眉的言语蛊惑下将沿途的陶俑全都“杀死”，变成了嗜血成性的恶人。想必黄眉座下弟子都是被同样的妖言所迷惑，变成了十恶不赦的人，这在第三章的过场动画中也得到了印证。 杨戬击败杨戬是触发不带金箍结局的必要条件，在游戏里他为了帮助大圣封存六根的秘密找弥勒帮忙藏身于浮屠塔下的壁画中，梅山里万径人踪灭，杨戬整日只有与哮天犬作伴，不知持续了多少年，他的孤独之甚不得而知，天命人进入梅山来到他面前时他才会如此欢喜。他是大圣出生入死的挚友，“继承了大圣的意志”，在梅山中非常难熬。打其他的 Boss 时或多或少都有些逃课打法，但是打杨戬却没有太多的技巧。虽然他只有一条血条，但是却拥有三个阶段和可再生的护盾机制，拥有大量的招式和技能，近距离和远距离攻击招式都有且伤害不俗，这让他成了很多人难以通过的难关。 杨戬是人形 Boss 特点的集大成者，与他战斗虽说会遭遇很多次失败，但是最终战胜他时给人带来的喜悦是爆棚的，更别说法天象地里暴打四大天王的奖励关卡了。也正是因为杨戬的战斗风格特点众多，所以打法也有很多。因为之前在网上看到过芭蕉扇配合戳棍搅阵的简易组合打法，我没有认真研究这个打法，尝试过几次之后我就放弃了这个简易打法，转而采取了禁字法识破流。卡了两三个小时之后最终是把他打败了，不得不说杨戬的快慢刀比较多，而且哮天犬和飞剑的存在让专注识破的我变得捉襟见肘，最终以微弱的优势战胜了他，得知了这这场战斗是原来是他在为大圣考验天命人。 再入轮回，更多惊喜到现在一周目双结局我大约花了 50 小时的游戏时间，人物等级是 90，当我觉得一周目有些“无聊”后，我选择了“再入轮回”。在新的游玩中发现了许多一周目没有的意外之喜，例如： 得到了第六个根器“意”，其中一项能力是让“身外身法”变换出的毛猴能够跟随天命人一同使出某些强力招式，例如三四段棍势的蓄力重击，轻棍的最后一击等。 得到了与“身外身法”平行的另外一个法术“救命毫毛”，装备上这个法术后可以让天命人战败后立即复生，有些只狼战死复活的影子。 每个根器在二周目击败对应章节的关底首领之后也能得到对应的升级，最终的效果应该是能够把每个根器的每项能力都激活。 击败寅虎可以佩戴更多的珍玩，上线似乎是五个槽位。 二周目在我看来是纯粹的“爽玩”，因为天命人的装备和能力得到了很大的提升，但是妖怪们的数值提升却不大，玩家得以在二周目为一周目收到的欺负出出恶气。例如一周目中卡了我 1 小时的幽魂在二周目中被身外身法的毛猴们四段棍势蓄力重击一棍击败，让心神得到了极大的愉悦～据说天命人的等级上限是 342，在棍法、法术、变身技能众多的基础之上，我认为至少完成三个轮回才能够差不多把大部分机制都体验到位。就我自己而言，我一周目完全使用劈棍的棍法，没有怎么接触过立棍和戳棍，算是一大遗憾吧。 我的期许既然已经在一周目把两个结局都完成了，在二周目前三章发泄了一周目的郁闷之后也暂停继续玩下去的念头，安心等待后续拓展内容的发售了。对于拓展内容我有两个方面的期待，第一我认为灵吉菩萨背后还藏着不为人知的秘密，在斯哈哩国的剧情当中黄风大圣形象是正直、勇敢的，而到了主线剧情中则变得贪婪、邪恶，这些应该都是灵吉菩萨所为，我希望在接下来的拓展内容中能够将这条故事线继续深入挖掘下去；第二是蜘蛛精四妹，在击败盘丝洞右手虫之后她忧心忡忡地谈到希望看到山岭外的世界，并最终在黄花观被鹤仙人抓走，她和大圣、和天命人之间有什么纠葛，她的下落和生死状况究竟如何，我也希望在接下来的 DLC 中能够有所涉及。 纵使游戏中确实存在不少问题，例如空气墙、无地图设计、隐藏支线指引欠缺以及后两章剧情不够立体等，但依旧瑕不掩瑜，它在我这里的评分仍然是 10/10，是我的 2024 年度游戏。 参考资料 Game Porting Tool Kit 云游戏白皮书 - 华为","link":"/2024/09/12/BlackMythWukong/"},{"title":"再见北京 👋","text":"因公司在北京举办的活动需要人员到现场支持，我提前了一个周末来到北京。刚到那天的温度就给了我一个下马威：寒潮过境，最低温度来到了 0 度，而我刚从气温接近 30 度的杭州过来，一时间难以适应。 上午 10 点到达预定的酒店被告知没有空余房间，这也合乎情理，金秋时节的北京是热门的出游目的地，游客众多，用房需求也相应的高。和朋友相约到二里沟地铁站旁边的新疆大厦吃了顿自助，羊肉串、羊腰大快朵颐，酸奶、格瓦斯畅快下肚，好不痛快，就是最后肠胃承受了消化的重负，撑了一下午。 晚上跟许久没见的好几位朋友吃火锅，一年多没见还是那么熟悉，仿佛一切都还像在学校里。桌上 7 个人，3 个人已开始工作，另外 4 人仍博士在读，从他们口中了解到许多学校里的趣事，欢声笑语间碗碟、菜品交递太多，专门的服务员也难于应付，只能陪声道歉。 次日上午有些时间能来到校园里拍照，去年没见北京的秋，此番再会仍然让人心旷神怡。打车到学校西门，跟着游客们的步伐一起进入校园，看到夹道的树木刚开始时茂密苍翠，而接近二校门处道路两侧金黄的银杏叶却比阳光还耀眼。游客群体中不乏一家三口者，也不知道此时家长们有没有鼓励孩子努力学习并寄予某种期许。 因校园比较大，在校园里步行效率是比较低的，我也只跟随大多数游客的步伐在“经典打卡点”简单拍了拍照。但我专门到明德路边足球场外看望了那边的花圃，铁丝网背景映衬的花朵在前年差不多的时候给了我一些慰藉。校园封闭时期的秋招让人十分郁闷，心烦意乱时骑车路过这片花圃会让我重新焕发一些生机和斗志。校园里其他处的花和叶也都被阳光染得很美。 但周日下午就已经变阴天了，没有阳光的秋里一切又回归了万物凋敝的旋律。和好友艰难挤上 10 号线往城东南走，我们走马观花地转了一圈潘家园古玩市场，我甚至顺便配了副眼镜。 工作日正常到公司上班，幸而住得较近，无需搭乘地铁，上班体验良好。23 日公司的活动顺利举办，当天早上 7 点就位，一直持续到了晚上 9 点，虽然持续时间比较长，但整场持续下来感觉并不疲倦，可能睡得越少人越精神吧。 活动在晚上收场后我还同一位朋友再吃了顿夜宵。这位朋友也恰好从深圳到北京出差，毕业后就没有见过面，但当面说上两句话又觉得两人风格没有任何改变。工作内容、生活环境、社交圈子都发生重大变化后，以为已经时过境迁物是人非，但都回到北京碰面侃天侃地，同时发出“还好不留北京”的感叹时，还是让人感到亲切。 之后几天又跟好几位朋友、同学和同乡的学弟妹们碰了面，仿佛为这次北京之行打上了一个个对勾。去年毕业时的愉快与不愉快都在回北京的一周时间内被浓缩成一个生态球，随着毕业时的忙碌被搁置在一边，以至于我始终没有什么毕业的感觉。对我而言只是多了一份学位证书、毕业证书而已。这次得知可以在北京停留一周时间后，我竟感到近乡情怯，要如何同一年未见的朋友们交谈，要跟哪些朋友见面做什么事情，要去哪些地方记录秋色等等对我而言都成了待定的问题。还好朋友们仍然像从前那样，秋景也如前年一般绚烂，唯一不同的是自己的心境。我偶尔遐想若自己去年选择留在北京工作会是怎样的光景，但这次回来打消了我未来的遐想空间。 拥挤的人潮，昏暗的车灯，阴沉的雾霾，凛冽的寒风，干燥的空气，凡此种种都在提醒我，我已经彻底成为了这里的客人。去年夏天的饯别放在了今秋，离别的话也就留在今天吧。第二个周末里北京被雾霾笼罩，如同 2016 年我刚来时看到的灰蒙蒙的样子，陌生又熟悉。 祝所有在北京的朋友们一切顺利，再见北京。👋","link":"/2024/10/27/GoodByeBeijing/"},{"title":"用 Cursor 快速搭建软件原型","text":"Cursor 是一款使用 AI 能力加持的代码编辑器，或者说得更大一点，文本编辑器。正如它在官网所说的那样 Built to make you extraordinarily productive, Cursor is the best way to code with AI. 使用它能够让在编程时更加高效，几倍地提升编码效率，将工程师从平常繁琐、简单、机械、重复的代码编写工作中解放出来，更加专注于分析、建模和解决关键的问题。 但需要说在开头的是，Cursor 无法让零基础的用户变成专精的软件工程师，以能够解决所编写代码中发生的各种疑难杂症，但能让他们轻松搭建出一些有意思的应用，例如简单的网页，便捷的脚本等，真正遇到问题需要定位和调试的时候，往往是需要人类提供指导方案的。Cursor 能够做的是让有一定编程基础的用户更加高效地编写代码，让专业的软件工程师更加专注于解决问题本身。简而言之，让零基础的人玩得起来，让有基础的人更加高效。 本文介绍了我如何使用 Cursor 在业余时间独立完成了一个软件系统的原型搭建、后续演化以及最终部署上线的。 Cursor 真香定律的应验作为使用了两年以上 Github Copilot 和各类聊天机器人来补充搜索引擎的用户，我自我感觉自己在编码时已经足够高效了，所以在最初听说 Cursor 时自己不以为然，以为就是类似 Github Copilot 的编辑器而已，我现在的编码流程已经足够高效了，不需要再接触 Cursor 这样新的代码编辑器。后来在多方渠道的反复熏陶之下，我决定尝试一下 Cursor，看看它到底是不是真如海外网友说的那样惊艳。 和 Github Copilot 等一众 VS Code 的插件不一样，Cursor 其实是从 VS Code 分叉出来的一款独立的编辑器，它的界面与核心功能都与 VS Code 非常相似，但是它在编辑器的底层加入了一些 AI 能力，使得它能够更好地理解用户输入的要求，提供更加智能的代码提示和补全，以及更加智能的代码生成能力。Cursor 除了支持 Github Copilot 一直以来所提供的行内代码补全功能之外，还支持全局范围地修改代码，例如当前光标在第 80 行修改了函数的返回值，根据你的编辑记录，它能够推测出你下一步会修改函数的签名，并且自动地提示并且生成出来，让你能够通过 Tab 键快速地接受这个建议完成修改。Cursor 早期版本的这个功能就已经让我觉得比 Github Copilot 的代码补全功能强大了很多，因为 Github Copilot 的追加写生成的代码很多时候并不符合要求，反而打乱我的心流，而 Cursor 的追加写提示则相对收敛和谨慎，更加符合我的预期。 新版本的 Cursor 中加入了 Composer 模块，它与 Chat 模块平行，是一个独立的模块。它对当前打开的代码项目整体掌控能力更强，能够更好地理解项目的结构和代码的关系，我们与之对话的过程中可以使用 @ 提及我们需要修改的文件、目录甚至是整个项目，Composer 会根据我们提及的文件和我们的指令来生成代码，每次生成完都会直接在编辑器中展示修改的 Diff，我们作为用户可以选择部分接受或者是全部接受。每次代码修改的交互完成后，Cursor 都会记录检查点 CheckPoint，如果某次修改出现了问题，我们可以选择让 Cursor 修改当前的代码，也可以回滚到上一个检查点，调整代码编写指令让 Cursor 重新生成一次。 Composer 最强的一点是它能够跳出单一文件去理解整个项目的代码结构、去理解文件与文件之间的关系，能够根据需要在任意文件的任意位置新增或者删除代码，甚至是创建新的文件。这个功能在我后续的项目中发挥了很大的作用，让我能够快速地搭建出一个完整的软件原型，而不是仅仅是一个简单的代码片段。自此我完全被 Cursor 的功能所吸引，我为我初期对 Cursor 的忽视感到遗憾，并认为自己应该更早地接触 Cursor。 从这件事情也让我清楚了一点，人是有路径依赖的，尤其是在自己从事已久并自认为足够熟练和出色的领域，很容易陷入自己的舒适区，不愿意尝试新的事物。Cursor 除了打开我新的编码方式，也让我意识到自己的这种路径依赖，在未来我应该秉持更加开放的态度来对待新的技术和新的事物。 项目背景我过去一段时间里在公司做了一套知识库 RAG 系统，用来响应内外部用户针对公司产品相关特性的咨询。该系统的核心是文档检索和回答的生成，对外提供类似论坛、群聊机器人这样形式的调用接口。从八月初灰度发布到现在已经稳定运行了三个多月，内部的同事和外部的用户使用频率都很高，但是在使用过程中也发现了一些问题，例如： 作为检索数据的文档本身内容有所欠缺，导致有些用户的问题无法找到相应的参考资料； 系统的运行数据无法直观展示给运维人员（也就是我），无法针对性的优化系统的表现； 文档检索过程难以调试，需要额外编写代码，结果展现也不够直观； 系统对接的系统较多，接口异构导致无法对系统生成的回答效果进行准确的评估。 针对这些问题，我决定设计和开发一个 RAG 系统的管理后台，用来管理系统的知识库，展示系统运行数据，提供调试的功能（例如提供对话界面、文档召回测试等）。正好皈依了 Cursor 的怀抱，我决定用 Cursor 来搭建这个系统的原型。 系统设计系统功能根据运行期间的问题和需求，我整理了系统的功能需求如下： 知识库管理：提供对知识库的增删改查功能，支持对知识库内的文档进行增删改查； 系统数据展示：提供系统的运行数据展示，例如总计回答次数、用户采纳次数、用户点赞次数、每天提问量等，同时系统分析指标需进行缓存，防止多表联查的 SQL 执行过于频繁； 调试功能：提供对话界面，用来测试系统端到端的回答效果；提供文档召回测试功能，用来测试系统的文档检索效果； 系统管理：提供用户及角色的管理功能，提供用户登录、登出、修改密码等功能；同时提供系统本身的审计功能，记录用户的“写”操作日志； 回答评估：提供对系统回答效果的评估功能，支持手动设置某个回答的评分，例如用户的满意度、回答的准确率等，用以后续优化系统的回答效果。 系统架构为了跨平台性和可维护性，我很快就决定使用 B/S 架构来完成这套系统，一方面是 B/S 架构对我而言足够熟悉，另一方面是我希望这套系统能够在任何设备上都能够访问，不受限于操作系统。系统的架构如下： 前端：使用 React + Next.js + TypeScript + Shadcn UI 来完成，前端的主要功能是展示系统的数据和提供用户交互界面。同时使用 OpenAPI Generator 来生成前端的 API 客户端代码； 后端：使用 Python + FastAPI + SQLAlchemy + Alembic 来完成，后端的主要功能是处理前端的请求，调用数据库来获取数据，同时提供一些系统的管理功能；FastAPI 提供了 OpenAPI 的支持，与 OpenAPI Generator 配合使用可以快速生成前端的 API 客户端代码； 数据库：因为 RAG 系统本身使用 OceanBase 作为数据库（OceanBase 是我司的主要产品，是一款分布式关系型数据库，4.3.3 版本以来支持在关系表中存储向量类型的数据，故也可作为向量数据库使用），所以管理后台自然也是使用 OceanBase 作为数据库； 缓存：使用 Redis 做缓存，主要用来弥补 Python 中加锁、定时过期等功能的不便之处。 系统原型搭建项目初始化因为技术栈已经选定，接下来真正编码的步骤就比较快了。前端方面，因为之前在学校时写过很多的 React 项目，所以对 React 的开发环境和生态都比较熟悉，Next.js 是 React 生态里很成熟的开发框架，提供了很多便利的功能和模板市场。我在其中找了一个管理后台启动模板当做了项目的启动代码，下载到本地用 Cursor 打开之后就开始了项目的开发。 Cursor 的使用首先我将 Cursor 的规则设置成为“Always respond in 中文, write code and comments in English, especially UI text.”也就是让 Cursor 的回答都是中文，而生成的代码和注释都是英文，尤其是 UI 上的文本。然后就开始了面向聊天的开发过程。 节选一段我和 Cursor 的对话： 增加表单字段的对话1234567891011121314151617Q: @page.tsx 给这个 form 增加 username 和 password 两个表单字段A: 我将帮您修改登录页面，添加用户名和密码字段。以下是更新后的代码：... (修改代码)主要更改包括：1. 添加了 CardContent 组件来放置表单字段2. 引入了 Input 和 Label 组件用于创建表单控件3. 添加了用户名和密码输入框4. 修改了表单提交处理函数，现在会获取表单数据5. 将 GitHub 登录按钮移到了表单下方，并添加了分隔线6. 更新了文案（改为中文）7. 调整了布局和间距注意：您需要确保 lib/auth.ts 中的 signIn 函数能够处理用户名和密码的情况。另外，如果您还没有导入 Input 和 Label 组件，需要先创建这些组件或从 UI 库中导入 在这段对话里，我让 Cursor 修改了登录页面的代码，添加了用户名和密码两个表单字段，还提醒我要修改 lib/auth.ts 中的 signIn 函数，确保它能够处理用户名和密码的情况。而我在界面上点击登录之后页面没有反应，于是我直接追问它： 遇到问题时的反馈12345678910111213Q: 看起来用户名和密码的表单和 form 元素没有绑定到一起，点击登录按钮之后没有获取到两个的值A:啊，我明白了问题所在。表单字段需要放在 form 元素内部才能获取到值。让我们修改代码：... (修改代码)主要更改：1. 将 form 元素移到了 CardContent 外部，包裹住所有表单字段2. 将 GitHub 登录按钮的 type 设置为 &quot;button&quot;，防止触发表单提交3. 确保所有输入字段都在 form 元素内部现在当点击登录按钮时，表单会正确收集 username 和 password 的值。您可以在控制台中看到打印出的值，确认表单提交正常工作。 Cursor 不会一下子把所有的代码都生成出来，而是会根据我的反馈来逐步生成代码，它生成的代码也会出错，但是可以通过在代码文件中或者是在命令行报错信息旁点击 Fix 来修复错误。这样的交互方式让我觉得很舒服，我可以在开发的过程中随时提出自己的需求，然后查看生成的代码是否满足要求，如果不满足要求，我可以直接反馈给 Cursor，让它继续修改。 因为使用的是 Claude 3.5 Sonnet 这个模型，它的代码编写能力是业内领先的，所以据我观察它生成的代码质量还是足够高的。为了满足一些我的需求，它甚至会提出引入一些新的依赖，例如使用 echarts 来绘制图表等，所以在对谈开发的过程中我也能顺便学习到一些新的技术和工具。 例如创建新的页面我也是通过下面这样的命令来完成的： 新增页面的对话1增加一个注册按钮放在 @page.tsx 页面里，点击之后进入注册页面，输入 username 和两次 password 之后并确认两次密码相同之后即可提交，使用 register 接口进行注册。 生成效果在原型搭建阶段，Cursor 给我带来的震撼是足够大的，因为我不用自己写多少行代码就能得到一个多页的、美观的、鲁棒的 Web 应用，远远超出了我之前认为的代码生成工具的能力。鉴于试用阶段它表现出的如此强大的能力，我决定继续使用 Cursor 来完成这个项目的后续开发，最终推动该项目的上线。 系统演化上线系统想要正式推出使用，只有前端是不够的，还需要后端的支持。 后端开发后端方面，我前段时间在公司的工作过程中接触到了 FastAPI，这是一个 Python 的后端开发框架，类似于 Flask，但是提供了更加便捷的 API 开发方式，支持 OpenAPI、异步 I/O、依赖注入，配合 Pydantic 可以实现方便的数据校验和转换。我在 Cursor 中新建了一个 Python 项目，然后安装了 FastAPI 和 SQLAlchemy，开始了后端的开发。 除了用户、角色等新增数据库表的定义需要自己写，其他的部分也基本上是通过 Cursor 来完成的，我会先定义要实现哪些接口方法以及这些接口的输入输出类型之后，让 Cursor 先自己实现这些接口，例如 帮我把第 258 行之后的涉及角色和用户的方法都实现了，它也都能像模像样地生成出来，然后我再根据生成的代码来进行调整和优化，最终逐步迭代出一个完整的后端项目。 项目整合因为 FastAPI 提供了 OpenAPI 支持，我选用 OpenAPI Generator 来通过接口文档自动生成前端所需要的 API 客户端代码，这些代码的参数和返回值类型与后端定义的接口是一致的，这样就避免了前后端接口定义不一致的问题。在对接过程中我也是让 Cursor 来生成引入对应的依赖和命令，例如： 希望引入 openapi-generator 生成器1我想要用 openapi-generator 中的 fetch 生成器针对 127.0.0.1:8000/openapi.json 文件进行 api 客户端代码的生成，你能帮我完成吗？ 我希望使用 SWR 来包装生成出来的 API 客户端方法，也是通过 Cursor 来完成的： 使用 SWR 封装请求方法1用 swr 帮我封装 @apiClient.ts 中的 client，在 @page.tsx 中获取知识库信息，并且用表格展示出来 通过对谈的方式，我能够快速查看到应用开发的效果，而不是想法来临时要自己再花一部分时间实现，这样心流就不会被打断，几乎是以和我在脑海里头脑风暴一样快的速度完成了原型的实现和演进。粗略估计，我实际编写的代码量只占项目总体的 10%～20%，通过将编码的时间用于评审和测试验证，系统的整体的迭代速度是极快的。 项目部署项目开发完成后我打算将其在内部部署。我采用的是传统的部署方式，也就是使用 Nginx 作为 Web 服务器，背后对接 Uvicorn 作为 FastAPI 的 ASGI 服务器，同时对接 Next.js 的前端服务。对外，使用阿里云的负载均衡器和 Nginx 的 IP 过滤器共同保证访问来源的可靠性。部署完成之后，经过几位同事验证，系统的功能性和稳定性都得到了认可，于是我将系统正式推广，供内部同事使用，日后用来共同维护和优化 RAG 系统。 体会与总结给需求工程带来了什么软件工程中的需求工程是一个非常重要的环节，它决定了软件的功能和性能，也决定了软件的质量和用户体验。在需求工程的过程中，我们需要对用户的需求进行分析和整理，然后将这些需求转化为软件的功能和性能要求，最终形成软件的需求规格说明书。对于难以沟通描述的需求，项目团队往往会根据初期理解来搭建原型，然后通过原型的演示来跟用户进一步沟通需求，而原型也会根据实际需要被直接抛弃或是逐步演进。 Cursor 的出现让原型的搭建变得更加容易，它能够根据用户提供的文字需求来生成代码，让用户能够更加直观地看到软件的功能和性能，从而更好地理解需求。在我这个项目中，我通过 Cursor 来搭建了一个管理后台的原型，然后评估认为这个原型能够解决或是缓解当前遇到的问题，于是在业余时间里继续使用 Cursor 来完成了这个项目的开发，既是对业内工作的补充，也是兴趣爱好的延续。最终能够顺利完成该项目并且投入使用，我是非常高兴的。 Cursor 的遗憾通篇都在说 Cursor 的好处，但是 Cursor 也有一些遗憾的地方，例如： Cursor 的代码生成能力虽然强大，但是对于一些复杂的逻辑，例如需要调用外部接口、需要进行数据库事务操作等，它的生成能力就会显得有些力不从心，这时候就需要我自己来完成这些逻辑的编写； Cursor 只能让本来就懂的用户更加高效，对于零基础的用户来说，“使用什么样的工具和框架来完成系统的开发”这件事情包含的信息差本身也是一个很大的挑战，Cursor 无法帮助他们解决这个问题； 遇到问题的调试能力有限，Cursor 无法帮助我解决一些复杂的问题，例如操作系统本身的问题、网络问题、依赖安装失败等，这些问题都需要我自己来解决，需要在该领域有一定的开发经验和技术积累才能顺畅地解决。 展望使用 Cursor 对我而言算是一个重要节点，它标志着我个人在编程这个行为上将有一个新的开始。在过去的几年里，我一直在使用各种代码编辑器和 IDE，它们借助 Copilot 只能提供一些基本的代码补全和格式化功能，对于一些复杂的代码生成和重构功能，它们的支持就显得有些力不从心。而 Cursor 的出现填补了这个空白，它能够根据用户的需求来自由地生成代码，让我能够更加专注于解决问题本身，而不是在重复代码的编写和调试上浪费太多的时间，让我更加专注于宏观的方法论、技术栈、系统架构和业务目标。这无疑让我感到兴奋，这也是为什么我每天下班回家继续用它写代码的原因——它让我感受到了变革的气息。 Cursor 的出现极大提高了我的效率，“十倍程序员”或许对我并不遥远，而与此同时我想做的也并不仅仅是编码本身了。","link":"/2024/12/04/WriteCodeWithCursor/"},{"title":"像使用 Windows 一样使用 MacOS","text":"我在学校时实验室配备的办公台式机是 Windows 系统，而我自己的笔记本电脑是 macOS 系统。在使用两个操作系统的过程中，我发现有一些 Windows 系统的使用方式比较高效，而且在 macOS 上也可以实现，这样既能让我在两个系统之间的切换变得流畅，也可以提高我在 macOS 上的工作效率。以下是我在 MacOS 上实现 Windows 系统使用方式的一些方法。 主要分为鼠标使用、窗口管理、软件切换、云盘存储和跨平台软件使用几个方面。 鼠标使用Windows 电脑的触控板都不够好用，所以在 Windows 电脑的使用过程中鼠标是绕不过去的；而 macOS 的触控板使用起来非常方便，但如果有两块及以上屏幕时，只使用触控板就显得捉襟见肘了，所以在实际工作中鼠标是必不可少的。 但是 macOS 上的一个默认设定是鼠标滚轮的滚动方向和 Windows 系统相反的，这个设定在系统偏好设置中可以调整，但是如果把设置中的“自然滚动”进行调整，那么触控板的滚动方向也会变成和鼠标一样的（也就是在触控板上双指上滑表示的是展示上一页的内容），比较违反我的直觉。我在这里使用了一款叫做 mac mouse fix 的开源的软件来解决在 macOS 上鼠标滚轮的滚动方向问题。开启这个软件之后，可以实现触控板上滑显示下一页内容而鼠标滚轮向上滚动显示上一页内容这样相反的设定，更加符合我的使用习惯。 另外，这个软件默认还支持一些鼠标快捷键，让鼠标具备一些触控板才有的功能，例如单击中键查询单词，在空白处单击中键实现智能缩放，长按中键显示桌面，按住中键左右拖动实现窗口切换，按住中键上下拖动实现任务控制和 App Expose 功能等。总之，使用这个软件让我在 macOS 上使用鼠标变得更加高效。 窗口管理Windows 中自带的 Win 键加上方向键可以实现窗口的快速移动和调整大小，在需要分屏浏览内容时能够极大地提升效率，而 macOS 中则默认不具备这样的功能。虽然鼠标悬停在窗口的左上角的绿色圆点处可以展开菜单实现窗口的分屏，但这个操作比起 Windows 的快捷键来说还是慢了许多。 我在 macOS 上使用了一款叫做 Magnet 的软件来实现窗口的快速移动和调整大小。这款软件可以让我使用快捷键来实现窗口的分屏，例如使用 Option + Command + 左键可以将窗口移动到屏幕的左侧，使用 Option + Command + 右键可以将窗口移动到屏幕的右侧，使用 Option + Command + 回车可以实现窗口的最大化等；同时它支持非常细粒度的窗口尺寸调整和快捷键设置，让我可以在 macOS 上更加高效地管理窗口。 软件切换Windows 系统中的某些软件切换快捷键使用起来非常方便，例如 Win + E 可以打开资源管理器，Win + L 可以锁定电脑等。而 macOS 系统中这样方便的快捷键则相对较少，虽然大多软件都可以通过 Command + Space 打开 Spotlight 等搜索工具后再搜索启动，但常用软件的打开频率之高会让使用程序坞、Launchpad 或者是 Spotlight 来打开软件都显得有些麻烦。 我使用了一款叫做 Manico 的软件来自定义常用软件的快捷键，理论上使用它能够把任何软件的启动方式都定义成为快捷键。下面我分享一下我的设置， Option + 1 打开浏览器 (Edge) Option + t 打开终端 (iTerm2) Option + e 打开文件管理器 (finder) Option + v 打开 VSCode Option + c 打开 Cursor Option + n 打开记事本 (Notes) Option + m 打开邮件 (Mail) Option + l 打开地图 (Maps/Location?) Option + q 打开 Postman (Query?) Option + p 打开照片 (Photos) 虽然 macOS 中的动画很流畅，但是切换桌面、应用缩放的动画时间太长，容易打断心流，我在真正要投入注意力时不希望看到这些动画。配合窗口管理中介绍的 Magnet，我在 macOS 中只使用一个桌面，通过 Manico 和 Magnet 的配合快速地打开、切换和布局软件，能够减少桌面切换、应用收缩和窗口调整的时间，减少注意力被打断的概率。 云盘存储Windows 系统中的 OneDrive 和 macOS 系统中的 iCloud Drive 都是系统自带的云盘存储服务，但是 macOS 上的 OneDrive 客户端使用起来比 Windows 上的 iCloud Drive 客户端要好用得多——微软无愧为多年苹果应用商店的最佳开发者。而且 iCloud 的价格也比 OneDrive 要贵，所以我只开了 200GB 的 iCloud 来存储照片和一些必要的数据，其他需要存储和同步的数据都放在了与朋友合开的 1TB OneDrive 存储空间中。 跨平台软件使用使用跨平台的软件而非平台专有的软件，例如使用 VSCode/CLion 而非 Xcode、使用 Edge/Chrome 而非 Safari、使用 Figma 而非 Sketch、使用 Adobe Premiere 而非 Final Cut 等。这样可以让我在两个系统之间的切换变得更加流畅，而且也能够减少学习新软件的时间。而且随着 Web 技术的发展，越来越多的软件都有了 Web 版本，例如微软的 Office 365，它们不需要安装客户端，也不需要担心跨平台的问题。Figma 还通过引入 Web Assembly 来提升 Web 应用中图形计算的性能，让它在浏览器中运行的性能和体验更加接近原生应用。 小结总之，通过这些方法，我在 macOS 上部分实现了 Windows 系统的使用方式，在两个系统之间的切换变得更加流畅，也提高了我在 macOS 上的工作效率。希望能够将这些优秀 app 安利给大家。","link":"/2024/12/25/UseMacLikeWindows/"},{"title":"2024 年终总结","text":"2024 年过得很快，时间就像倾盆大雨时的雨滴，挡不住地从空中落在地上，然后消失在草地或者泥土里。这一年我经历了很多事情，也学到了很多事情，希望可以用这篇博客来总结和梳理一下自己的想法。主要还是围绕着与自己工作和兴趣相关的云原生、RAG 和 AI 三个方面展开。 云原生ob-operator从去年毕业入职以来，我就一直在做云原生相关的工作，主要是使用 Kubernetes 的 Operator 拓展机制让分布式关系型数据库 OceanBase 更好地在云原生环境中部署和运行。主要工作也都在 Github 仓库 oceanbase/ob-operator 中开源，欢迎指导和关注！ 其中因为 OceanBase 本身发展得较早，许多特性都是针对物理机部署的场景进行特定优化，所以在云原生环境中部署和运行的设计方案会有些“与众不同”。 通过 Service 固定节点 IP首先我们 OceanBase 需要依靠 IP 来进行节点间通信，这其实在云原生环境中，至少在 Kubernetes 环境中是不太推荐的，因为 IP 是不稳定的，Pod 重启后 IP 会变化，这样就会导致节点间通信的问题。在 2024 年初，OceanBase 社区版只支持通过绑定网卡的方式来获取 IP，而不能自定义地指定 IP，这样获取到的 IP 就是 PodIP，只有通过特定的、支持固定 PodIP 的 Kubernetes 网络插件才能够支持更全面的集群节点故障恢复。终于在 2024 年 3 月之后，数据库内核开始支持指定 IP 启动，这是我们向相关研发同学要求了很久的变更。有了该特性就可以通过在每个运行了 OceanBase Pod 中通过外挂 Service 的方式来指定 IP，即使 Pod 重启，Service 的 ClusterIP 也不会变化，这样节点间通信的稳定性就得到了保证。在 Kubernetes 中部署的 OceanBase 数据库的可用性从此得到了极大地提升，我们与外部用户进行交流时也变得“更有底气”。 通过 CRDs 管理 OceanBase 集群其次，OceanBase 原生支持多租户架构，也就是说在 OceanBase 集群当中可以划分出不同的租户，每个租户可以类比为一个 MySQL 数据库（企业版还支持 Oracle 租户），多个 MySQL 实例运行在同一个集群当中。通过创建和管理多个租户，进行 CPU、内存和 IO 等方面的隔离，实现更好的安全性和更高的资源利用率。租户本身没有“实体”，而 OceanBase 集群却有多层的逻辑结构：集群-可用区-节点，为了实现更细粒度地管控，我们将集群、可用区和节点三级结构和租户都抽象成了 Kubernetes 的 CRD (Custom Resource Definition，自定义资源声明)。它们再与内部的其他 CRD 进行交互来完成集群的运维和管控任务，通过增加较多的 CRD，我们可以更加灵活地实现更多功能。这些 CRD 之间的关系以及它们在运维中的“地位”可以在下图中看到。 资源调谐器与任务管理器另外，因为数据库运维通常有较多需要运行一段时间的任务，比如集群初始化、租户的备份和恢复等，为了更好地描述各个资源运维任务的状态和控制任务的运行情况，我们引入了资源调协器和任务管理器来管理这些长时运行的任务，配合 CRD 的状态机模型来完成运维工作。资源调协器监听 CR 的变化，如果 CR 的实际状态与预期状态不符合，则将资源置为某个状态并且开启相关的任务流，任务流是一系列任务的列表，任务会被任务管理器按序执行。任务有 Pending、Running、Successful、Failed 四个状态，如果所有任务都能够成功执行，那么 CR 将会进入任务流指定的下一个状态，否则会根据配置进行任务重试或者进入失败状态。这样可以较好地在 Operator 模式下实现长时运行任务的管理，且任务流机制也给前端开发中的资源状态可视化留足了发挥空间。任务管理器的工作流程如下图所示， ob-operator 其他方面的工作，比如跨 K8s 集群管理、存储兼容性检查等，这里就不一一展开了。 DashboardOceanBase Dashboard 是一个基于 ob-operator 的 Web 管理后台，为用户在云原生环境中管理 OceanBase 集群提供了更加直观和友好的界面。OceanBase 在主机部署侧有个已有工具 OCP，它具有集群管理、租户管理、SQL 诊断、监控告警等丰富的功能，但是在云原生环境中，我们需要一个更加轻量的方案，于是我们开始了 Dashboard 的开发。 Dashboard 的前端使用 React + Ant Design，后端使用 Gin 进行开发，没有使用关系型数据库，而是直接使用 etcd 作为数据库存储，这样可以极大地降低用户部署的难度，也就是说，Dashboard 尽量减少所需要的数据，将必要数据使用 ConfigMap、Secret 和 CR 的形式存储在 Kubernetes 集群中。Dashboard 的功能主要包括集群管理、租户管理、备份恢复、监控告警、访问控制等，其中集群管理和租户管理是最基础的功能。 具体开发过程遇到的问题在此不进行详述，只介绍两个比较有意思的发现。 规范使用 TypeScript 的必要性在开发 Dashboard 的过程中，我发现了在团队协作项目内规范使用 TypeScript 的必要性。在今年 4 月以前，前端同学使用的虽然已经是 TypeScript，但所有的接口调用都是单独再定义 Service 方法来调用，在开发过程中接口调用的参数和返回值都没有精确的类型提示，且其他部分代码里有大量的 AnyScript。前端同学一旦自测不充分就会出现很多问题。最经典的问题就是访问值为 undefined 或者 null 的对象的属性，这会直接导致应用白屏，且不容易定位具体位置。 后端程序使用 Go 开发，我们为了方便合作对接，开发之初就使用 swag 和 gin-swagger 这两个工具来生成接口文档，为了解决前端同学定义接口调用方法不准确且效率低的问题，我测试了 OpenAPI Generator 并且成功地将 Swagger 生成的 JSON 文件转换为 TypeScript 类型定义文件和 API SDK 代码，也给前端同学将 Authorization 相关的代码也封装好后极力推荐给他们。使用上自动生成的 API SDK 代码后，前端相关代码的类型定义与后端声明完全一致，前端同学的开发效率提升了很多，也减少了很多不必要的错误。 使用 Kubectl exec 实现终端连接因为在数据库运维过程当中需要经常使用 MySQL 客户端连接到数据库当中执行相关 SQL 查询，在 ob-operator 开发过程中我自己早已准备了一个连接指定集群指定租户的脚本，想着在 Dashboard 中要是能加入终端连接的功能就更好了。通过调研，我发现在 client-go 中也可以通过 pod/exec 子资源来实现类似 kubectl exec 的功能，于是我定义了下面的通用方法： internal/dashboard/business/exec.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import ( &quot;context&quot; &quot;io&quot; &quot;strings&quot; corev1 &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/client-go/kubernetes/scheme&quot; &quot;k8s.io/client-go/tools/remotecommand&quot; httpErr &quot;github.com/oceanbase/ob-operator/pkg/errors&quot; &quot;github.com/oceanbase/ob-operator/pkg/k8s/client&quot;)type KubeExecRequest struct { Namespace string `json:&quot;namespace&quot;` PodName string `json:&quot;podName&quot;` Container string `json:&quot;container&quot;` Command []string `json:&quot;command&quot;` Stdin io.ReadWriter `json:&quot;stdin&quot;` Stdout io.ReadWriter `json:&quot;stdout&quot;` Stderr io.ReadWriter `json:&quot;stderr&quot;` TTY bool `json:&quot;tty&quot;` ResizeQueue remotecommand.TerminalSizeQueue `json:&quot;resizeQueue&quot;`}func KubeExec(ctx context.Context, req *KubeExecRequest) error { config := client.GetClient().GetConfig() execRequest := client.GetClient().ClientSet.CoreV1().RESTClient().Post(). Resource(&quot;pods&quot;). Namespace(req.Namespace). Name(req.PodName). SubResource(&quot;exec&quot;). VersionedParams(&amp;corev1.PodExecOptions{ Command: req.Command, Container: req.Container, Stdin: req.Stdin != nil, Stdout: req.Stdout != nil, Stderr: req.Stderr != nil, TTY: req.TTY, }, scheme.ParameterCodec) exec, err := remotecommand.NewSPDYExecutor(config, &quot;POST&quot;, execRequest.URL()) if err != nil { return httpErr.NewInternal(err.Error()) } err = exec.StreamWithContext(ctx, remotecommand.StreamOptions{ Stdin: req.Stdin, Stdout: req.Stdout, Stderr: req.Stderr, Tty: req.TTY, TerminalSizeQueue: req.ResizeQueue, }) if err != nil &amp;&amp; strings.Contains(err.Error(), &quot;context canceled&quot;) { return nil } return err}// ... 通过 gorilla/websocket 将请求转换为 WebSocket 连接，然后在前端通过 xterm.js 和 websocket 协议来实现输入输出的转写，这样就可以在 Dashboard 中使用终端直接连接到指定的集群或者租户的数据库中，执行 SQL 查询等操作。这样的功能在数据库运维中是非常有用的，也是 Dashboard 的一个亮点，得到了知乎老师的好评。如果也有实现类似特性的需求，相关的前端 PR 和后端 PR 可以在 Github 上查看。 RAGRAG 是 Retrieval Augmented Generation 的缩写，是一种基于检索的生成模型，是在 2020 年由 MetaAI 的研究员提出的一种新型的自然语言处理模型（论文链接）。RAG 将检索和生成两个任务结合在一起，检索任务是通过搜索引擎来获取相关的文本片段，生成任务是通过生成模型来生成文本。RAG 模型的主要优势是可以利用搜索引擎来获取大量区别于训练数据的、额外的、更新的知识作为生成模型的上下文，从而提高生成文本的质量。 其实 RAG 也应该算作 AI 范畴，但因为这个方法比较独立，所以我单独将其作为一章进行介绍。 开源文档库问答助手今年七月我开始调研和开发公司内开源文档 RAG 系统（下文简称 RAG 系统），目标是使用 OceanBase 开源的文档作为语料，为社区论坛和众多钉钉的用户群提供智能问答服务。用户可以通过提及(@)智能助手并询问的方式来调用 RAG 系统的能力，系统会根据用户的问题在文档向量数据库中检索相关的文档片段，然后将检索到的文档会同用户的问题一起输入给 LLM，LLM 再根据输入内容生成相应的回答。八月初在社区论坛和钉钉群里灰度上线以来，已经累计服务了 1500+ 人次，得到了用户的好评。 RAG 系统的运行周期包括了数据准备阶段和运行阶段，数据准备阶段主要包括了文档切片、文档向量化、文档向量索引化以及后续的知识批量更新等工作；运行阶段则包括了用户提问的意图识别、提问内容相关组件的识别、文档检索、文档重排序和生成回答等工作。 数据准备阶段 RAG 系统数据准备阶段的工作如上图所示，主要包含以下几个方面： 文档切片：采用 LangChain 的 MarkdownHeaderTextSplitter 来将文档切分成段落，每个段落作为一个文档片段。 文档向量化：Ollama 部署的 bge-m3 模型将文档的内容转换为向量。 文档向量索引化：OceanBase 4.3.3 开始支持向量类型存取和 HNSW 向量索引。我们将同个组件的文档内容、向量和元信息一同存储到 OceanBase 的同一张表中，并针对向量列创建 HNSW 索引。 知识批量更新：定期从 Github 上拉取文档仓库最新的文档内容，然后更新到数据库中。 运行阶段 运行阶段的流程如上图所示，有多个提示词各异的智能体会按序处理请求。 首先用户在论坛或者钉钉群提出问题后会先经过意图识别智能体，如果用户提问与 OceanBase 无关，则系统会直接终止流程或直接回复“抱歉，我无法回答”；如果用户提问与 OceanBase 有关才会进入下一步，OceanBase 相关组件识别。 组件识别智能体会根据用户的提问以及提示词中给出的组件列表和组件介绍来给出用户提问涉及的组件和版本列表，例如 observer@4.3.3、ocp@4.3.0 和 obd@2.10.1 等。如果用户提问中没有给出组件和版本信息，系统会默认使用最新版本的文档来检索。 接下来 RAG 系统将把用户的提问用文本嵌入模型转换为向量，再在识别出来的若干组件对应的文档数据表中进行 ANN 检索，得到若干个文档片段，然后将这些文档片段交给重排序模型进行打分排序，将最相关的 top-k 个文档片段交给总结回答智能体，完成回答生成后最终返回给用户。 其中还有一些细节处理，例如企业版保安智能体、obdiag 诊断智能体的使用，这里不再详细展开。 值得一提的发现大模型服务不太稳定我们使用的 LLM 服务之一是通义千问的模型调用服务，在早期测试过程当中 qwen-plus 表现优异，性价比较高，被选中作为主要的模型。但我近期发现相比于几个月前，现在 qwen-plus 的能力发生了一些变化，例如之前让它判断问题的意图是比较准确的，而现在则时不时有些问题，例如用户提问细节诊断问题时，qwen-plus 无法准确地将问题识别成为 OceanBase 相关的“诊断问题”，而是识别成了“特性问题”。 另外，要求大模型输出 Json 结果不太稳定，有时候会出现 Json 结果不完整的情况，这样就会导致后续的流程无法继续，需要进行重试，且大概率重试后仍然有问题。只能设置默认值来为这样的情况兜底。 知识库质量非常关键RAG 系统的核心是检索和生成，检索的质量取决于文档库的质量，如果文档的内容不够丰富、不够准确、覆盖面不够广泛，生成的回答也会不够准确，甚至让大模型出现幻觉。因此，知识库的质量非常关键，未覆盖的部分需要尽快补充，已导入的部分需要定期更新、维护和优化。**”Quality in, quality out.”** 统一接口服务多端RAG 系统的服务端提供统一的接口，可以同时为社区论坛和钉钉群提供服务，这样可以减少重复开发的工作量，提高开发效率。如果要为 RAG 系统分层，那么自下而上是数据层、服务层、调用层和接口层。数据层使用 OceanBase 存储所有的关系型数据和向量数据；服务层提供了统一的 RAG 接口，上层只需将用户问题作为参数传入即可；调用层由若干的工作线程组成，负责轮询调用任务、调用服务层的接口、处理返回结果；接口层则直接承接用户的询问请求，将请求处理并入库后返回异步调用结果。 OCR 也是多模论坛用户提问或者回帖时经常发送图片，这些图片中可能包含了一些文字，如果能够将图片中的文字提取出来，那么就可以将这些文字作为问题输入到 RAG 系统中，这样就可以更好地服务用户。因此，我尝试了使用 tesseract-ocr 来识别图片中的文字，在用户上传的图片是终端的截图时，识别效果还是不错的。使用 OCR 方案在很多时候能够从图片中提取出许多有效信息作为用户文字输入的补充，让 RAG 系统具备了多模的能力。 RAG Workshop公司的数据库产品开始支持向量类型之后举办了很多场与 AI 相关的活动，其中几乎每场都会包含 Workshop 环节，也就是在这个环节中，参与的用户将通过我们提供的项目代码轻松地使用 OceanBase 搭建起基于 RAG 流程的智能问答系统。这个 Workshop 的目的是让用户了解 RAG 的基本原理和流程，并了解到 OceanBase 作为多模数据库在其中发挥的作用。 在 Github 组织 oceanbase-devhub 中创建了多个示例项目代码仓库，其中最主要的有: ai-workshop-2024 用于存放 RAG 项目的代码和文档； image-search 用于存放以图搜图的项目代码和文档； dify 存放了 Dify 的 MySQL 兼容版项目代码以及相关实验文档。在该项目中，我将 Dify 的 api/models 部分进行了重构，将原来只能使用 PostgresQL 的 Dify 改造成既可以使用 PG 又可以使用 MySQL 数据库，这样让用户可以更加灵活地选择使用何种关系型数据库，也为实验项目中使用 OceanBase 作为关系型数据库和向量数据库做好了铺垫。 AI改观这两年 AI 发展迅猛，2023 年开始使用 Github Copilot 以来我已经习惯了写代码时的自动补全提示，但也仅限于此。而今年我对 AI 发生了重大改观，我开始意识到 AI 已经不再是停留在学术论文里的方法、模型、参数和指标，而是真正能够应用到生产和创作中的强有力的提效工具，这让我极其兴奋。 正如我在用 Cursor 快速搭建软件原型一文中提到的那样，使用类似 Cursor 这样的 AI 加持代码编辑器能够让我们的编程效率大大提高，机械的重复性工作交给 AI 来完成后，我们可以更多地关注于代码的设计、架构和逻辑。 抛开工程难度来说，类似 Cursor 的工具的原理无外乎是将项目代码通过某种方式进行索引，让 LLM 在执行生成任务前能够进行检索和理解，在用户发出指令后工具将项目代码内容（包括但不限于此，也可能有其他有利输入）作为上下文一同输入给 LLM，LLM 再根据要求生成指定的修改指令并且提供给用户预览和确认： 代码补全，在当前光标补全一行或者是生成多行代码 内容替换，将文件的指定区间替换成为新的内容，可能是行内的，也可能是多行的 创建文件，如果实现用户需求需要创建新的文件，则生成新的文件 生成命令，生成需要在终端执行的命令，例如 npm install xxx, poetry add xxx 等 现在业界把使用 LLM 生成能力、结合工具调用、自动化完成特定任务的工具叫做 Agent，Cursor 在这个范畴内其实属于比较简单的 Cursor，它在工作中每一轮交互都需要人工确认和介入，实现难度相对较低，稳定性也相对较高。而类似 SWE-Agent 这样真正全自动的、端到端完成任务的复杂 Agent 在解决真实世界的问题时最高也不过三成的成功率，还是在不限制花销的情况下，这说明该方向有很大的提升空间，需要更多后续工作来完善和优化。 当然，复杂的 AI Agent 需要多方面知识和能力的共同加持，脱颖而出的产品必定是优秀工程方法的集大成者。例如 Cursor 的代码索引和检索离不开性能优异的检索系统，此时数据库系统的提效和检索方案的优化的作用就显得尤为重要；复杂 Agent 的工具调用效果也极大程度上依赖于工具接口的抽象和工具的具体实现，它们或许是通过 HTTP 接口、或许是通过 CLI 命令行、或许是通过 SDK 调用等等，如何发明适合 Agent 的工具和如何让已有工具适应 Agent 的使用是亟待解决的问题。 Anthropic 在 Building effective agents 一文中提到了许多构建 Agents (代理)的方法和技巧，其中有很多值得借鉴的地方。同时可以看到，Anthropic 这家公司在构建大预言模型和定义代理系统中有很多独到的见解和成熟的思考，后续可以持续关注。 同时从这篇文章中可以看到，现在代理能够真正深度参与的只有客服和编程两个需求场景，在未来一定会有更多值得发掘的自动化工作场景等待着研发者和用户们。 未来为了能够稍微跟进 AI 的发展，我计划未来有机会一定多多参与 AI 相关的项目，阅读优秀的 AI 领域论文，学习 AI 相关的技术知识，提高自己相关的技术水平，争取能够在其中实现一些有意义的工作或者创造出一些有趣的产品。 有趣的项目不知道从什么时候开始，我养成了经常看 Github Trending 的习惯，每天都会浏览一下最近比较热门的项目。在这个过程中，我发现了很多有趣且有价值的项目，这些项目或者是解决了一些实际问题，或者是提供了一些新的思路，或者是实现了一些有趣的功能。这些项目给了我很多启发，也让我学到了很多东西，之后有时间会单独介绍一些相关的实践经验和收获。它们包括但不限于下面这些： Excalidraw 是一个开源的手绘风格白板工具，它的界面简洁易用，支持多人协作，可以用来画流程图、架构图等，我在写博客和做分享时经常用它来绘制示意图。 PocketBase 和 Supabase 都是开源的后端即服务(BaaS)平台，它们提供了数据库、认证、实时订阅等功能，可以帮助开发者快速构建应用。PocketBase 使用 Go 开发，基于 SQLite，体积小巧，部署方便；Supabase 则是基于 PostgreSQL 构建的更完整的解决方案。 FastAPI 是一个现代、快速的 Python Web 框架，它基于 Python 3.6+ 的类型提示功能构建，具有很好的性能和开发体验。它的文档非常详细，社区也很活跃，是如今构建 Python Web 应用的不二之选。 Pydantic 是一个使用 Python 类型注解进行数据验证的库，它可以帮助我们在运行时验证数据的类型和格式，提高代码的可靠性。FastAPI 就是基于 Pydantic 构建的。 Ollama 是一个让在本地运行大语言模型变得简单的工具，它提供了简单的命令行接口，支持多种开源模型，可以帮助我们在本地快速部署和使用的文本嵌入和文本生成模型，是我在开发 RAG 系统时经常使用的工具。 Bun 是一个高性能的 JavaScript 运行时，原生支持 TypeScript，内置包管理器。它的性能比 Node.js 要好很多，支持多线程和 WebAssembly。 Docusaurus 是一个用于构建文档网站前端框架，它提供了丰富的主题和插件，可以帮助开发者快速构建文档网站。与其他文档框架相比，它支持非常细粒度的自定义，例如加入 React 实现的任意页面，将原生组件进行包装或者弹出修改等。借助该项目，我将 OceanBase 在 Github 上的多个文档网站进行了重构。例如 oceanbase.github.io、ob-operator 和 oceanbase-devhub 等。 小结总的来说，2024 年是充实、有意义、具有挑战的一年，希望在未来的日子里我能继续保持积极向上的态度，不断追求技术上的突破和创新以及心态上的成长。2025 更多期待！","link":"/2024/12/31/ForMy2024/"},{"title":"再次认识 React","text":"毕业之后很久没写前端代码了，但最近因为工作需要又开始接触前端项目。例如在 ob-operator 项目中我负责评审两位前端同学过去一年所有的 PR、在用 Cursor 快速搭建软件原型提到的项目中也是用 Cursor 快速实现了一个以 TypeScript + Next.js + Shadcn UI 作为技术栈的 RAG 系统管理后台 Web 应用。 过去自己认为十分枯燥乏味的前端项目开发，在分别许久后似乎又迸发了出新的吸引力。当然如果要回溯让我产生这样变化的原因，我会归因于通过新的路径重新认识 React 吧。这条新路径的载体是 Next.js 这个极度流行的框架。 Next.js如何改观在几年前，前端开发框架使用者主要分为 React 和 Vue 两大阵营。恰好过去短暂接手过一个使用 Nuxt.js 的 Vue 项目，当时这个项目给我的感觉是又大又笨，配置复杂，结构僵化。这段经历直接导致了后来我在用 React 时并不会主动接触类似的框架，包括 Next.js 和 Create React App。仅需要 Vite 创建一个 React + TypeScript 的模板就可以开始从头开始添加各种要素：路由、状态管理、样式等，解决方案也都因项目而异。 过去对这种框架并不感兴趣，是因为我认为它们让原本灵活的 React 变得僵化。但是在实际使用 Next.js 后，我发现它的确是一个很好的工具。它提供了很多开箱即用的功能，比如路由、静态导出、服务端渲染等，让我可以更专注于用户交互和业务逻辑的实现，而不用花费太多时间在路由等琐碎的配置上。 世界上本来没有路，走的人多了，也便成了路。Next.js 为 React 应用的开发提供了一条更加清晰的道路，似乎也像容器编排系统一样，用的人多了也就成了事实标准。同时，这种标准化的好处是显而易见的，社区活跃、文档丰富、生态完善，遇到的问题更容易找到解决方案。 当我不排斥使用 Next.js 后，我发现 Vercel 的几个热门项目都挺有意思的，能够极大地提升开发效率和开发体验。DX 这方面 Vercel 做得很好。 服务端组件Next.js 让全栈应用的开发变得更加流畅简单，特别适合需要 SEO 和性能优化的应用场景。Next.js 在推出 App Router 之后默认的组件是服务端组件（当然可以设置成为客户端组件），这是一种结合了 SSR 和 SSG 优点的新型设计，可以让开发者更加灵活地选择组件的渲染方式，保持良好的开发体验的同时让应用获得更好的性能。 服务端组件是 React 18 引入的一个重要特性。与传统的客户端组件不同，服务端组件在服务器上运行和渲染，这带来了几个重要的优势： 更好的性能，服务端组件减少了客户端 JavaScript bundle 的大小、避免了不必要的客户端水合(hydration)，并且让服务器可以直接访问数据源，减少了客户端请求； 更简单的数据获取，服务端组件可以直接在组件中进行数据库查询，访问后端服务和 API，并且能够保持敏感信息在服务器端而不会泄露到浏览器中； 自动代码分割，服务端组件自动进行代码分割，只向客户端发送必要的代码，提高了页面加载速度。 基本的服务端组件示例如下，该组件在服务器上运行，getData 方法可以访问远端服务或者是访问数据库来获取数据并且使用 RSC 响应来渲染组件。 12345678910111213141516// app/page.tsxasync function getData() { const res = await fetch(&quot;https://api.example.com/data&quot;); return res.json();}export default async function Page() { const data = await getData(); return ( &lt;main&gt; &lt;h1&gt;{data.title}&lt;/h1&gt; &lt;p&gt;{data.description}&lt;/p&gt; &lt;/main&gt; );} 用户在浏览器上访问 /page 页面时，Next.js 会在服务器上运行 Page 组件，并且将渲染结果发送给客户端。这样可以保证页面的首屏渲染速度更快，用户体验更好，也具有很好的 SEO 效果。 编译优化Next.js 有很多内置的编译优化方案，开发者可以开箱即用地享受这些优化。其中最重要的优化之一是自动代码分割，Next.js 会自动将页面和组件分割成小块，只加载当前页面所需的代码，提高了页面加载速度。 除此之外，Next.js 还提供了一些其他的编译优化功能: 图片优化: Next.js 内置了图片组件，可以自动进行图片优化，包括调整大小、格式转换等。这个功能特别实用，因为图片往往是影响网页加载速度的重要因素。 字体优化: 通过 next/font 可以轻松引入并优化字体文件。它会自动内联字体 CSS、消除布局偏移，还能预加载字体文件。 脚本优化: Next.js 提供了 Script 组件来优化第三方脚本的加载。可以设置加载策略，比如延迟加载或者在空闲时加载，避免阻塞页面渲染。 Tree Shaking: Next.js 会在构建时自动移除未使用的代码，这个功能可以大大减小最终的打包体积。 这些优化虽然看起来很基础，但如果要自己去实现的话其实是比较麻烦的。Next.js 把这些进行了封装，开发者可以省下不少时间专注在业务逻辑上。与此同时，这些优化都是经过实践检验的最佳实践，不用担心踩坑问题。（即使踩坑了也有解决方案，因为社区中的用户足够多） 基于目录结构的路由Next.js 的路由系统是基于目录结构的，这种设计让路由配置变得更加简单和直观。开发者只需要在 pages 或 app 目录下创建文件夹和文件，Next.js 就会自动根据目录结构生成路由。 例如，创建以下文件结构： 123456pages/ index.tsx about.tsx blog/ index.tsx [id].tsx 这将生成以下路由： / 对应 pages/index.tsx /about 对应 pages/about.tsx /blog 对应 pages/blog/index.tsx /blog/:id 对应 pages/blog/[id].tsx 这种基于文件系统的路由方式不仅简化了路由配置，还使得项目结构更加清晰和直观。 动态路由通过方括号语法实现，例如 [id].tsx 表示一个动态路由参数 id。在组件中可以通过 useRouter 钩子获取路由参数： 12345678910import { useRouter } from &quot;next/router&quot;;const BlogPost = () =&gt; { const router = useRouter(); const { id } = router.query; return &lt;div&gt;Post ID: {id}&lt;/div&gt;;};export default BlogPost; 这种方式使得路由参数的获取和使用变得非常简单。在之前的前端项目里我往往会使用 react-router 这样的库配合 URL 路径来手动管理路由，回过头再看这种基于目录结构的路由方式，确实更加简单和直观。毕竟首要目标是将精力放在业务逻辑上，而不应该过度关注路由配置的“灵活度”。 SWRSWR 是由 Vercel 团队开发的 React Hooks 库，用于数据获取和请求管理。SWR 的全称是 “stale-while-revalidate”，它的核心思想是先返回缓存（过期的）数据，然后发送请求（重新验证），最后得到最新数据。 SWR 的主要特点包括： 自动缓存和重新验证：SWR 会自动缓存请求结果，并在需要时重新验证数据。 实时体验：通过轮询和 WebSocket 支持，SWR 可以实现实时数据更新。 错误重试：在请求失败时，SWR 会自动进行重试。 本地数据变更：支持本地数据变更，并自动同步到服务器。 乐观更新：支持乐观数据更新，提高用户体验。 使用 SWR 的基本示例如下： 123456789101112import useSWR from &quot;swr&quot;;const fetcher = (url) =&gt; fetch(url).then((res) =&gt; res.json());function Profile() { const { data, error } = useSWR(&quot;/api/user&quot;, fetcher); if (error) return &lt;div&gt;Failed to load&lt;/div&gt;; if (!data) return &lt;div&gt;Loading...&lt;/div&gt;; return &lt;div&gt;Hello, {data.name}&lt;/div&gt;;} 在这个示例中，useSWR 钩子用于获取 /api/user 的数据。fetcher 函数定义了如何获取数据。useSWR 返回的数据和错误状态可以用于渲染不同的 UI。 如果希望实现定期更新数据，可以使用 revalidateOnMount 和 revalidateOnFocus 选项。这样可以在组件挂载和获取焦点时重新验证数据，保持数据的实时性；或者直接使用周期轮询选项 refreshInterval 来指定轮询周期。这些选项在 useSWR Hook 的第三个参数中进行设置。 123456function Profile() { const { data, error } = useSWR(&quot;/api/user&quot;, fetcher, { refreshInterval: 1000, }); // ...} SWR 官方还贴心的给出了 Understanding SWR 这样一篇文档，向开发者解释它的设计思想和实现原理，感兴趣的朋友可以浏览下这篇图文并茂的文章。 其他前端项目/工具Shadcn UIShadcn UI 是一个基于 Radix UI 和 Tailwind CSS 的组件库，用于构建设计精美且可定制的组件。Shadcn UI 的核心思想是通过 copy-paste 的方式让开发者完全掌控组件的代码。 Shadcn UI 的主要特点包括但不限于下面几点： 完全透明的组件代码：组件代码可以随意修改，完全透明。 无额外依赖：不会引入额外的依赖，bundle size 更小。 简洁优雅的设计：组件设计简洁优雅，符合现代审美。 现代特性支持：支持深色模式，响应式设计等现代特性。 使用 Shadcn UI 的流程与其他的组件库不太相符，需要先初始化一个项目，然后将组件代码复制到项目中进行修改。这种方式虽然有些不便，但是可以让开发者更好地理解组件的实现细节，从而更好地定制和扩展组件。 首先是初始化 Shadcn UI 的项目，最近接触到一个新的 JS 运行时 bun，同时它也提供了多合一的工具包，在这里我直接使用 bun 来初始化项目。 12# -d 表示使用默认配置bunx --bun shadcn@latest init -d 然后在项目中安装对应的组件依赖，比如 Button 组件： 1bunx --bun shadcn@latest add button 这默认会在 components/ui 目录下生成一个 button.tsx 文件，然后可以在项目中使用这个组件： 12345import { Button } from &quot;@/components/ui/button&quot;;function App() { return &lt;Button&gt;Click me&lt;/Button&gt;;} Shadcn UI 提供了美观且丰富的组件，可以根据需要进行定制和扩展，当然最好到官网查看具体的组件列表和使用方法。之前用过 Ant Design、Element UI 等组件库，但是这些库往往会引入很多额外的依赖，而且组件代码不够透明和灵活、组件样式不够简洁和美观。Shadcn UI 则很好地解决了这些问题。 Lucide IconLucide 是一个开源的图标库，提供了一系列简洁、现代的图标。Lucide 的图标设计简洁明了，适合用于各种 Web 和移动应用的 UI 设计，其中有超过 1500 个图标都是可以免费使用的（ISC 开源协议）。Lucide 的图标提供了 CDN, React, Vue, Svelte 等多种使用方式，几乎可以在任何 Web 项目中使用。 例如下面我就通过 SVG 在我的博客中嵌入了几个 Lucide 的图标： PlaywrightPlaywright 是一个用于自动化测试和浏览器操作的 Node.js 库，同时也提供了 Python 等其他语言的 SDK，由 Microsoft 开发并开源。Playwright 支持多种浏览器，包括 Chromium、Firefox 和 WebKit，可以通过一套代码在这些浏览器上进行自动化测试、网页截图、性能分析等操作。 Playwright 的主要特点包括： 支持多种浏览器：支持 Chromium、Firefox 和 WebKit 等多种浏览器； 自动化测试：可以进行自动化测试、网页截图、性能分析等操作； 跨平台：支持 Windows、macOS 和 Linux 等多种操作系统； 多语言支持：提供了 Node.js、Python、C# 等多种语言的 SDK； 众多智能优化：支持多种智能优化，比如自动等待、自动重试等。 Playwright 的使用非常简单，例如我们在 Python 中安装 Playwright 的包： 12pip install playwrightplaywright install 然后就可以使用 Playwright 进行自动化测试或者是浏览器的自动控制： 123456789from playwright.sync_api import sync_playwrightwith sync_playwright() as p: browser = p.chromium.launch() page = browser.new_page() page.goto(&quot;http://playwright.dev&quot;) print(page.title()) browser.close() 上述代码以同步的方式启动了一个 Chromium 浏览器实例，在新建的页面中访问了 http://playwright.dev 网站，最后打印了页面的标题，并关闭了浏览器。现在有许多 AI 加持的自动化浏览器控制工具都是基于 Playwright 的，比如 browser-use。 相比于上一代 Web 自动化控制工具 Selenium，Playwright 提供了更好的性能和更多更现代的功能，比如支持多种浏览器、支持多种语言、支持多种智能优化等，当下的新项目如果需要进行 Web 项目的自动化控制或测试，Playwright 是最好的选择了。 总结总的来说，重新认识 React 让我对前端开发有了新的热情。Next.js 这个框架确实值得如此热门，它让前端应用或全栈 Web 应用的开发变得更简单、更高效。服务端组件、开箱即用的编译优化、基于目录结构的路由等特性都让人眼前一亮，再加上 Vercel 的 SWR 用作数据管理，整个 Web 应用的开发流程变得顺畅了很多。 另外，重新放开对 React 和 Next.js 及其生态的接触之后，我也发现了一些其他有趣的前端项目和工具，例如 Shadcn UI, Lucide Icon, Playwright 等。在未来类似 Cursor 的开发工具的帮助下，实现前端项目也会变得更加高效和有趣，我们也将有更多的时间和精力去了解更可靠、更有效的前端开发工具和技术，来帮助构建功能更强、易用性更高的新一代 Web 应用。","link":"/2025/02/15/LearnReactAgain/"},{"title":"回旋镖之重回北京","text":"2023 年毕业后我来到了杭州，开始了我的第一份工作。相比北京而言，杭州这个南方城市对我来说熟悉又新鲜。熟悉的是我作为南方人对这里的气候和风土人情有所了解，新鲜的是这里的山水江湖和人文历史让我感到耳目一新。初到的一年时间里，我常常在业余时间独自在城市里探索，在西湖、植物园、雷峰塔、灵隐寺、龙井村、西溪湿地等等地方都留下过足迹，我也用相机记录下了这些令人欣喜的时光。 在 2022 年找工作时经历过的难以抉择的时光如今依然历历在目。2020 年疫情开始我的学生生活就发生了翻天覆地的变化，不能自由地进出校门、进出北京让我在研究生阶段的探索计划通通落空，取而代之的是食堂的透明隔板、进出校的艰难报备以及隐隐约约的焦虑不安。三年的限制让我对北京没有过多的留恋，我甚至是抱着埋怨的心态选择了杭州的工作岗位，并且在毕业后如愿地、如释重负地来到了杭州。杭州的山水人文环境也确实疗愈了我，让我能够在今时今日决定回到北京。 很感谢在杭州一年半的经历，这一段时间中我也确确实实参与到了开源项目的开发和开源社区的运维工作当中。具体来说，我深度参与了 OceanBase on Kubernetes 拓展的开发，与 Kubeblocks 以及 Dify 两个社区也都有所交集，发现不同社区的风格和工作模式截然不同。在参与开源的过程中结交了一些朋友，通过学习开源项目也些许拓宽了眼界，也让我越发的清楚我适合做什么、我想要做什么。 我不太擅长直接对接客户，这会极大地消耗我的能量。而 ToB 业务的开源部门或多或少需要直接对客沟通，包括但不限于拉群支持，在线会议支持，组织动手实验等形式。这些面向客户的活动本身是好的，但我觉得这些支持或者科普性质的工作较多地占用本该用于研发的时间后，真正要开始研发时自己可能就处在“低电量”模式了。 尽管如此，我的工作基本也都圆满完成并且得到了同事们的认可。但我深知这样的状态不可持续，因为我所追求的首要目标不是高绩效或者安逸的生活，而是高效地产出有意义有价值的工作成果。我需要做出改变。 在近期的几篇播客里我都提到了 AI 相关的内容，从 2022 年开始通过 Github Copilot 接触 AI 工具以来，我已经在 AI 的影响下度过了整整三个年头了。我目睹着 AI 工具从 Copilot 单纯的单行推断补全，到 ChatGPT 简单强大的智能对话，再到 Cursor 这样的新一代 AI 编辑器让我不用写一行代码就能完成应用开发。我一年前也曾经抱着“封闭”的态度看待 AI 及其应用的发展：ChatGPT 不过是优化版的搜索引擎，而且可能就会止步于此。到了现在，我发现 AI 已经深切地影响到了我从事的行业，我也利用它极大地提升了自己的效率，也完全认为它带来的影响是积极乐观的。既然已经从旁观者变成了冲浪者，那么不如全面拥抱变化成为造浪者！ 我接下来将参与到 AI 的创业团队当中，开展 AI Infra 的研发工作，这让我踌躇满志。幸运的是团队成员都很有活力，一切都生机盎然，让我感觉这不是到了新的环境，而是与老朋友们的阔别重逢。 在杭州的最近半年时间里，我接触了很多与 RAG 有关的应用，看到了数据库检索给 AI 加持的能力，同时也看到了该过程的局限性。开发者为 LLM 预定义了一套他们认为具有高价值的检索方案，初期也确实看到了些许成效，但想进一步发掘 RAG 的能力时就会发现预定义的检索过程对 LLM 来说是一种限制或者规训，让人工创造的智能又重新受困于人工定义的工作流当中。 RAG 方案要解决的问题是对 LLM 进行时效性知识和领域特定知识的补充，根本场景还是要回答用户的问题。不妨把“回答用户问题”这一目标再往上抽象一层，要完成用户指定的特定任务，那么作为开发者是否需要把完成所有任务的 SOP 给模型预定好呢？我想着是不需要而且也是不可行的。我们应当为模型提供好工具、环境和规则，让它学习在规则约束下使用工具在环境中完成任务，正如同我们生活在这个世界上需要遵守各种规章制度，也要善用各种工具，以实现符合我们价值观的目标。 强化学习之父 Richard Sutton 在其 2019 年发表的一篇著名的文章 The Bitter Lesson 中提到，在 70 多年的 AI 领域研究过程中总是发现充分利用计算的通用方法最终总是大幅优于其他的特定方法，基于人类知识的方法总是让解决方案复杂化，使其难以充分利用计算的优势。 The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.…And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. There were many examples of AI researchers’ belated learning of this bitter lesson, and it is instructive to review some of the most prominent.…One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.– Richard Sutton 我认为通用方法是实现通用智能的路径，既然如此我会给它搭好平台、造好工具，静静等待它的成长和出现。而现在所见到的针对特定场景的 AI 应用（In-Domain RAG、Web Search、Image Generation 等）都将成为通用智能的工具。那接下来就争取成为强有力的通用智能启动器吧！","link":"/2025/03/01/BackToBeijing/"},{"title":"放弃精耕细作式的编程吧","text":"是时候放弃精耕细作的写代码方式了。 我们更应该站在宏观的、高阶的、更广阔的视角来审视整个项目：最根本的目标是什么？为了达成目标，我们真正需要什么？而不是纠结于某一处技术细节该如何实现——这类问题，已经可以交给 AI 来协助解决。 若仍执着于事无巨细地掌控每一行代码，即使用上 AI，能带来的提升也相当有限。无非是从挥锄头换成了开拖拉机，效率或许能提升五倍、十倍，但本质上我们依然在「耕地」，而不是「想清楚要种什么、怎么收成」。我们只是换了一件更趁手的工具，却没有换一种角色：从执行者，变成能看见更大图景、设定更高目标的人。 接触 Vibe Coding 的过程2022 年下半年，我在 GitHub 上了解到 Copilot。在 VS Code 里启用之后，光是「光标后自动补全代码」这一项能力，在当时就让我很受震撼。 在此之前，一旦遇到不熟悉的内容或需要查资料，就不得不把注意力从编辑器切到浏览器，整个写代码的过程不断在「打断 → 恢复 → 再打断 → 再恢复」里循环，非常消耗精力。这种模式贯穿了我 2022 年之前几乎所有的编程时光。但 Copilot 能做的有限：它只能在光标之后做顺序补全，无法在任意位置跳转补全，实际带来的效率提升并不算大。 到了 2024 年下半年，我开始用 Cursor，才意识到局面已经不一样了： 我们或许已经不再需要「面向搜索引擎编程」。 大部分写代码的步骤可以在编辑器里完成——写好函数签名或 TODO，按 Tab 就能续写。 遇到真正棘手的问题，打开 Chat，把你想做的事说清楚，Cursor 也能帮你完成一大部分。 那时我强烈地觉得：不能再沿用过去那套写代码和工作的方式了，得做点不一样的事。于是换了城市、换了工作，来到北京，开启了一段新旅程。 一度停止思考在新公司，我很快投入到 Sandbox 相关的基建里：从镜像构建、Sandbox 调度，到和云厂商的对接与协同，很多环节都由我推进落地。 当模型训练、产品上线等事情都压在同一段时间里时，就有一种连轴转的感觉——每天早上醒来都在想今天又要 Rush 什么，这种状态持续了相当长一段时间。 表面上看产出很多、节奏很快，但后遗症也很明显： 很多事都不得不以「冲刺」的节奏完成。 往往一件事还没做到自己满意，就要立刻切换上下文去做另一件。 在这种节奏下，很难留出时间做充分的设计与规划，也很难在结束时做收尾、整理和归纳。这种连轴转，让我一度陷入「没空思考」的状态，现在回想起来仍有些后怕。再加上 Vibe Coding 类工具用得很猛，这种状态带来的混乱只会更剧烈。 AI 生成的内容往往需要审阅，尤其在 2025 年上半年使用 Cursor 的那段「混沌期」：当时用得上头，觉得可以把工作大量交给它，不必再逐行审视。 结果就是：它给你的东西看起来符合预期，其实是虚假的繁荣。当你越来越依赖它，会不自觉地膨胀，觉得什么都能做、什么都能实现，但最终都会付出代价： 可维护性变差； 功能与预期不符； 对项目整体代码的理解变浅。 这不是 Vibe Coding 或工具的问题，而是使用方式的问题——也就是我自己的问题。我仍然试图把所有事都装在自己脑子里，却又没有真正花时间思考，这本身就很矛盾。但那一阵子，我确实长期处于这种状态，这也是我后来想要改变的地方。 不要停止思考从 2026 年开始，手头的事逐渐收束、更聚焦了一些，不再被各种杂事推着连轴转，终于有一点喘息的空间可以用来思考、总结，看看自己的工作方式是否需要调整。 也正是在这时，我明显感觉到：新一代的 coding agent 和之前的「AI 编辑器」已经不一样了。可以把它当成一位水平相当的 peer engineer，用来交流想法、请教实现。在得到清晰的需求描述之后，它们往往能做得很好，并且已经具备一定的 code review 能力——能指出潜在问题并给出改进建议，也能结合项目上下文给出更贴合的方案。 以前我们总嫌工具效率不够，现在工具已经上了一个台阶；真正卡住产出的，往往是我们自己：如何把需求梳理清楚、描述清楚，成了瓶颈。 所以：放弃像匠人那样对每一行代码精雕细琢的执念吧。我们或许该从「建筑工人」转向「建筑师」——用更宏观的视角去观察、审视和思考整个软件产品。放弃我们曾“擅长”和“引以为傲”的「手艺」，把具体实现交给 Agent 吧，它们更快也更好。 Handtyping 不重要了，代码实现也不重要了，最重要的是：不要停止思考。","link":"/2026/02/08/GiveupHandtyping/"}],"tags":[{"name":"GC","slug":"GC","link":"/tags/GC/"},{"name":"胡思乱想","slug":"胡思乱想","link":"/tags/%E8%83%A1%E6%80%9D%E4%B9%B1%E6%83%B3/"},{"name":"CI&#x2F;CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"Reading","slug":"Reading","link":"/tags/Reading/"},{"name":"AccessControl","slug":"AccessControl","link":"/tags/AccessControl/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"小程序","slug":"小程序","link":"/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"ECMAScript","slug":"ECMAScript","link":"/tags/ECMAScript/"},{"name":"Lint","slug":"Lint","link":"/tags/Lint/"},{"name":"摄影","slug":"摄影","link":"/tags/%E6%91%84%E5%BD%B1/"},{"name":"游记","slug":"游记","link":"/tags/%E6%B8%B8%E8%AE%B0/"},{"name":"GraphQL","slug":"GraphQL","link":"/tags/GraphQL/"},{"name":"Deployment","slug":"Deployment","link":"/tags/Deployment/"},{"name":"Web Server","slug":"Web-Server","link":"/tags/Web-Server/"},{"name":"NPM","slug":"NPM","link":"/tags/NPM/"},{"name":"Rust","slug":"Rust","link":"/tags/Rust/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"读后感","slug":"读后感","link":"/tags/%E8%AF%BB%E5%90%8E%E6%84%9F/"},{"name":"CloudNative","slug":"CloudNative","link":"/tags/CloudNative/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"Security","slug":"Security","link":"/tags/Security/"},{"name":"TypeScript","slug":"TypeScript","link":"/tags/TypeScript/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Docs","slug":"Docs","link":"/tags/Docs/"},{"name":"Coding","slug":"Coding","link":"/tags/Coding/"},{"name":"游戏","slug":"游戏","link":"/tags/%E6%B8%B8%E6%88%8F/"},{"name":"云游戏","slug":"云游戏","link":"/tags/%E4%BA%91%E6%B8%B8%E6%88%8F/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"RAG","slug":"RAG","link":"/tags/RAG/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Vibe Coding","slug":"Vibe-Coding","link":"/tags/Vibe-Coding/"},{"name":"Agent","slug":"Agent","link":"/tags/Agent/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/%E5%AD%A6%E4%B9%A0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"}],"pages":[{"title":"关于我 About Me","text":"努力成为软件工程师的码农，希望能够运用简单的方式解构复杂的工作和生活。热爱构建，希望能创造出有价值的应用、有意义的生活。 我是个追求从容的人，什么是从容呢？我觉得就是尽可能的把握自己的时间，让自己忙的时候不那么忙，闲的时候不那么闲，这样就可以让自己不至于奔走至狼狈，也不至于闲散到颓废，少一些无可奈何的身不由己，少一些心血来潮的浅尝辄止，这应该就是我眼中的从容吧。 A developer who is striving to become a software engineer, hoping to deconstruct complex work and life in a simple way. Love to build, hoping to create valuable applications and meaningful life. I am a person who pursues calmness. What is calmness? I think it is to grasp one’s own time as much as possible, so that when you are busy, you are not so struggled, and when you are free, you are not so idle. In this way, you can avoid being so busy that you are at a loss, and not so idle that you are decadent. You can have less helplessness and less superficiality. This should be the calmness in my eyes.","link":"/about/index.html"}]}